{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-22 12:37:37.558701: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-22 12:37:37.558806: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-22 12:37:37.558819: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-01-22 12:37:38.658228: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-01-22 12:37:38.658275: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: andy-GA-970A-D3\n",
      "2023-01-22 12:37:38.658289: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: andy-GA-970A-D3\n",
      "2023-01-22 12:37:38.658435: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 520.61.5\n",
      "2023-01-22 12:37:38.658487: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5\n",
      "2023-01-22 12:37:38.658501: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 520.61.5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import pandas as pd\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import feature_manager as fma\n",
    "import classifier.multi_dnn_classifier as dnn\n",
    "from random import randint\n",
    "from keras import callbacks, losses\n",
    "import visualizer\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import importlib\n",
    "import tr_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(fma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported ../data/BTCUSDT-1h.csv with 46982 rows\n",
      "Imported ../nocommit/BTCUSDT-1m.csv with 2817999 rows\n",
      "Scanning 7 future timeframes to build trade signal: \n",
      "1, 2, 3, 4, 5, 6, 7, \n",
      "Label producing completed. \n",
      " Value counts:\n",
      "2    15959\n",
      "0    15689\n",
      "1    15334\n",
      "Name: trade_signal, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "metric_list = [\n",
    "    \"loss\",\n",
    "    \"accuracy\",\n",
    "    \"precision\",\n",
    "    \"recall\",\n",
    "    \"precision-0.65\",\n",
    "    \"recall-0.65\",\n",
    "    \"precision-0.80\",\n",
    "    \"recall-0.80\",\n",
    "    \"precision-0.95\",\n",
    "    \"recall-0.95\"\n",
    "]\n",
    "\n",
    "symbol = \"BTCUSDT\"\n",
    "trade_tf = \"1h\"\n",
    "granular_tf = \"1m\"\n",
    "\n",
    "tp = 0.013\n",
    "sl = 0.013\n",
    "md = 7\n",
    "\n",
    "fm = fma.FeatureManager(\n",
    "    target_col=\"trade_signal\"\n",
    ")\n",
    "\n",
    "fm.import_data(symbol=symbol,trade_timeframe=trade_tf,granular_timeframe=granular_tf)\n",
    "\n",
    "fm.prepare_trade_forward_data(\n",
    "    take_profit_rate=tp,\n",
    "    stop_loss_rate=sl,\n",
    "    max_duration=md,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating features...\n",
      "\n",
      "Adding feature: returns, dir, sma, boll, boll7, boll14, boll21, min, min7, min14, min21, max, max7, max14, max21, mom, mom7, mom14, mom21, vol, vol7, vol14, vol21, obv, mfi7, mfi14, mfi21, rsi7, rsi14, rsi21, adx7, adx14, adx21, roc, roc7, roc14, roc21, atr7, atr14, atr21, bop, ad, adosc, trange, ado, willr7, willr14, willr21, dx7, dx14, dx21, trix, ultosc, high, low, \n",
      "\n",
      "Normalizing feature: returns_lag_1, returns_lag_2, returns_lag_3, returns_lag_4, returns_lag_5, returns_lag_6, returns_lag_7, returns_lag_8, returns_lag_9, returns_lag_10, returns_lag_11, returns_lag_12, returns_lag_13, returns_lag_14, returns_lag_15, returns_lag_16, returns_lag_17, returns_lag_18, returns_lag_19, returns_lag_20, returns_lag_21, returns_lag_22, returns_lag_23, returns_lag_24, returns_lag_25, returns_lag_26, returns_lag_27, returns_lag_28, returns_lag_29, returns_lag_30, dir_lag_1, dir_lag_2, dir_lag_3, dir_lag_4, dir_lag_5, dir_lag_6, dir_lag_7, dir_lag_8, dir_lag_9, dir_lag_10, dir_lag_11, dir_lag_12, dir_lag_13, dir_lag_14, dir_lag_15, dir_lag_16, dir_lag_17, dir_lag_18, dir_lag_19, dir_lag_20, dir_lag_21, dir_lag_22, dir_lag_23, dir_lag_24, dir_lag_25, dir_lag_26, dir_lag_27, dir_lag_28, dir_lag_29, dir_lag_30, sma_lag_1, sma_lag_2, sma_lag_3, sma_lag_4, sma_lag_5, sma_lag_6, sma_lag_7, sma_lag_8, sma_lag_9, sma_lag_10, sma_lag_11, sma_lag_12, sma_lag_13, sma_lag_14, sma_lag_15, sma_lag_16, sma_lag_17, sma_lag_18, sma_lag_19, sma_lag_20, sma_lag_21, sma_lag_22, sma_lag_23, sma_lag_24, sma_lag_25, sma_lag_26, sma_lag_27, sma_lag_28, sma_lag_29, sma_lag_30, boll_lag_1, boll_lag_2, boll_lag_3, boll_lag_4, boll_lag_5, boll_lag_6, boll_lag_7, boll_lag_8, boll_lag_9, boll_lag_10, boll_lag_11, boll_lag_12, boll_lag_13, boll_lag_14, boll_lag_15, boll_lag_16, boll_lag_17, boll_lag_18, boll_lag_19, boll_lag_20, boll_lag_21, boll_lag_22, boll_lag_23, boll_lag_24, boll_lag_25, boll_lag_26, boll_lag_27, boll_lag_28, boll_lag_29, boll_lag_30, boll7_lag_1, boll7_lag_2, boll7_lag_3, boll7_lag_4, boll7_lag_5, boll7_lag_6, boll7_lag_7, boll7_lag_8, boll7_lag_9, boll7_lag_10, boll7_lag_11, boll7_lag_12, boll7_lag_13, boll7_lag_14, boll7_lag_15, boll7_lag_16, boll7_lag_17, boll7_lag_18, boll7_lag_19, boll7_lag_20, boll7_lag_21, boll7_lag_22, boll7_lag_23, boll7_lag_24, boll7_lag_25, boll7_lag_26, boll7_lag_27, boll7_lag_28, boll7_lag_29, boll7_lag_30, boll14_lag_1, boll14_lag_2, boll14_lag_3, boll14_lag_4, boll14_lag_5, boll14_lag_6, boll14_lag_7, boll14_lag_8, boll14_lag_9, boll14_lag_10, boll14_lag_11, boll14_lag_12, boll14_lag_13, boll14_lag_14, boll14_lag_15, boll14_lag_16, boll14_lag_17, boll14_lag_18, boll14_lag_19, boll14_lag_20, boll14_lag_21, boll14_lag_22, boll14_lag_23, boll14_lag_24, boll14_lag_25, boll14_lag_26, boll14_lag_27, boll14_lag_28, boll14_lag_29, boll14_lag_30, boll21_lag_1, boll21_lag_2, boll21_lag_3, boll21_lag_4, boll21_lag_5, boll21_lag_6, boll21_lag_7, boll21_lag_8, boll21_lag_9, boll21_lag_10, boll21_lag_11, boll21_lag_12, boll21_lag_13, boll21_lag_14, boll21_lag_15, boll21_lag_16, boll21_lag_17, boll21_lag_18, boll21_lag_19, boll21_lag_20, boll21_lag_21, boll21_lag_22, boll21_lag_23, boll21_lag_24, boll21_lag_25, boll21_lag_26, boll21_lag_27, boll21_lag_28, boll21_lag_29, boll21_lag_30, min_lag_1, min_lag_2, min_lag_3, min_lag_4, min_lag_5, min_lag_6, min_lag_7, min_lag_8, min_lag_9, min_lag_10, min_lag_11, min_lag_12, min_lag_13, min_lag_14, min_lag_15, min_lag_16, min_lag_17, min_lag_18, min_lag_19, min_lag_20, min_lag_21, min_lag_22, min_lag_23, min_lag_24, min_lag_25, min_lag_26, min_lag_27, min_lag_28, min_lag_29, min_lag_30, min7_lag_1, min7_lag_2, min7_lag_3, min7_lag_4, min7_lag_5, min7_lag_6, min7_lag_7, min7_lag_8, min7_lag_9, min7_lag_10, min7_lag_11, min7_lag_12, min7_lag_13, min7_lag_14, min7_lag_15, min7_lag_16, min7_lag_17, min7_lag_18, min7_lag_19, min7_lag_20, min7_lag_21, min7_lag_22, min7_lag_23, min7_lag_24, min7_lag_25, min7_lag_26, min7_lag_27, min7_lag_28, min7_lag_29, min7_lag_30, min14_lag_1, min14_lag_2, min14_lag_3, min14_lag_4, min14_lag_5, min14_lag_6, min14_lag_7, min14_lag_8, min14_lag_9, min14_lag_10, min14_lag_11, min14_lag_12, min14_lag_13, min14_lag_14, min14_lag_15, min14_lag_16, min14_lag_17, min14_lag_18, min14_lag_19, min14_lag_20, min14_lag_21, min14_lag_22, min14_lag_23, min14_lag_24, min14_lag_25, min14_lag_26, min14_lag_27, min14_lag_28, min14_lag_29, min14_lag_30, min21_lag_1, min21_lag_2, min21_lag_3, min21_lag_4, min21_lag_5, min21_lag_6, min21_lag_7, min21_lag_8, min21_lag_9, min21_lag_10, min21_lag_11, min21_lag_12, min21_lag_13, min21_lag_14, min21_lag_15, min21_lag_16, min21_lag_17, min21_lag_18, min21_lag_19, min21_lag_20, min21_lag_21, min21_lag_22, min21_lag_23, min21_lag_24, min21_lag_25, min21_lag_26, min21_lag_27, min21_lag_28, min21_lag_29, min21_lag_30, max_lag_1, max_lag_2, max_lag_3, max_lag_4, max_lag_5, max_lag_6, max_lag_7, max_lag_8, max_lag_9, max_lag_10, max_lag_11, max_lag_12, max_lag_13, max_lag_14, max_lag_15, max_lag_16, max_lag_17, max_lag_18, max_lag_19, max_lag_20, max_lag_21, max_lag_22, max_lag_23, max_lag_24, max_lag_25, max_lag_26, max_lag_27, max_lag_28, max_lag_29, max_lag_30, max7_lag_1, max7_lag_2, max7_lag_3, max7_lag_4, max7_lag_5, max7_lag_6, max7_lag_7, max7_lag_8, max7_lag_9, max7_lag_10, max7_lag_11, max7_lag_12, max7_lag_13, max7_lag_14, max7_lag_15, max7_lag_16, max7_lag_17, max7_lag_18, max7_lag_19, max7_lag_20, max7_lag_21, max7_lag_22, max7_lag_23, max7_lag_24, max7_lag_25, max7_lag_26, max7_lag_27, max7_lag_28, max7_lag_29, max7_lag_30, max14_lag_1, max14_lag_2, max14_lag_3, max14_lag_4, max14_lag_5, max14_lag_6, max14_lag_7, max14_lag_8, max14_lag_9, max14_lag_10, max14_lag_11, max14_lag_12, max14_lag_13, max14_lag_14, max14_lag_15, max14_lag_16, max14_lag_17, max14_lag_18, max14_lag_19, max14_lag_20, max14_lag_21, max14_lag_22, max14_lag_23, max14_lag_24, max14_lag_25, max14_lag_26, max14_lag_27, max14_lag_28, max14_lag_29, max14_lag_30, max21_lag_1, max21_lag_2, max21_lag_3, max21_lag_4, max21_lag_5, max21_lag_6, max21_lag_7, max21_lag_8, max21_lag_9, max21_lag_10, max21_lag_11, max21_lag_12, max21_lag_13, max21_lag_14, max21_lag_15, max21_lag_16, max21_lag_17, max21_lag_18, max21_lag_19, max21_lag_20, max21_lag_21, max21_lag_22, max21_lag_23, max21_lag_24, max21_lag_25, max21_lag_26, max21_lag_27, max21_lag_28, max21_lag_29, max21_lag_30, mom_lag_1, mom_lag_2, mom_lag_3, mom_lag_4, mom_lag_5, mom_lag_6, mom_lag_7, mom_lag_8, mom_lag_9, mom_lag_10, mom_lag_11, mom_lag_12, mom_lag_13, mom_lag_14, mom_lag_15, mom_lag_16, mom_lag_17, mom_lag_18, mom_lag_19, mom_lag_20, mom_lag_21, mom_lag_22, mom_lag_23, mom_lag_24, mom_lag_25, mom_lag_26, mom_lag_27, mom_lag_28, mom_lag_29, mom_lag_30, mom7_lag_1, mom7_lag_2, mom7_lag_3, mom7_lag_4, mom7_lag_5, mom7_lag_6, mom7_lag_7, mom7_lag_8, mom7_lag_9, mom7_lag_10, mom7_lag_11, mom7_lag_12, mom7_lag_13, mom7_lag_14, mom7_lag_15, mom7_lag_16, mom7_lag_17, mom7_lag_18, mom7_lag_19, mom7_lag_20, mom7_lag_21, mom7_lag_22, mom7_lag_23, mom7_lag_24, mom7_lag_25, mom7_lag_26, mom7_lag_27, mom7_lag_28, mom7_lag_29, mom7_lag_30, mom14_lag_1, mom14_lag_2, mom14_lag_3, mom14_lag_4, mom14_lag_5, mom14_lag_6, mom14_lag_7, mom14_lag_8, mom14_lag_9, mom14_lag_10, mom14_lag_11, mom14_lag_12, mom14_lag_13, mom14_lag_14, mom14_lag_15, mom14_lag_16, mom14_lag_17, mom14_lag_18, mom14_lag_19, mom14_lag_20, mom14_lag_21, mom14_lag_22, mom14_lag_23, mom14_lag_24, mom14_lag_25, mom14_lag_26, mom14_lag_27, mom14_lag_28, mom14_lag_29, mom14_lag_30, mom21_lag_1, mom21_lag_2, mom21_lag_3, mom21_lag_4, mom21_lag_5, mom21_lag_6, mom21_lag_7, mom21_lag_8, mom21_lag_9, mom21_lag_10, mom21_lag_11, mom21_lag_12, mom21_lag_13, mom21_lag_14, mom21_lag_15, mom21_lag_16, mom21_lag_17, mom21_lag_18, mom21_lag_19, mom21_lag_20, mom21_lag_21, mom21_lag_22, mom21_lag_23, mom21_lag_24, mom21_lag_25, mom21_lag_26, mom21_lag_27, mom21_lag_28, mom21_lag_29, mom21_lag_30, vol_lag_1, vol_lag_2, vol_lag_3, vol_lag_4, vol_lag_5, vol_lag_6, vol_lag_7, vol_lag_8, vol_lag_9, vol_lag_10, vol_lag_11, vol_lag_12, vol_lag_13, vol_lag_14, vol_lag_15, vol_lag_16, vol_lag_17, vol_lag_18, vol_lag_19, vol_lag_20, vol_lag_21, vol_lag_22, vol_lag_23, vol_lag_24, vol_lag_25, vol_lag_26, vol_lag_27, vol_lag_28, vol_lag_29, vol_lag_30, vol7_lag_1, vol7_lag_2, vol7_lag_3, vol7_lag_4, vol7_lag_5, vol7_lag_6, vol7_lag_7, vol7_lag_8, vol7_lag_9, vol7_lag_10, vol7_lag_11, vol7_lag_12, vol7_lag_13, vol7_lag_14, vol7_lag_15, vol7_lag_16, vol7_lag_17, vol7_lag_18, vol7_lag_19, vol7_lag_20, vol7_lag_21, vol7_lag_22, vol7_lag_23, vol7_lag_24, vol7_lag_25, vol7_lag_26, vol7_lag_27, vol7_lag_28, vol7_lag_29, vol7_lag_30, vol14_lag_1, vol14_lag_2, vol14_lag_3, vol14_lag_4, vol14_lag_5, vol14_lag_6, vol14_lag_7, vol14_lag_8, vol14_lag_9, vol14_lag_10, vol14_lag_11, vol14_lag_12, vol14_lag_13, vol14_lag_14, vol14_lag_15, vol14_lag_16, vol14_lag_17, vol14_lag_18, vol14_lag_19, vol14_lag_20, vol14_lag_21, vol14_lag_22, vol14_lag_23, vol14_lag_24, vol14_lag_25, vol14_lag_26, vol14_lag_27, vol14_lag_28, vol14_lag_29, vol14_lag_30, vol21_lag_1, vol21_lag_2, vol21_lag_3, vol21_lag_4, vol21_lag_5, vol21_lag_6, vol21_lag_7, vol21_lag_8, vol21_lag_9, vol21_lag_10, vol21_lag_11, vol21_lag_12, vol21_lag_13, vol21_lag_14, vol21_lag_15, vol21_lag_16, vol21_lag_17, vol21_lag_18, vol21_lag_19, vol21_lag_20, vol21_lag_21, vol21_lag_22, vol21_lag_23, vol21_lag_24, vol21_lag_25, vol21_lag_26, vol21_lag_27, vol21_lag_28, vol21_lag_29, vol21_lag_30, obv_lag_1, obv_lag_2, obv_lag_3, obv_lag_4, obv_lag_5, obv_lag_6, obv_lag_7, obv_lag_8, obv_lag_9, obv_lag_10, obv_lag_11, obv_lag_12, obv_lag_13, obv_lag_14, obv_lag_15, obv_lag_16, obv_lag_17, obv_lag_18, obv_lag_19, obv_lag_20, obv_lag_21, obv_lag_22, obv_lag_23, obv_lag_24, obv_lag_25, obv_lag_26, obv_lag_27, obv_lag_28, obv_lag_29, obv_lag_30, mfi7_lag_1, mfi7_lag_2, mfi7_lag_3, mfi7_lag_4, mfi7_lag_5, mfi7_lag_6, mfi7_lag_7, mfi7_lag_8, mfi7_lag_9, mfi7_lag_10, mfi7_lag_11, mfi7_lag_12, mfi7_lag_13, mfi7_lag_14, mfi7_lag_15, mfi7_lag_16, mfi7_lag_17, mfi7_lag_18, mfi7_lag_19, mfi7_lag_20, mfi7_lag_21, mfi7_lag_22, mfi7_lag_23, mfi7_lag_24, mfi7_lag_25, mfi7_lag_26, mfi7_lag_27, mfi7_lag_28, mfi7_lag_29, mfi7_lag_30, mfi14_lag_1, mfi14_lag_2, mfi14_lag_3, mfi14_lag_4, mfi14_lag_5, mfi14_lag_6, mfi14_lag_7, mfi14_lag_8, mfi14_lag_9, mfi14_lag_10, mfi14_lag_11, mfi14_lag_12, mfi14_lag_13, mfi14_lag_14, mfi14_lag_15, mfi14_lag_16, mfi14_lag_17, mfi14_lag_18, mfi14_lag_19, mfi14_lag_20, mfi14_lag_21, mfi14_lag_22, mfi14_lag_23, mfi14_lag_24, mfi14_lag_25, mfi14_lag_26, mfi14_lag_27, mfi14_lag_28, mfi14_lag_29, mfi14_lag_30, mfi21_lag_1, mfi21_lag_2, mfi21_lag_3, mfi21_lag_4, mfi21_lag_5, mfi21_lag_6, mfi21_lag_7, mfi21_lag_8, mfi21_lag_9, mfi21_lag_10, mfi21_lag_11, mfi21_lag_12, mfi21_lag_13, mfi21_lag_14, mfi21_lag_15, mfi21_lag_16, mfi21_lag_17, mfi21_lag_18, mfi21_lag_19, mfi21_lag_20, mfi21_lag_21, mfi21_lag_22, mfi21_lag_23, mfi21_lag_24, mfi21_lag_25, mfi21_lag_26, mfi21_lag_27, mfi21_lag_28, mfi21_lag_29, mfi21_lag_30, rsi7_lag_1, rsi7_lag_2, rsi7_lag_3, rsi7_lag_4, rsi7_lag_5, rsi7_lag_6, rsi7_lag_7, rsi7_lag_8, rsi7_lag_9, rsi7_lag_10, rsi7_lag_11, rsi7_lag_12, rsi7_lag_13, rsi7_lag_14, rsi7_lag_15, rsi7_lag_16, rsi7_lag_17, rsi7_lag_18, rsi7_lag_19, rsi7_lag_20, rsi7_lag_21, rsi7_lag_22, rsi7_lag_23, rsi7_lag_24, rsi7_lag_25, rsi7_lag_26, rsi7_lag_27, rsi7_lag_28, rsi7_lag_29, rsi7_lag_30, rsi14_lag_1, rsi14_lag_2, rsi14_lag_3, rsi14_lag_4, rsi14_lag_5, rsi14_lag_6, rsi14_lag_7, rsi14_lag_8, rsi14_lag_9, rsi14_lag_10, rsi14_lag_11, rsi14_lag_12, rsi14_lag_13, rsi14_lag_14, rsi14_lag_15, rsi14_lag_16, rsi14_lag_17, rsi14_lag_18, rsi14_lag_19, rsi14_lag_20, rsi14_lag_21, rsi14_lag_22, rsi14_lag_23, rsi14_lag_24, rsi14_lag_25, rsi14_lag_26, rsi14_lag_27, rsi14_lag_28, rsi14_lag_29, rsi14_lag_30, rsi21_lag_1, rsi21_lag_2, rsi21_lag_3, rsi21_lag_4, rsi21_lag_5, rsi21_lag_6, rsi21_lag_7, rsi21_lag_8, rsi21_lag_9, rsi21_lag_10, rsi21_lag_11, rsi21_lag_12, rsi21_lag_13, rsi21_lag_14, rsi21_lag_15, rsi21_lag_16, rsi21_lag_17, rsi21_lag_18, rsi21_lag_19, rsi21_lag_20, rsi21_lag_21, rsi21_lag_22, rsi21_lag_23, rsi21_lag_24, rsi21_lag_25, rsi21_lag_26, rsi21_lag_27, rsi21_lag_28, rsi21_lag_29, rsi21_lag_30, adx7_lag_1, adx7_lag_2, adx7_lag_3, adx7_lag_4, adx7_lag_5, adx7_lag_6, adx7_lag_7, adx7_lag_8, adx7_lag_9, adx7_lag_10, adx7_lag_11, adx7_lag_12, adx7_lag_13, adx7_lag_14, adx7_lag_15, adx7_lag_16, adx7_lag_17, adx7_lag_18, adx7_lag_19, adx7_lag_20, adx7_lag_21, adx7_lag_22, adx7_lag_23, adx7_lag_24, adx7_lag_25, adx7_lag_26, adx7_lag_27, adx7_lag_28, adx7_lag_29, adx7_lag_30, adx14_lag_1, adx14_lag_2, adx14_lag_3, adx14_lag_4, adx14_lag_5, adx14_lag_6, adx14_lag_7, adx14_lag_8, adx14_lag_9, adx14_lag_10, adx14_lag_11, adx14_lag_12, adx14_lag_13, adx14_lag_14, adx14_lag_15, adx14_lag_16, adx14_lag_17, adx14_lag_18, adx14_lag_19, adx14_lag_20, adx14_lag_21, adx14_lag_22, adx14_lag_23, adx14_lag_24, adx14_lag_25, adx14_lag_26, adx14_lag_27, adx14_lag_28, adx14_lag_29, adx14_lag_30, adx21_lag_1, adx21_lag_2, adx21_lag_3, adx21_lag_4, adx21_lag_5, adx21_lag_6, adx21_lag_7, adx21_lag_8, adx21_lag_9, adx21_lag_10, adx21_lag_11, adx21_lag_12, adx21_lag_13, adx21_lag_14, adx21_lag_15, adx21_lag_16, adx21_lag_17, adx21_lag_18, adx21_lag_19, adx21_lag_20, adx21_lag_21, adx21_lag_22, adx21_lag_23, adx21_lag_24, adx21_lag_25, adx21_lag_26, adx21_lag_27, adx21_lag_28, adx21_lag_29, adx21_lag_30, roc_lag_1, roc_lag_2, roc_lag_3, roc_lag_4, roc_lag_5, roc_lag_6, roc_lag_7, roc_lag_8, roc_lag_9, roc_lag_10, roc_lag_11, roc_lag_12, roc_lag_13, roc_lag_14, roc_lag_15, roc_lag_16, roc_lag_17, roc_lag_18, roc_lag_19, roc_lag_20, roc_lag_21, roc_lag_22, roc_lag_23, roc_lag_24, roc_lag_25, roc_lag_26, roc_lag_27, roc_lag_28, roc_lag_29, roc_lag_30, roc7_lag_1, roc7_lag_2, roc7_lag_3, roc7_lag_4, roc7_lag_5, roc7_lag_6, roc7_lag_7, roc7_lag_8, roc7_lag_9, roc7_lag_10, roc7_lag_11, roc7_lag_12, roc7_lag_13, roc7_lag_14, roc7_lag_15, roc7_lag_16, roc7_lag_17, roc7_lag_18, roc7_lag_19, roc7_lag_20, roc7_lag_21, roc7_lag_22, roc7_lag_23, roc7_lag_24, roc7_lag_25, roc7_lag_26, roc7_lag_27, roc7_lag_28, roc7_lag_29, roc7_lag_30, roc14_lag_1, roc14_lag_2, roc14_lag_3, roc14_lag_4, roc14_lag_5, roc14_lag_6, roc14_lag_7, roc14_lag_8, roc14_lag_9, roc14_lag_10, roc14_lag_11, roc14_lag_12, roc14_lag_13, roc14_lag_14, roc14_lag_15, roc14_lag_16, roc14_lag_17, roc14_lag_18, roc14_lag_19, roc14_lag_20, roc14_lag_21, roc14_lag_22, roc14_lag_23, roc14_lag_24, roc14_lag_25, roc14_lag_26, roc14_lag_27, roc14_lag_28, roc14_lag_29, roc14_lag_30, roc21_lag_1, roc21_lag_2, roc21_lag_3, roc21_lag_4, roc21_lag_5, roc21_lag_6, roc21_lag_7, roc21_lag_8, roc21_lag_9, roc21_lag_10, roc21_lag_11, roc21_lag_12, roc21_lag_13, roc21_lag_14, roc21_lag_15, roc21_lag_16, roc21_lag_17, roc21_lag_18, roc21_lag_19, roc21_lag_20, roc21_lag_21, roc21_lag_22, roc21_lag_23, roc21_lag_24, roc21_lag_25, roc21_lag_26, roc21_lag_27, roc21_lag_28, roc21_lag_29, roc21_lag_30, atr7_lag_1, atr7_lag_2, atr7_lag_3, atr7_lag_4, atr7_lag_5, atr7_lag_6, atr7_lag_7, atr7_lag_8, atr7_lag_9, atr7_lag_10, atr7_lag_11, atr7_lag_12, atr7_lag_13, atr7_lag_14, atr7_lag_15, atr7_lag_16, atr7_lag_17, atr7_lag_18, atr7_lag_19, atr7_lag_20, atr7_lag_21, atr7_lag_22, atr7_lag_23, atr7_lag_24, atr7_lag_25, atr7_lag_26, atr7_lag_27, atr7_lag_28, atr7_lag_29, atr7_lag_30, atr14_lag_1, atr14_lag_2, atr14_lag_3, atr14_lag_4, atr14_lag_5, atr14_lag_6, atr14_lag_7, atr14_lag_8, atr14_lag_9, atr14_lag_10, atr14_lag_11, atr14_lag_12, atr14_lag_13, atr14_lag_14, atr14_lag_15, atr14_lag_16, atr14_lag_17, atr14_lag_18, atr14_lag_19, atr14_lag_20, atr14_lag_21, atr14_lag_22, atr14_lag_23, atr14_lag_24, atr14_lag_25, atr14_lag_26, atr14_lag_27, atr14_lag_28, atr14_lag_29, atr14_lag_30, atr21_lag_1, atr21_lag_2, atr21_lag_3, atr21_lag_4, atr21_lag_5, atr21_lag_6, atr21_lag_7, atr21_lag_8, atr21_lag_9, atr21_lag_10, atr21_lag_11, atr21_lag_12, atr21_lag_13, atr21_lag_14, atr21_lag_15, atr21_lag_16, atr21_lag_17, atr21_lag_18, atr21_lag_19, atr21_lag_20, atr21_lag_21, atr21_lag_22, atr21_lag_23, atr21_lag_24, atr21_lag_25, atr21_lag_26, atr21_lag_27, atr21_lag_28, atr21_lag_29, atr21_lag_30, bop_lag_1, bop_lag_2, bop_lag_3, bop_lag_4, bop_lag_5, bop_lag_6, bop_lag_7, bop_lag_8, bop_lag_9, bop_lag_10, bop_lag_11, bop_lag_12, bop_lag_13, bop_lag_14, bop_lag_15, bop_lag_16, bop_lag_17, bop_lag_18, bop_lag_19, bop_lag_20, bop_lag_21, bop_lag_22, bop_lag_23, bop_lag_24, bop_lag_25, bop_lag_26, bop_lag_27, bop_lag_28, bop_lag_29, bop_lag_30, ad_lag_1, ad_lag_2, ad_lag_3, ad_lag_4, ad_lag_5, ad_lag_6, ad_lag_7, ad_lag_8, ad_lag_9, ad_lag_10, ad_lag_11, ad_lag_12, ad_lag_13, ad_lag_14, ad_lag_15, ad_lag_16, ad_lag_17, ad_lag_18, ad_lag_19, ad_lag_20, ad_lag_21, ad_lag_22, ad_lag_23, ad_lag_24, ad_lag_25, ad_lag_26, ad_lag_27, ad_lag_28, ad_lag_29, ad_lag_30, adosc_lag_1, adosc_lag_2, adosc_lag_3, adosc_lag_4, adosc_lag_5, adosc_lag_6, adosc_lag_7, adosc_lag_8, adosc_lag_9, adosc_lag_10, adosc_lag_11, adosc_lag_12, adosc_lag_13, adosc_lag_14, adosc_lag_15, adosc_lag_16, adosc_lag_17, adosc_lag_18, adosc_lag_19, adosc_lag_20, adosc_lag_21, adosc_lag_22, adosc_lag_23, adosc_lag_24, adosc_lag_25, adosc_lag_26, adosc_lag_27, adosc_lag_28, adosc_lag_29, adosc_lag_30, trange_lag_1, trange_lag_2, trange_lag_3, trange_lag_4, trange_lag_5, trange_lag_6, trange_lag_7, trange_lag_8, trange_lag_9, trange_lag_10, trange_lag_11, trange_lag_12, trange_lag_13, trange_lag_14, trange_lag_15, trange_lag_16, trange_lag_17, trange_lag_18, trange_lag_19, trange_lag_20, trange_lag_21, trange_lag_22, trange_lag_23, trange_lag_24, trange_lag_25, trange_lag_26, trange_lag_27, trange_lag_28, trange_lag_29, trange_lag_30, ado_lag_1, ado_lag_2, ado_lag_3, ado_lag_4, ado_lag_5, ado_lag_6, ado_lag_7, ado_lag_8, ado_lag_9, ado_lag_10, ado_lag_11, ado_lag_12, ado_lag_13, ado_lag_14, ado_lag_15, ado_lag_16, ado_lag_17, ado_lag_18, ado_lag_19, ado_lag_20, ado_lag_21, ado_lag_22, ado_lag_23, ado_lag_24, ado_lag_25, ado_lag_26, ado_lag_27, ado_lag_28, ado_lag_29, ado_lag_30, willr7_lag_1, willr7_lag_2, willr7_lag_3, willr7_lag_4, willr7_lag_5, willr7_lag_6, willr7_lag_7, willr7_lag_8, willr7_lag_9, willr7_lag_10, willr7_lag_11, willr7_lag_12, willr7_lag_13, willr7_lag_14, willr7_lag_15, willr7_lag_16, willr7_lag_17, willr7_lag_18, willr7_lag_19, willr7_lag_20, willr7_lag_21, willr7_lag_22, willr7_lag_23, willr7_lag_24, willr7_lag_25, willr7_lag_26, willr7_lag_27, willr7_lag_28, willr7_lag_29, willr7_lag_30, willr14_lag_1, willr14_lag_2, willr14_lag_3, willr14_lag_4, willr14_lag_5, willr14_lag_6, willr14_lag_7, willr14_lag_8, willr14_lag_9, willr14_lag_10, willr14_lag_11, willr14_lag_12, willr14_lag_13, willr14_lag_14, willr14_lag_15, willr14_lag_16, willr14_lag_17, willr14_lag_18, willr14_lag_19, willr14_lag_20, willr14_lag_21, willr14_lag_22, willr14_lag_23, willr14_lag_24, willr14_lag_25, willr14_lag_26, willr14_lag_27, willr14_lag_28, willr14_lag_29, willr14_lag_30, willr21_lag_1, willr21_lag_2, willr21_lag_3, willr21_lag_4, willr21_lag_5, willr21_lag_6, willr21_lag_7, willr21_lag_8, willr21_lag_9, willr21_lag_10, willr21_lag_11, willr21_lag_12, willr21_lag_13, willr21_lag_14, willr21_lag_15, willr21_lag_16, willr21_lag_17, willr21_lag_18, willr21_lag_19, willr21_lag_20, willr21_lag_21, willr21_lag_22, willr21_lag_23, willr21_lag_24, willr21_lag_25, willr21_lag_26, willr21_lag_27, willr21_lag_28, willr21_lag_29, willr21_lag_30, dx7_lag_1, dx7_lag_2, dx7_lag_3, dx7_lag_4, dx7_lag_5, dx7_lag_6, dx7_lag_7, dx7_lag_8, dx7_lag_9, dx7_lag_10, dx7_lag_11, dx7_lag_12, dx7_lag_13, dx7_lag_14, dx7_lag_15, dx7_lag_16, dx7_lag_17, dx7_lag_18, dx7_lag_19, dx7_lag_20, dx7_lag_21, dx7_lag_22, dx7_lag_23, dx7_lag_24, dx7_lag_25, dx7_lag_26, dx7_lag_27, dx7_lag_28, dx7_lag_29, dx7_lag_30, dx14_lag_1, dx14_lag_2, dx14_lag_3, dx14_lag_4, dx14_lag_5, dx14_lag_6, dx14_lag_7, dx14_lag_8, dx14_lag_9, dx14_lag_10, dx14_lag_11, dx14_lag_12, dx14_lag_13, dx14_lag_14, dx14_lag_15, dx14_lag_16, dx14_lag_17, dx14_lag_18, dx14_lag_19, dx14_lag_20, dx14_lag_21, dx14_lag_22, dx14_lag_23, dx14_lag_24, dx14_lag_25, dx14_lag_26, dx14_lag_27, dx14_lag_28, dx14_lag_29, dx14_lag_30, dx21_lag_1, dx21_lag_2, dx21_lag_3, dx21_lag_4, dx21_lag_5, dx21_lag_6, dx21_lag_7, dx21_lag_8, dx21_lag_9, dx21_lag_10, dx21_lag_11, dx21_lag_12, dx21_lag_13, dx21_lag_14, dx21_lag_15, dx21_lag_16, dx21_lag_17, dx21_lag_18, dx21_lag_19, dx21_lag_20, dx21_lag_21, dx21_lag_22, dx21_lag_23, dx21_lag_24, dx21_lag_25, dx21_lag_26, dx21_lag_27, dx21_lag_28, dx21_lag_29, dx21_lag_30, trix_lag_1, trix_lag_2, trix_lag_3, trix_lag_4, trix_lag_5, trix_lag_6, trix_lag_7, trix_lag_8, trix_lag_9, trix_lag_10, trix_lag_11, trix_lag_12, trix_lag_13, trix_lag_14, trix_lag_15, trix_lag_16, trix_lag_17, trix_lag_18, trix_lag_19, trix_lag_20, trix_lag_21, trix_lag_22, trix_lag_23, trix_lag_24, trix_lag_25, trix_lag_26, trix_lag_27, trix_lag_28, trix_lag_29, trix_lag_30, ultosc_lag_1, ultosc_lag_2, ultosc_lag_3, ultosc_lag_4, ultosc_lag_5, ultosc_lag_6, ultosc_lag_7, ultosc_lag_8, ultosc_lag_9, ultosc_lag_10, ultosc_lag_11, ultosc_lag_12, ultosc_lag_13, ultosc_lag_14, ultosc_lag_15, ultosc_lag_16, ultosc_lag_17, ultosc_lag_18, ultosc_lag_19, ultosc_lag_20, ultosc_lag_21, ultosc_lag_22, ultosc_lag_23, ultosc_lag_24, ultosc_lag_25, ultosc_lag_26, ultosc_lag_27, ultosc_lag_28, ultosc_lag_29, ultosc_lag_30, high_lag_1, high_lag_2, high_lag_3, high_lag_4, high_lag_5, high_lag_6, high_lag_7, high_lag_8, high_lag_9, high_lag_10, high_lag_11, high_lag_12, high_lag_13, high_lag_14, high_lag_15, high_lag_16, high_lag_17, high_lag_18, high_lag_19, high_lag_20, high_lag_21, high_lag_22, high_lag_23, high_lag_24, high_lag_25, high_lag_26, high_lag_27, high_lag_28, high_lag_29, high_lag_30, low_lag_1, low_lag_2, low_lag_3, low_lag_4, low_lag_5, low_lag_6, low_lag_7, low_lag_8, low_lag_9, low_lag_10, low_lag_11, low_lag_12, low_lag_13, low_lag_14, low_lag_15, low_lag_16, low_lag_17, low_lag_18, low_lag_19, low_lag_20, low_lag_21, low_lag_22, low_lag_23, low_lag_24, low_lag_25, low_lag_26, low_lag_27, low_lag_28, low_lag_29, low_lag_30, \n",
      "Total 1650 features added.\n"
     ]
    }
   ],
   "source": [
    "fm.build_features(lags=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'classifier.multi_dnn_classifier' from '/home/andy/CryptoTradingPlatform/TraderRobot/project/classifier/multi_dnn_classifier.py'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(dnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============\n",
      "DATA PREPARATION PARAMS:\n",
      "trade_timeframe:1h\n",
      "take_profit_rate:0.013\n",
      "stop_loss_rate:0.013\n",
      "max_duration:7\n",
      "lags:30\n",
      "fold_number:5\n",
      "train_size:0.7\n",
      "val_size:0.15\n",
      "categorical_label:True\n",
      "rebalance:None\n",
      "==========\n",
      "DATA:\n",
      "Total rows: 46803\n",
      "Label 0: 15686(33.51%)\n",
      "Label 1: 15252(32.59%)\n",
      "Label 2: 15865(33.9%)\n",
      "Splitting the data...\n",
      "Proccesing fold 1\n",
      "\n",
      "FOLD 1\n",
      "Train: 6552, Validation: 1404, Test: 1404\n",
      "\n",
      "Train:\n",
      "Label 0: 597(9.11%)\n",
      "Label 1: 3058(46.67%)\n",
      "Label 2: 2897(44.22%)\n",
      "\n",
      "Validation:\n",
      "Label 0: 574(40.88%)\n",
      "Label 1: 412(29.34%)\n",
      "Label 2: 418(29.77%)\n",
      "\n",
      "Test:\n",
      "Label 0: 532(37.89%)\n",
      "Label 1: 381(27.14%)\n",
      "Label 2: 491(34.97%)\n",
      "Proccesing fold 2\n",
      "\n",
      "FOLD 2\n",
      "Train: 6552, Validation: 1404, Test: 1404\n",
      "\n",
      "Train:\n",
      "Label 0: 3414(52.11%)\n",
      "Label 1: 1598(24.39%)\n",
      "Label 2: 1540(23.5%)\n",
      "\n",
      "Validation:\n",
      "Label 0: 221(15.74%)\n",
      "Label 1: 562(40.03%)\n",
      "Label 2: 621(44.23%)\n",
      "\n",
      "Test:\n",
      "Label 0: 673(47.93%)\n",
      "Label 1: 342(24.36%)\n",
      "Label 2: 389(27.71%)\n",
      "Proccesing fold 3\n",
      "\n",
      "FOLD 3\n",
      "Train: 6552, Validation: 1404, Test: 1404\n",
      "\n",
      "Train:\n",
      "Label 0: 2882(43.99%)\n",
      "Label 1: 1748(26.68%)\n",
      "Label 2: 1922(29.33%)\n",
      "\n",
      "Validation:\n",
      "Label 0: 730(51.99%)\n",
      "Label 1: 308(21.94%)\n",
      "Label 2: 366(26.07%)\n",
      "\n",
      "Test:\n",
      "Label 0: 795(56.62%)\n",
      "Label 1: 333(23.72%)\n",
      "Label 2: 276(19.66%)\n",
      "Proccesing fold 4\n",
      "\n",
      "FOLD 4\n",
      "Train: 6552, Validation: 1404, Test: 1404\n",
      "\n",
      "Train:\n",
      "Label 0: 815(12.44%)\n",
      "Label 1: 2867(43.76%)\n",
      "Label 2: 2870(43.8%)\n",
      "\n",
      "Validation:\n",
      "Label 0: 390(27.78%)\n",
      "Label 1: 486(34.62%)\n",
      "Label 2: 528(37.61%)\n",
      "\n",
      "Test:\n",
      "Label 0: 366(26.07%)\n",
      "Label 1: 483(34.4%)\n",
      "Label 2: 555(39.53%)\n",
      "Proccesing fold 5\n",
      "\n",
      "FOLD 5\n",
      "Train: 6552, Validation: 1404, Test: 1404\n",
      "\n",
      "Train:\n",
      "Label 0: 2052(31.32%)\n",
      "Label 1: 2136(32.6%)\n",
      "Label 2: 2364(36.08%)\n",
      "\n",
      "Validation:\n",
      "Label 0: 718(51.14%)\n",
      "Label 1: 334(23.79%)\n",
      "Label 2: 352(25.07%)\n",
      "\n",
      "Test:\n",
      "Label 0: 924(65.81%)\n",
      "Label 1: 204(14.53%)\n",
      "Label 2: 276(19.66%)\n",
      "\n",
      ">>>>>> FOLD 1\n",
      "\n",
      "\n",
      "=============\n",
      "CLASSIFIER PARAMS:\n",
      "hu:100\n",
      "output_bias:[-1.0710377193291125, 0.5625615537695406, 0.5084761650458264]\n",
      "loss:categorical_crossentropy\n",
      "dropout:True\n",
      "dropout_rate:0.3\n",
      "learning_rate:0.0001\n",
      "gpu:False\n",
      "set_class_weight:True\n",
      "save_check_point:False\n",
      "early_stopping:True\n",
      "patience:5\n",
      "epochs:200\n",
      "shuffle_when_train:False\n",
      "batch_size:20\n",
      "\n",
      "=============\n",
      "CLASSIFIER PARAMS:\n",
      "hu:100\n",
      "output_bias:[-1.0710377193291125, 0.5625615537695406, 0.5084761650458264]\n",
      "loss:categorical_crossentropy\n",
      "dropout:True\n",
      "dropout_rate:0.3\n",
      "learning_rate:0.0001\n",
      "gpu:False\n",
      "set_class_weight:True\n",
      "save_check_point:False\n",
      "early_stopping:True\n",
      "patience:5\n",
      "epochs:200\n",
      "shuffle_when_train:False\n",
      "batch_size:20\n",
      "Epoch 1/200\n",
      "328/328 [==============================] - 7s 12ms/step - loss: 1.2259 - tp: 2162.0000 - fp: 2997.0000 - tn: 10107.0000 - fn: 4390.0000 - accuracy: 0.4078 - precision: 0.4191 - precision-0.55: 0.4179 - precision-0.60: 0.4205 - precision-0.65: 0.4164 - precision-0.70: 0.4132 - precision-0.75: 0.4157 - precision-0.80: 0.4253 - precision-0.85: 0.4287 - precision-0.90: 0.4406 - precision-0.95: 0.4535 - recall: 0.3300 - recall-0.55: 0.2763 - recall-0.60: 0.2306 - recall-0.65: 0.1865 - recall-0.70: 0.1461 - recall-0.75: 0.1148 - recall-0.80: 0.0861 - recall-0.85: 0.0615 - recall-0.90: 0.0374 - recall-0.95: 0.0179 - val_loss: 1.3643 - val_tp: 572.0000 - val_fp: 667.0000 - val_tn: 2141.0000 - val_fn: 832.0000 - val_accuracy: 0.4487 - val_precision: 0.4617 - val_precision-0.55: 0.4719 - val_precision-0.60: 0.4714 - val_precision-0.65: 0.4852 - val_precision-0.70: 0.4961 - val_precision-0.75: 0.5200 - val_precision-0.80: 0.5287 - val_precision-0.85: 0.5556 - val_precision-0.90: 0.5902 - val_precision-0.95: 0.6239 - val_recall: 0.4074 - val_recall-0.55: 0.3832 - val_recall-0.60: 0.3575 - val_recall-0.65: 0.3376 - val_recall-0.70: 0.3148 - val_recall-0.75: 0.2870 - val_recall-0.80: 0.2493 - val_recall-0.85: 0.2066 - val_recall-0.90: 0.1375 - val_recall-0.95: 0.0520\n",
      "Epoch 2/200\n",
      "328/328 [==============================] - 2s 7ms/step - loss: 1.0019 - tp: 2336.0000 - fp: 2689.0000 - tn: 10415.0000 - fn: 4216.0000 - accuracy: 0.4487 - precision: 0.4649 - precision-0.55: 0.4674 - precision-0.60: 0.4677 - precision-0.65: 0.4618 - precision-0.70: 0.4603 - precision-0.75: 0.4604 - precision-0.80: 0.4624 - precision-0.85: 0.4522 - precision-0.90: 0.4741 - precision-0.95: 0.5181 - recall: 0.3565 - recall-0.55: 0.2877 - recall-0.60: 0.2297 - recall-0.65: 0.1714 - recall-0.70: 0.1274 - recall-0.75: 0.0905 - recall-0.80: 0.0629 - recall-0.85: 0.0368 - recall-0.90: 0.0195 - recall-0.95: 0.0066 - val_loss: 1.3161 - val_tp: 553.0000 - val_fp: 639.0000 - val_tn: 2169.0000 - val_fn: 851.0000 - val_accuracy: 0.4430 - val_precision: 0.4639 - val_precision-0.55: 0.4719 - val_precision-0.60: 0.4852 - val_precision-0.65: 0.4994 - val_precision-0.70: 0.5157 - val_precision-0.75: 0.5323 - val_precision-0.80: 0.5383 - val_precision-0.85: 0.5561 - val_precision-0.90: 0.5744 - val_precision-0.95: 0.6076 - val_recall: 0.3939 - val_recall-0.55: 0.3704 - val_recall-0.60: 0.3397 - val_recall-0.65: 0.3184 - val_recall-0.70: 0.2927 - val_recall-0.75: 0.2699 - val_recall-0.80: 0.2251 - val_recall-0.85: 0.1731 - val_recall-0.90: 0.0990 - val_recall-0.95: 0.0342\n",
      "Epoch 3/200\n",
      "328/328 [==============================] - 2s 7ms/step - loss: 0.9275 - tp: 2359.0000 - fp: 2537.0000 - tn: 10567.0000 - fn: 4193.0000 - accuracy: 0.4690 - precision: 0.4818 - precision-0.55: 0.4823 - precision-0.60: 0.4862 - precision-0.65: 0.4877 - precision-0.70: 0.5030 - precision-0.75: 0.4852 - precision-0.80: 0.4782 - precision-0.85: 0.5234 - precision-0.90: 0.5325 - precision-0.95: 0.5556 - recall: 0.3600 - recall-0.55: 0.2814 - recall-0.60: 0.2125 - recall-0.65: 0.1574 - recall-0.70: 0.1157 - recall-0.75: 0.0751 - recall-0.80: 0.0452 - recall-0.85: 0.0273 - recall-0.90: 0.0125 - recall-0.95: 0.0038 - val_loss: 1.3354 - val_tp: 552.0000 - val_fp: 623.0000 - val_tn: 2185.0000 - val_fn: 852.0000 - val_accuracy: 0.4480 - val_precision: 0.4698 - val_precision-0.55: 0.4781 - val_precision-0.60: 0.4870 - val_precision-0.65: 0.5045 - val_precision-0.70: 0.5118 - val_precision-0.75: 0.5307 - val_precision-0.80: 0.5485 - val_precision-0.85: 0.5414 - val_precision-0.90: 0.5455 - val_precision-0.95: 0.5652 - val_recall: 0.3932 - val_recall-0.55: 0.3647 - val_recall-0.60: 0.3348 - val_recall-0.65: 0.3177 - val_recall-0.70: 0.2942 - val_recall-0.75: 0.2707 - val_recall-0.80: 0.2336 - val_recall-0.85: 0.1816 - val_recall-0.90: 0.1111 - val_recall-0.95: 0.0370\n",
      "Epoch 4/200\n",
      "328/328 [==============================] - 2s 7ms/step - loss: 0.8980 - tp: 2407.0000 - fp: 2419.0000 - tn: 10685.0000 - fn: 4145.0000 - accuracy: 0.4817 - precision: 0.4988 - precision-0.55: 0.5049 - precision-0.60: 0.5134 - precision-0.65: 0.5147 - precision-0.70: 0.5187 - precision-0.75: 0.5312 - precision-0.80: 0.5223 - precision-0.85: 0.5306 - precision-0.90: 0.5179 - precision-0.95: 0.5758 - recall: 0.3674 - recall-0.55: 0.2845 - recall-0.60: 0.2134 - recall-0.65: 0.1496 - recall-0.70: 0.0997 - recall-0.75: 0.0650 - recall-0.80: 0.0375 - recall-0.85: 0.0198 - recall-0.90: 0.0089 - recall-0.95: 0.0029 - val_loss: 1.3393 - val_tp: 558.0000 - val_fp: 644.0000 - val_tn: 2164.0000 - val_fn: 846.0000 - val_accuracy: 0.4487 - val_precision: 0.4642 - val_precision-0.55: 0.4740 - val_precision-0.60: 0.4914 - val_precision-0.65: 0.5084 - val_precision-0.70: 0.5139 - val_precision-0.75: 0.5288 - val_precision-0.80: 0.5500 - val_precision-0.85: 0.5519 - val_precision-0.90: 0.5666 - val_precision-0.95: 0.5795 - val_recall: 0.3974 - val_recall-0.55: 0.3697 - val_recall-0.60: 0.3440 - val_recall-0.65: 0.3241 - val_recall-0.70: 0.3027 - val_recall-0.75: 0.2742 - val_recall-0.80: 0.2350 - val_recall-0.85: 0.1816 - val_recall-0.90: 0.1182 - val_recall-0.95: 0.0363\n",
      "Epoch 5/200\n",
      "328/328 [==============================] - 2s 7ms/step - loss: 0.8614 - tp: 2500.0000 - fp: 2290.0000 - tn: 10814.0000 - fn: 4052.0000 - accuracy: 0.5015 - precision: 0.5219 - precision-0.55: 0.5364 - precision-0.60: 0.5507 - precision-0.65: 0.5734 - precision-0.70: 0.5717 - precision-0.75: 0.5839 - precision-0.80: 0.5700 - precision-0.85: 0.5659 - precision-0.90: 0.5299 - precision-0.95: 0.4091 - recall: 0.3816 - recall-0.55: 0.2946 - recall-0.60: 0.2189 - recall-0.65: 0.1610 - recall-0.70: 0.1090 - recall-0.75: 0.0707 - recall-0.80: 0.0423 - recall-0.85: 0.0223 - recall-0.90: 0.0095 - recall-0.95: 0.0014 - val_loss: 1.3378 - val_tp: 555.0000 - val_fp: 634.0000 - val_tn: 2174.0000 - val_fn: 849.0000 - val_accuracy: 0.4509 - val_precision: 0.4668 - val_precision-0.55: 0.4711 - val_precision-0.60: 0.4943 - val_precision-0.65: 0.5011 - val_precision-0.70: 0.5139 - val_precision-0.75: 0.5302 - val_precision-0.80: 0.5348 - val_precision-0.85: 0.5484 - val_precision-0.90: 0.5294 - val_precision-0.95: 0.5000 - val_recall: 0.3953 - val_recall-0.55: 0.3597 - val_recall-0.60: 0.3397 - val_recall-0.65: 0.3141 - val_recall-0.70: 0.2906 - val_recall-0.75: 0.2628 - val_recall-0.80: 0.2187 - val_recall-0.85: 0.1695 - val_recall-0.90: 0.1026 - val_recall-0.95: 0.0313\n",
      "Epoch 6/200\n",
      "328/328 [==============================] - 2s 7ms/step - loss: 0.8339 - tp: 2665.0000 - fp: 2156.0000 - tn: 10948.0000 - fn: 3887.0000 - accuracy: 0.5232 - precision: 0.5528 - precision-0.55: 0.5633 - precision-0.60: 0.5805 - precision-0.65: 0.5816 - precision-0.70: 0.5838 - precision-0.75: 0.5997 - precision-0.80: 0.6220 - precision-0.85: 0.6463 - precision-0.90: 0.6379 - precision-0.95: 0.6897 - recall: 0.4067 - recall-0.55: 0.3129 - recall-0.60: 0.2332 - recall-0.65: 0.1665 - recall-0.70: 0.1132 - recall-0.75: 0.0720 - recall-0.80: 0.0440 - recall-0.85: 0.0243 - recall-0.90: 0.0113 - recall-0.95: 0.0031 - val_loss: 1.3329 - val_tp: 556.0000 - val_fp: 623.0000 - val_tn: 2185.0000 - val_fn: 848.0000 - val_accuracy: 0.4509 - val_precision: 0.4716 - val_precision-0.55: 0.4867 - val_precision-0.60: 0.4953 - val_precision-0.65: 0.5040 - val_precision-0.70: 0.5151 - val_precision-0.75: 0.5302 - val_precision-0.80: 0.5427 - val_precision-0.85: 0.5438 - val_precision-0.90: 0.5354 - val_precision-0.95: 0.5132 - val_recall: 0.3960 - val_recall-0.55: 0.3661 - val_recall-0.60: 0.3376 - val_recall-0.65: 0.3141 - val_recall-0.70: 0.2913 - val_recall-0.75: 0.2564 - val_recall-0.80: 0.2172 - val_recall-0.85: 0.1681 - val_recall-0.90: 0.0969 - val_recall-0.95: 0.0278\n",
      "Epoch 7/200\n",
      "328/328 [==============================] - 2s 7ms/step - loss: 0.8203 - tp: 2756.0000 - fp: 2189.0000 - tn: 10915.0000 - fn: 3796.0000 - accuracy: 0.5295 - precision: 0.5573 - precision-0.55: 0.5664 - precision-0.60: 0.5819 - precision-0.65: 0.5848 - precision-0.70: 0.5994 - precision-0.75: 0.6143 - precision-0.80: 0.6164 - precision-0.85: 0.6120 - precision-0.90: 0.5856 - precision-0.95: 0.5357 - recall: 0.4206 - recall-0.55: 0.3249 - recall-0.60: 0.2439 - recall-0.65: 0.1722 - recall-0.70: 0.1174 - recall-0.75: 0.0759 - recall-0.80: 0.0449 - recall-0.85: 0.0234 - recall-0.90: 0.0099 - recall-0.95: 0.0023 - val_loss: 1.3450 - val_tp: 561.0000 - val_fp: 616.0000 - val_tn: 2192.0000 - val_fn: 843.0000 - val_accuracy: 0.4558 - val_precision: 0.4766 - val_precision-0.55: 0.4786 - val_precision-0.60: 0.4944 - val_precision-0.65: 0.5056 - val_precision-0.70: 0.5074 - val_precision-0.75: 0.5185 - val_precision-0.80: 0.5357 - val_precision-0.85: 0.5516 - val_precision-0.90: 0.5563 - val_precision-0.95: 0.4943 - val_recall: 0.3996 - val_recall-0.55: 0.3661 - val_recall-0.60: 0.3454 - val_recall-0.65: 0.3212 - val_recall-0.70: 0.2920 - val_recall-0.75: 0.2593 - val_recall-0.80: 0.2244 - val_recall-0.85: 0.1788 - val_recall-0.90: 0.1125 - val_recall-0.95: 0.0306\n",
      "Epoch 7: early stopping\n",
      "44/44 [==============================] - 0s 2ms/step\n",
      "71/71 [==============================] - 0s 4ms/step - loss: 1.3884 - tp: 525.0000 - fp: 618.0000 - tn: 2190.0000 - fn: 879.0000 - accuracy: 0.4380 - precision: 0.4593 - precision-0.55: 0.4667 - precision-0.60: 0.4733 - precision-0.65: 0.4765 - precision-0.70: 0.4746 - precision-0.75: 0.4789 - precision-0.80: 0.4640 - precision-0.85: 0.4597 - precision-0.90: 0.4718 - precision-0.95: 0.5748 - recall: 0.3739 - recall-0.55: 0.3348 - recall-0.60: 0.3091 - recall-0.65: 0.2821 - recall-0.70: 0.2528 - recall-0.75: 0.2258 - recall-0.80: 0.1880 - recall-0.85: 0.1503 - recall-0.90: 0.1011 - recall-0.95: 0.0520                                        \n",
      "\n",
      "=============\n",
      "CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.85      0.59       532\n",
      "           1       0.33      0.07      0.12       381\n",
      "           2       0.44      0.27      0.34       491\n",
      "\n",
      "    accuracy                           0.44      1404\n",
      "   macro avg       0.41      0.40      0.35      1404\n",
      "weighted avg       0.41      0.44      0.37      1404\n",
      "\n",
      "\n",
      "=============\n",
      "CONFUSION MATRIX:\n",
      "        P-0  P-1  P-2  Total   RP-0   RP-1   RP-2\n",
      "0       452   18   62    532  0.850  0.034  0.117\n",
      "1       241   28  112    381  0.633  0.073  0.294\n",
      "2       318   38  135    491  0.648  0.077  0.275\n",
      "Total  1011   84  309   1404  0.720  0.060  0.220\n",
      "\n",
      ">>>>>> FOLD 2\n",
      "\n",
      "\n",
      "=============\n",
      "CLASSIFIER PARAMS:\n",
      "hu:100\n",
      "output_bias:[0.5184113336666485, -0.24072044337670112, -0.27769087426260725]\n",
      "loss:categorical_crossentropy\n",
      "dropout:True\n",
      "dropout_rate:0.3\n",
      "learning_rate:0.0001\n",
      "gpu:False\n",
      "set_class_weight:True\n",
      "save_check_point:False\n",
      "early_stopping:True\n",
      "patience:5\n",
      "epochs:200\n",
      "shuffle_when_train:False\n",
      "batch_size:20\n",
      "\n",
      "=============\n",
      "CLASSIFIER PARAMS:\n",
      "hu:100\n",
      "output_bias:[0.5184113336666485, -0.24072044337670112, -0.27769087426260725]\n",
      "loss:categorical_crossentropy\n",
      "dropout:True\n",
      "dropout_rate:0.3\n",
      "learning_rate:0.0001\n",
      "gpu:False\n",
      "set_class_weight:True\n",
      "save_check_point:False\n",
      "early_stopping:True\n",
      "patience:5\n",
      "epochs:200\n",
      "shuffle_when_train:False\n",
      "batch_size:20\n",
      "Epoch 1/200\n",
      "328/328 [==============================] - 7s 12ms/step - loss: 1.2088 - tp: 3284.0000 - fp: 2909.0000 - tn: 13003.0000 - fn: 4672.0000 - accuracy: 0.4981 - precision: 0.5303 - precision-0.55: 0.5485 - precision-0.60: 0.5618 - precision-0.65: 0.5767 - precision-0.70: 0.5879 - precision-0.75: 0.5928 - precision-0.80: 0.5933 - precision-0.85: 0.5931 - precision-0.90: 0.6029 - precision-0.95: 0.6413 - recall: 0.4128 - recall-0.55: 0.3611 - recall-0.60: 0.3131 - recall-0.65: 0.2656 - recall-0.70: 0.2182 - recall-0.75: 0.1726 - recall-0.80: 0.1342 - recall-0.85: 0.0949 - recall-0.90: 0.0574 - recall-0.95: 0.0222 - val_loss: 1.1266 - val_tp: 499.0000 - val_fp: 571.0000 - val_tn: 2237.0000 - val_fn: 905.0000 - val_accuracy: 0.4395 - val_precision: 0.4664 - val_precision-0.55: 0.4718 - val_precision-0.60: 0.4802 - val_precision-0.65: 0.4931 - val_precision-0.70: 0.5242 - val_precision-0.75: 0.5297 - val_precision-0.80: 0.5020 - val_precision-0.85: 0.4972 - val_precision-0.90: 0.4804 - val_precision-0.95: 0.5000 - val_recall: 0.3554 - val_recall-0.55: 0.2920 - val_recall-0.60: 0.2415 - val_recall-0.65: 0.2037 - val_recall-0.70: 0.1695 - val_recall-0.75: 0.1332 - val_recall-0.80: 0.0905 - val_recall-0.85: 0.0634 - val_recall-0.90: 0.0349 - val_recall-0.95: 0.0164\n",
      "Epoch 2/200\n",
      "328/328 [==============================] - 2s 7ms/step - loss: 1.0201 - tp: 2919.0000 - fp: 1790.0000 - tn: 11314.0000 - fn: 3633.0000 - accuracy: 0.5676 - precision: 0.6199 - precision-0.55: 0.6413 - precision-0.60: 0.6706 - precision-0.65: 0.6944 - precision-0.70: 0.6886 - precision-0.75: 0.6920 - precision-0.80: 0.7003 - precision-0.85: 0.6807 - precision-0.90: 0.7095 - precision-0.95: 0.7460 - recall: 0.4455 - recall-0.55: 0.3755 - recall-0.60: 0.3072 - recall-0.65: 0.2407 - recall-0.70: 0.1745 - recall-0.75: 0.1210 - recall-0.80: 0.0784 - recall-0.85: 0.0446 - recall-0.90: 0.0227 - recall-0.95: 0.0072 - val_loss: 1.1223 - val_tp: 485.0000 - val_fp: 582.0000 - val_tn: 2226.0000 - val_fn: 919.0000 - val_accuracy: 0.4373 - val_precision: 0.4545 - val_precision-0.55: 0.4552 - val_precision-0.60: 0.4782 - val_precision-0.65: 0.4828 - val_precision-0.70: 0.4964 - val_precision-0.75: 0.4919 - val_precision-0.80: 0.5064 - val_precision-0.85: 0.4663 - val_precision-0.90: 0.4455 - val_precision-0.95: 0.4500 - val_recall: 0.3454 - val_recall-0.55: 0.2785 - val_recall-0.60: 0.2265 - val_recall-0.65: 0.1802 - val_recall-0.70: 0.1453 - val_recall-0.75: 0.1083 - val_recall-0.80: 0.0840 - val_recall-0.85: 0.0541 - val_recall-0.90: 0.0321 - val_recall-0.95: 0.0128\n",
      "Epoch 3/200\n",
      "328/328 [==============================] - 2s 7ms/step - loss: 0.9552 - tp: 3046.0000 - fp: 1569.0000 - tn: 11535.0000 - fn: 3506.0000 - accuracy: 0.6010 - precision: 0.6600 - precision-0.55: 0.6803 - precision-0.60: 0.7024 - precision-0.65: 0.7182 - precision-0.70: 0.7250 - precision-0.75: 0.7410 - precision-0.80: 0.7533 - precision-0.85: 0.7543 - precision-0.90: 0.7561 - precision-0.95: 0.8222 - recall: 0.4649 - recall-0.55: 0.3858 - recall-0.60: 0.3077 - recall-0.65: 0.2283 - recall-0.70: 0.1618 - recall-0.75: 0.1035 - recall-0.80: 0.0611 - recall-0.85: 0.0333 - recall-0.90: 0.0142 - recall-0.95: 0.0056 - val_loss: 1.1000 - val_tp: 504.0000 - val_fp: 577.0000 - val_tn: 2231.0000 - val_fn: 900.0000 - val_accuracy: 0.4430 - val_precision: 0.4662 - val_precision-0.55: 0.4839 - val_precision-0.60: 0.4934 - val_precision-0.65: 0.5046 - val_precision-0.70: 0.4953 - val_precision-0.75: 0.5080 - val_precision-0.80: 0.4954 - val_precision-0.85: 0.4812 - val_precision-0.90: 0.4694 - val_precision-0.95: 0.3889 - val_recall: 0.3590 - val_recall-0.55: 0.2991 - val_recall-0.60: 0.2407 - val_recall-0.65: 0.1952 - val_recall-0.70: 0.1517 - val_recall-0.75: 0.1125 - val_recall-0.80: 0.0762 - val_recall-0.85: 0.0548 - val_recall-0.90: 0.0328 - val_recall-0.95: 0.0100\n",
      "Epoch 4/200\n",
      "328/328 [==============================] - 2s 7ms/step - loss: 0.9195 - tp: 3166.0000 - fp: 1446.0000 - tn: 11658.0000 - fn: 3386.0000 - accuracy: 0.6248 - precision: 0.6865 - precision-0.55: 0.7167 - precision-0.60: 0.7361 - precision-0.65: 0.7543 - precision-0.70: 0.7660 - precision-0.75: 0.7774 - precision-0.80: 0.7912 - precision-0.85: 0.7799 - precision-0.90: 0.8538 - precision-0.95: 0.8222 - recall: 0.4832 - recall-0.55: 0.3970 - recall-0.60: 0.3121 - recall-0.65: 0.2357 - recall-0.70: 0.1589 - recall-0.75: 0.1082 - recall-0.80: 0.0682 - recall-0.85: 0.0379 - recall-0.90: 0.0169 - recall-0.95: 0.0056 - val_loss: 1.1234 - val_tp: 512.0000 - val_fp: 588.0000 - val_tn: 2220.0000 - val_fn: 892.0000 - val_accuracy: 0.4409 - val_precision: 0.4655 - val_precision-0.55: 0.4729 - val_precision-0.60: 0.4945 - val_precision-0.65: 0.4938 - val_precision-0.70: 0.5083 - val_precision-0.75: 0.5000 - val_precision-0.80: 0.4981 - val_precision-0.85: 0.5055 - val_precision-0.90: 0.4701 - val_precision-0.95: 0.4630 - val_recall: 0.3647 - val_recall-0.55: 0.3048 - val_recall-0.60: 0.2557 - val_recall-0.65: 0.1994 - val_recall-0.70: 0.1738 - val_recall-0.75: 0.1318 - val_recall-0.80: 0.0919 - val_recall-0.85: 0.0655 - val_recall-0.90: 0.0392 - val_recall-0.95: 0.0178\n",
      "Epoch 5/200\n",
      "328/328 [==============================] - 2s 6ms/step - loss: 0.8887 - tp: 3301.0000 - fp: 1401.0000 - tn: 11703.0000 - fn: 3251.0000 - accuracy: 0.6390 - precision: 0.7020 - precision-0.55: 0.7323 - precision-0.60: 0.7516 - precision-0.65: 0.7714 - precision-0.70: 0.7874 - precision-0.75: 0.8067 - precision-0.80: 0.8202 - precision-0.85: 0.8212 - precision-0.90: 0.8211 - precision-0.95: 0.8065 - recall: 0.5038 - recall-0.55: 0.4255 - recall-0.60: 0.3330 - recall-0.65: 0.2492 - recall-0.70: 0.1741 - recall-0.75: 0.1134 - recall-0.80: 0.0668 - recall-0.85: 0.0343 - recall-0.90: 0.0154 - recall-0.95: 0.0038 - val_loss: 1.1241 - val_tp: 506.0000 - val_fp: 599.0000 - val_tn: 2209.0000 - val_fn: 898.0000 - val_accuracy: 0.4459 - val_precision: 0.4579 - val_precision-0.55: 0.4721 - val_precision-0.60: 0.4893 - val_precision-0.65: 0.4983 - val_precision-0.70: 0.5148 - val_precision-0.75: 0.5121 - val_precision-0.80: 0.4943 - val_precision-0.85: 0.4913 - val_precision-0.90: 0.5041 - val_precision-0.95: 0.3590 - val_recall: 0.3604 - val_recall-0.55: 0.3077 - val_recall-0.60: 0.2607 - val_recall-0.65: 0.2108 - val_recall-0.70: 0.1731 - val_recall-0.75: 0.1353 - val_recall-0.80: 0.0926 - val_recall-0.85: 0.0605 - val_recall-0.90: 0.0434 - val_recall-0.95: 0.0100\n",
      "Epoch 6/200\n",
      "328/328 [==============================] - 2s 7ms/step - loss: 0.8694 - tp: 3336.0000 - fp: 1341.0000 - tn: 11763.0000 - fn: 3216.0000 - accuracy: 0.6487 - precision: 0.7133 - precision-0.55: 0.7425 - precision-0.60: 0.7727 - precision-0.65: 0.7976 - precision-0.70: 0.8259 - precision-0.75: 0.8505 - precision-0.80: 0.8555 - precision-0.85: 0.8654 - precision-0.90: 0.8859 - precision-0.95: 0.8974 - recall: 0.5092 - recall-0.55: 0.4264 - recall-0.60: 0.3445 - recall-0.65: 0.2622 - recall-0.70: 0.1905 - recall-0.75: 0.1276 - recall-0.80: 0.0795 - recall-0.85: 0.0412 - recall-0.90: 0.0201 - recall-0.95: 0.0053 - val_loss: 1.1470 - val_tp: 502.0000 - val_fp: 611.0000 - val_tn: 2197.0000 - val_fn: 902.0000 - val_accuracy: 0.4288 - val_precision: 0.4510 - val_precision-0.55: 0.4555 - val_precision-0.60: 0.4753 - val_precision-0.65: 0.4850 - val_precision-0.70: 0.5100 - val_precision-0.75: 0.4949 - val_precision-0.80: 0.5034 - val_precision-0.85: 0.4929 - val_precision-0.90: 0.4715 - val_precision-0.95: 0.4681 - val_recall: 0.3575 - val_recall-0.55: 0.3027 - val_recall-0.60: 0.2607 - val_recall-0.65: 0.2187 - val_recall-0.70: 0.1816 - val_recall-0.75: 0.1389 - val_recall-0.80: 0.1054 - val_recall-0.85: 0.0741 - val_recall-0.90: 0.0413 - val_recall-0.95: 0.0157\n",
      "Epoch 7/200\n",
      "328/328 [==============================] - 2s 7ms/step - loss: 0.8442 - tp: 3499.0000 - fp: 1299.0000 - tn: 11805.0000 - fn: 3053.0000 - accuracy: 0.6644 - precision: 0.7293 - precision-0.55: 0.7584 - precision-0.60: 0.7881 - precision-0.65: 0.8071 - precision-0.70: 0.8428 - precision-0.75: 0.8635 - precision-0.80: 0.8734 - precision-0.85: 0.8896 - precision-0.90: 0.9324 - precision-0.95: 0.9268 - recall: 0.5340 - recall-0.55: 0.4527 - recall-0.60: 0.3695 - recall-0.65: 0.2816 - recall-0.70: 0.2062 - recall-0.75: 0.1371 - recall-0.80: 0.0832 - recall-0.85: 0.0455 - recall-0.90: 0.0211 - recall-0.95: 0.0058 - val_loss: 1.1430 - val_tp: 518.0000 - val_fp: 599.0000 - val_tn: 2209.0000 - val_fn: 886.0000 - val_accuracy: 0.4395 - val_precision: 0.4637 - val_precision-0.55: 0.4681 - val_precision-0.60: 0.4889 - val_precision-0.65: 0.5000 - val_precision-0.70: 0.5143 - val_precision-0.75: 0.5274 - val_precision-0.80: 0.5190 - val_precision-0.85: 0.5273 - val_precision-0.90: 0.4688 - val_precision-0.95: 0.4444 - val_recall: 0.3689 - val_recall-0.55: 0.3134 - val_recall-0.60: 0.2678 - val_recall-0.65: 0.2258 - val_recall-0.70: 0.1916 - val_recall-0.75: 0.1574 - val_recall-0.80: 0.1168 - val_recall-0.85: 0.0826 - val_recall-0.90: 0.0427 - val_recall-0.95: 0.0171\n",
      "Epoch 8/200\n",
      "328/328 [==============================] - 2s 7ms/step - loss: 0.8332 - tp: 3545.0000 - fp: 1265.0000 - tn: 11839.0000 - fn: 3007.0000 - accuracy: 0.6662 - precision: 0.7370 - precision-0.55: 0.7700 - precision-0.60: 0.7980 - precision-0.65: 0.8221 - precision-0.70: 0.8377 - precision-0.75: 0.8664 - precision-0.80: 0.8754 - precision-0.85: 0.8736 - precision-0.90: 0.8935 - precision-0.95: 0.9153 - recall: 0.5411 - recall-0.55: 0.4650 - recall-0.60: 0.3842 - recall-0.65: 0.2970 - recall-0.70: 0.2111 - recall-0.75: 0.1455 - recall-0.80: 0.0900 - recall-0.85: 0.0485 - recall-0.90: 0.0230 - recall-0.95: 0.0082 - val_loss: 1.1428 - val_tp: 527.0000 - val_fp: 613.0000 - val_tn: 2195.0000 - val_fn: 877.0000 - val_accuracy: 0.4380 - val_precision: 0.4623 - val_precision-0.55: 0.4735 - val_precision-0.60: 0.4925 - val_precision-0.65: 0.5146 - val_precision-0.70: 0.5197 - val_precision-0.75: 0.5310 - val_precision-0.80: 0.5138 - val_precision-0.85: 0.5157 - val_precision-0.90: 0.4889 - val_precision-0.95: 0.4762 - val_recall: 0.3754 - val_recall-0.55: 0.3248 - val_recall-0.60: 0.2806 - val_recall-0.65: 0.2379 - val_recall-0.70: 0.1973 - val_recall-0.75: 0.1588 - val_recall-0.80: 0.1189 - val_recall-0.85: 0.0819 - val_recall-0.90: 0.0470 - val_recall-0.95: 0.0142\n",
      "Epoch 8: early stopping\n",
      "44/44 [==============================] - 0s 2ms/step\n",
      "71/71 [==============================] - 0s 4ms/step - loss: 1.1650 - tp: 377.0000 - fp: 487.0000 - tn: 2321.0000 - fn: 1027.0000 - accuracy: 0.4103 - precision: 0.4363 - precision-0.55: 0.4569 - precision-0.60: 0.4840 - precision-0.65: 0.4704 - precision-0.70: 0.4429 - precision-0.75: 0.4222 - precision-0.80: 0.4167 - precision-0.85: 0.5135 - precision-0.90: 0.7647 - precision-0.95: 0.8182 - recall: 0.2685 - recall-0.55: 0.2001 - recall-0.60: 0.1396 - recall-0.65: 0.0848 - recall-0.70: 0.0442 - recall-0.75: 0.0271 - recall-0.80: 0.0178 - recall-0.85: 0.0135 - recall-0.90: 0.0093 - recall-0.95: 0.0064                                                      \n",
      "\n",
      "=============\n",
      "CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.51      0.54       673\n",
      "           1       0.26      0.43      0.33       342\n",
      "           2       0.34      0.22      0.26       389\n",
      "\n",
      "    accuracy                           0.41      1404\n",
      "   macro avg       0.39      0.39      0.38      1404\n",
      "weighted avg       0.43      0.41      0.41      1404\n",
      "\n",
      "\n",
      "=============\n",
      "CONFUSION MATRIX:\n",
      "       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2\n",
      "0      345  240   88    673  0.513  0.357  0.131\n",
      "1      121  147   74    342  0.354  0.430  0.216\n",
      "2      133  172   84    389  0.342  0.442  0.216\n",
      "Total  599  559  246   1404  0.427  0.398  0.175\n",
      "\n",
      ">>>>>> FOLD 3\n",
      "\n",
      "\n",
      "=============\n",
      "CLASSIFIER PARAMS:\n",
      "hu:100\n",
      "output_bias:[0.3017101443724601, -0.19830207600369315, -0.10340804263600366]\n",
      "loss:categorical_crossentropy\n",
      "dropout:True\n",
      "dropout_rate:0.3\n",
      "learning_rate:0.0001\n",
      "gpu:False\n",
      "set_class_weight:True\n",
      "save_check_point:False\n",
      "early_stopping:True\n",
      "patience:5\n",
      "epochs:200\n",
      "shuffle_when_train:False\n",
      "batch_size:20\n",
      "\n",
      "=============\n",
      "CLASSIFIER PARAMS:\n",
      "hu:100\n",
      "output_bias:[0.3017101443724601, -0.19830207600369315, -0.10340804263600366]\n",
      "loss:categorical_crossentropy\n",
      "dropout:True\n",
      "dropout_rate:0.3\n",
      "learning_rate:0.0001\n",
      "gpu:False\n",
      "set_class_weight:True\n",
      "save_check_point:False\n",
      "early_stopping:True\n",
      "patience:5\n",
      "epochs:200\n",
      "shuffle_when_train:False\n",
      "batch_size:20\n",
      "Epoch 1/200\n",
      "328/328 [==============================] - 7s 11ms/step - loss: 1.3274 - tp: 2161.0000 - fp: 3065.0000 - tn: 12847.0000 - fn: 5795.0000 - accuracy: 0.3993 - precision: 0.4135 - precision-0.55: 0.4227 - precision-0.60: 0.4329 - precision-0.65: 0.4411 - precision-0.70: 0.4430 - precision-0.75: 0.4566 - precision-0.80: 0.4510 - precision-0.85: 0.4735 - precision-0.90: 0.4886 - precision-0.95: 0.5324 - recall: 0.2716 - recall-0.55: 0.2132 - recall-0.60: 0.1630 - recall-0.65: 0.1210 - recall-0.70: 0.0841 - recall-0.75: 0.0595 - recall-0.80: 0.0387 - recall-0.85: 0.0269 - recall-0.90: 0.0162 - recall-0.95: 0.0093 - val_loss: 1.0116 - val_tp: 696.0000 - val_fp: 486.0000 - val_tn: 2322.0000 - val_fn: 708.0000 - val_accuracy: 0.5541 - val_precision: 0.5888 - val_precision-0.55: 0.6105 - val_precision-0.60: 0.6307 - val_precision-0.65: 0.6586 - val_precision-0.70: 0.6578 - val_precision-0.75: 0.6657 - val_precision-0.80: 0.6471 - val_precision-0.85: 0.7031 - val_precision-0.90: 0.3333 - val_precision-0.95: 0.0000e+00 - val_recall: 0.4957 - val_recall-0.55: 0.4566 - val_recall-0.60: 0.4038 - val_recall-0.65: 0.3504 - val_recall-0.70: 0.2642 - val_recall-0.75: 0.1688 - val_recall-0.80: 0.0862 - val_recall-0.85: 0.0321 - val_recall-0.90: 7.1225e-04 - val_recall-0.95: 0.0000e+00\n",
      "Epoch 2/200\n",
      "328/328 [==============================] - 2s 7ms/step - loss: 1.1155 - tp: 2050.0000 - fp: 1933.0000 - tn: 11171.0000 - fn: 4502.0000 - accuracy: 0.4791 - precision: 0.5147 - precision-0.55: 0.5222 - precision-0.60: 0.5293 - precision-0.65: 0.5437 - precision-0.70: 0.5524 - precision-0.75: 0.5270 - precision-0.80: 0.5461 - precision-0.85: 0.5502 - precision-0.90: 0.5843 - precision-0.95: 0.6275 - recall: 0.3129 - recall-0.55: 0.2382 - recall-0.60: 0.1737 - recall-0.65: 0.1264 - recall-0.70: 0.0868 - recall-0.75: 0.0536 - recall-0.80: 0.0362 - recall-0.85: 0.0226 - recall-0.90: 0.0159 - recall-0.95: 0.0098 - val_loss: 1.0240 - val_tp: 672.0000 - val_fp: 478.0000 - val_tn: 2330.0000 - val_fn: 732.0000 - val_accuracy: 0.5577 - val_precision: 0.5843 - val_precision-0.55: 0.5988 - val_precision-0.60: 0.6171 - val_precision-0.65: 0.6436 - val_precision-0.70: 0.6250 - val_precision-0.75: 0.6090 - val_precision-0.80: 0.5752 - val_precision-0.85: 0.5667 - val_precision-0.90: 0.2500 - val_precision-0.95: 0.0000e+00 - val_recall: 0.4786 - val_recall-0.55: 0.4359 - val_recall-0.60: 0.3754 - val_recall-0.65: 0.3048 - val_recall-0.70: 0.2101 - val_recall-0.75: 0.1254 - val_recall-0.80: 0.0463 - val_recall-0.85: 0.0121 - val_recall-0.90: 7.1225e-04 - val_recall-0.95: 0.0000e+00\n",
      "Epoch 3/200\n",
      "328/328 [==============================] - 2s 7ms/step - loss: 1.0485 - tp: 2044.0000 - fp: 1605.0000 - tn: 11499.0000 - fn: 4508.0000 - accuracy: 0.5067 - precision: 0.5602 - precision-0.55: 0.5810 - precision-0.60: 0.5998 - precision-0.65: 0.6069 - precision-0.70: 0.6156 - precision-0.75: 0.6220 - precision-0.80: 0.6357 - precision-0.85: 0.6316 - precision-0.90: 0.6429 - precision-0.95: 0.6579 - recall: 0.3120 - recall-0.55: 0.2265 - recall-0.60: 0.1610 - recall-0.65: 0.1044 - recall-0.70: 0.0687 - recall-0.75: 0.0432 - recall-0.80: 0.0272 - recall-0.85: 0.0165 - recall-0.90: 0.0110 - recall-0.95: 0.0076 - val_loss: 1.0252 - val_tp: 690.0000 - val_fp: 486.0000 - val_tn: 2322.0000 - val_fn: 714.0000 - val_accuracy: 0.5584 - val_precision: 0.5867 - val_precision-0.55: 0.6025 - val_precision-0.60: 0.6090 - val_precision-0.65: 0.6151 - val_precision-0.70: 0.6074 - val_precision-0.75: 0.5936 - val_precision-0.80: 0.5263 - val_precision-0.85: 0.5926 - val_precision-0.90: 0.3333 - val_precision-0.95: 0.0000e+00 - val_recall: 0.4915 - val_recall-0.55: 0.4395 - val_recall-0.60: 0.3682 - val_recall-0.65: 0.2835 - val_recall-0.70: 0.1994 - val_recall-0.75: 0.1061 - val_recall-0.80: 0.0356 - val_recall-0.85: 0.0114 - val_recall-0.90: 7.1225e-04 - val_recall-0.95: 0.0000e+00\n",
      "Epoch 4/200\n",
      "328/328 [==============================] - 2s 6ms/step - loss: 1.0016 - tp: 2216.0000 - fp: 1424.0000 - tn: 11680.0000 - fn: 4336.0000 - accuracy: 0.5333 - precision: 0.6088 - precision-0.55: 0.6242 - precision-0.60: 0.6450 - precision-0.65: 0.6609 - precision-0.70: 0.6889 - precision-0.75: 0.7166 - precision-0.80: 0.7279 - precision-0.85: 0.7273 - precision-0.90: 0.7786 - precision-0.95: 0.7867 - recall: 0.3382 - recall-0.55: 0.2454 - recall-0.60: 0.1714 - recall-0.65: 0.1157 - recall-0.70: 0.0740 - recall-0.75: 0.0475 - recall-0.80: 0.0314 - recall-0.85: 0.0208 - recall-0.90: 0.0156 - recall-0.95: 0.0090 - val_loss: 1.0302 - val_tp: 672.0000 - val_fp: 479.0000 - val_tn: 2329.0000 - val_fn: 732.0000 - val_accuracy: 0.5534 - val_precision: 0.5838 - val_precision-0.55: 0.5875 - val_precision-0.60: 0.6115 - val_precision-0.65: 0.6178 - val_precision-0.70: 0.6038 - val_precision-0.75: 0.5751 - val_precision-0.80: 0.5446 - val_precision-0.85: 0.6207 - val_precision-0.90: 0.2000 - val_precision-0.95: 0.0000e+00 - val_recall: 0.4786 - val_recall-0.55: 0.4281 - val_recall-0.60: 0.3789 - val_recall-0.65: 0.2913 - val_recall-0.70: 0.2051 - val_recall-0.75: 0.1118 - val_recall-0.80: 0.0434 - val_recall-0.85: 0.0128 - val_recall-0.90: 7.1225e-04 - val_recall-0.95: 0.0000e+00\n",
      "Epoch 5/200\n",
      "328/328 [==============================] - 2s 6ms/step - loss: 0.9775 - tp: 2232.0000 - fp: 1415.0000 - tn: 11689.0000 - fn: 4320.0000 - accuracy: 0.5527 - precision: 0.6120 - precision-0.55: 0.6464 - precision-0.60: 0.6829 - precision-0.65: 0.7106 - precision-0.70: 0.7147 - precision-0.75: 0.7413 - precision-0.80: 0.7473 - precision-0.85: 0.7288 - precision-0.90: 0.7355 - precision-0.95: 0.7183 - recall: 0.3407 - recall-0.55: 0.2570 - recall-0.60: 0.1844 - recall-0.65: 0.1233 - recall-0.70: 0.0772 - recall-0.75: 0.0485 - recall-0.80: 0.0321 - recall-0.85: 0.0197 - recall-0.90: 0.0136 - recall-0.95: 0.0078 - val_loss: 1.0492 - val_tp: 684.0000 - val_fp: 495.0000 - val_tn: 2313.0000 - val_fn: 720.0000 - val_accuracy: 0.5442 - val_precision: 0.5802 - val_precision-0.55: 0.5853 - val_precision-0.60: 0.5961 - val_precision-0.65: 0.6137 - val_precision-0.70: 0.6098 - val_precision-0.75: 0.5806 - val_precision-0.80: 0.5731 - val_precision-0.85: 0.5517 - val_precision-0.90: 0.3750 - val_precision-0.95: 0.0000e+00 - val_recall: 0.4872 - val_recall-0.55: 0.4373 - val_recall-0.60: 0.3889 - val_recall-0.65: 0.3248 - val_recall-0.70: 0.2472 - val_recall-0.75: 0.1538 - val_recall-0.80: 0.0698 - val_recall-0.85: 0.0228 - val_recall-0.90: 0.0021 - val_recall-0.95: 0.0000e+00\n",
      "Epoch 6/200\n",
      "328/328 [==============================] - 2s 7ms/step - loss: 0.9587 - tp: 2378.0000 - fp: 1381.0000 - tn: 11723.0000 - fn: 4174.0000 - accuracy: 0.5554 - precision: 0.6326 - precision-0.55: 0.6636 - precision-0.60: 0.6943 - precision-0.65: 0.7197 - precision-0.70: 0.7322 - precision-0.75: 0.7451 - precision-0.80: 0.7764 - precision-0.85: 0.8310 - precision-0.90: 0.8527 - precision-0.95: 0.8025 - recall: 0.3629 - recall-0.55: 0.2761 - recall-0.60: 0.1996 - recall-0.65: 0.1297 - recall-0.70: 0.0847 - recall-0.75: 0.0580 - recall-0.80: 0.0382 - recall-0.85: 0.0270 - recall-0.90: 0.0168 - recall-0.95: 0.0099 - val_loss: 1.0645 - val_tp: 681.0000 - val_fp: 503.0000 - val_tn: 2305.0000 - val_fn: 723.0000 - val_accuracy: 0.5513 - val_precision: 0.5752 - val_precision-0.55: 0.5858 - val_precision-0.60: 0.5932 - val_precision-0.65: 0.5917 - val_precision-0.70: 0.6024 - val_precision-0.75: 0.5545 - val_precision-0.80: 0.5177 - val_precision-0.85: 0.5233 - val_precision-0.90: 0.5625 - val_precision-0.95: 0.0000e+00 - val_recall: 0.4850 - val_recall-0.55: 0.4473 - val_recall-0.60: 0.3967 - val_recall-0.65: 0.3333 - val_recall-0.70: 0.2536 - val_recall-0.75: 0.1595 - val_recall-0.80: 0.0833 - val_recall-0.85: 0.0321 - val_recall-0.90: 0.0064 - val_recall-0.95: 0.0000e+00\n",
      "Epoch 6: early stopping\n",
      "44/44 [==============================] - 0s 3ms/step\n",
      "71/71 [==============================] - 0s 4ms/step - loss: 1.0184 - tp: 748.0000 - fp: 498.0000 - tn: 2310.0000 - fn: 656.0000 - accuracy: 0.5805 - precision: 0.6003 - precision-0.55: 0.6152 - precision-0.60: 0.6224 - precision-0.65: 0.6263 - precision-0.70: 0.6429 - precision-0.75: 0.6615 - precision-0.80: 0.6816 - precision-0.85: 0.6338 - precision-0.90: 0.5000 - precision-0.95: 0.0000e+00 - recall: 0.5328 - recall-0.55: 0.4907 - recall-0.60: 0.4309 - recall-0.65: 0.3568 - recall-0.70: 0.2821 - recall-0.75: 0.1809 - recall-0.80: 0.0976 - recall-0.85: 0.0321 - recall-0.90: 0.0036 - recall-0.95: 0.0000e+00      \n",
      "\n",
      "=============\n",
      "CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.94      0.74       795\n",
      "           1       0.39      0.09      0.15       333\n",
      "           2       0.36      0.14      0.20       276\n",
      "\n",
      "    accuracy                           0.58      1404\n",
      "   macro avg       0.45      0.39      0.36      1404\n",
      "weighted avg       0.51      0.58      0.49      1404\n",
      "\n",
      "\n",
      "=============\n",
      "CONFUSION MATRIX:\n",
      "        P-0  P-1  P-2  Total   RP-0   RP-1   RP-2\n",
      "0       746   17   32    795  0.938  0.021  0.040\n",
      "1       267   31   35    333  0.802  0.093  0.105\n",
      "2       206   32   38    276  0.746  0.116  0.138\n",
      "Total  1219   80  105   1404  0.868  0.057  0.075\n",
      "\n",
      ">>>>>> FOLD 4\n",
      "\n",
      "\n",
      "=============\n",
      "CLASSIFIER PARAMS:\n",
      "hu:100\n",
      "output_bias:[-0.8389041826858622, 0.4189291699561003, 0.4199750128274604]\n",
      "loss:categorical_crossentropy\n",
      "dropout:True\n",
      "dropout_rate:0.3\n",
      "learning_rate:0.0001\n",
      "gpu:False\n",
      "set_class_weight:True\n",
      "save_check_point:False\n",
      "early_stopping:True\n",
      "patience:5\n",
      "epochs:200\n",
      "shuffle_when_train:False\n",
      "batch_size:20\n",
      "\n",
      "=============\n",
      "CLASSIFIER PARAMS:\n",
      "hu:100\n",
      "output_bias:[-0.8389041826858622, 0.4189291699561003, 0.4199750128274604]\n",
      "loss:categorical_crossentropy\n",
      "dropout:True\n",
      "dropout_rate:0.3\n",
      "learning_rate:0.0001\n",
      "gpu:False\n",
      "set_class_weight:True\n",
      "save_check_point:False\n",
      "early_stopping:True\n",
      "patience:5\n",
      "epochs:200\n",
      "shuffle_when_train:False\n",
      "batch_size:20\n",
      "Epoch 1/200\n",
      "328/328 [==============================] - 7s 11ms/step - loss: 1.2587 - tp: 2959.0000 - fp: 3439.0000 - tn: 12473.0000 - fn: 4997.0000 - accuracy: 0.4467 - precision: 0.4625 - precision-0.55: 0.4745 - precision-0.60: 0.4797 - precision-0.65: 0.4814 - precision-0.70: 0.4922 - precision-0.75: 0.4911 - precision-0.80: 0.4919 - precision-0.85: 0.4790 - precision-0.90: 0.4764 - precision-0.95: 0.4602 - recall: 0.3719 - recall-0.55: 0.3245 - recall-0.60: 0.2749 - recall-0.65: 0.2279 - recall-0.70: 0.1825 - recall-0.75: 0.1356 - recall-0.80: 0.0953 - recall-0.85: 0.0602 - recall-0.90: 0.0356 - recall-0.95: 0.0167 - val_loss: 1.2731 - val_tp: 306.0000 - val_fp: 566.0000 - val_tn: 2242.0000 - val_fn: 1098.0000 - val_accuracy: 0.3568 - val_precision: 0.3509 - val_precision-0.55: 0.3453 - val_precision-0.60: 0.3366 - val_precision-0.65: 0.3234 - val_precision-0.70: 0.3261 - val_precision-0.75: 0.3093 - val_precision-0.80: 0.2810 - val_precision-0.85: 0.2881 - val_precision-0.90: 0.2258 - val_precision-0.95: 0.0000e+00 - val_recall: 0.2179 - val_recall-0.55: 0.1645 - val_recall-0.60: 0.1218 - val_recall-0.65: 0.0848 - val_recall-0.70: 0.0641 - val_recall-0.75: 0.0427 - val_recall-0.80: 0.0242 - val_recall-0.85: 0.0121 - val_recall-0.90: 0.0050 - val_recall-0.95: 0.0000e+00\n",
      "Epoch 2/200\n",
      "328/328 [==============================] - 2s 6ms/step - loss: 1.0477 - tp: 1986.0000 - fp: 2438.0000 - tn: 10666.0000 - fn: 4566.0000 - accuracy: 0.4321 - precision: 0.4489 - precision-0.55: 0.4468 - precision-0.60: 0.4470 - precision-0.65: 0.4495 - precision-0.70: 0.4451 - precision-0.75: 0.4400 - precision-0.80: 0.4394 - precision-0.85: 0.4419 - precision-0.90: 0.4270 - precision-0.95: 0.5167 - recall: 0.3031 - recall-0.55: 0.2295 - recall-0.60: 0.1757 - recall-0.65: 0.1311 - recall-0.70: 0.0940 - recall-0.75: 0.0638 - recall-0.80: 0.0432 - recall-0.85: 0.0267 - recall-0.90: 0.0121 - recall-0.95: 0.0047 - val_loss: 1.2209 - val_tp: 271.0000 - val_fp: 498.0000 - val_tn: 2310.0000 - val_fn: 1133.0000 - val_accuracy: 0.3625 - val_precision: 0.3524 - val_precision-0.55: 0.3497 - val_precision-0.60: 0.3478 - val_precision-0.65: 0.3088 - val_precision-0.70: 0.2882 - val_precision-0.75: 0.3017 - val_precision-0.80: 0.2500 - val_precision-0.85: 0.2250 - val_precision-0.90: 0.1818 - val_precision-0.95: 0.0000e+00 - val_recall: 0.1930 - val_recall-0.55: 0.1425 - val_recall-0.60: 0.1026 - val_recall-0.65: 0.0598 - val_recall-0.70: 0.0349 - val_recall-0.75: 0.0249 - val_recall-0.80: 0.0128 - val_recall-0.85: 0.0064 - val_recall-0.90: 0.0014 - val_recall-0.95: 0.0000e+00\n",
      "Epoch 3/200\n",
      "328/328 [==============================] - 2s 7ms/step - loss: 0.9750 - tp: 2066.0000 - fp: 2201.0000 - tn: 10903.0000 - fn: 4486.0000 - accuracy: 0.4625 - precision: 0.4842 - precision-0.55: 0.4932 - precision-0.60: 0.5045 - precision-0.65: 0.5053 - precision-0.70: 0.5027 - precision-0.75: 0.5084 - precision-0.80: 0.4958 - precision-0.85: 0.5019 - precision-0.90: 0.4425 - precision-0.95: 0.3548 - recall: 0.3153 - recall-0.55: 0.2366 - recall-0.60: 0.1714 - recall-0.65: 0.1233 - recall-0.70: 0.0865 - recall-0.75: 0.0600 - recall-0.80: 0.0360 - recall-0.85: 0.0201 - recall-0.90: 0.0076 - recall-0.95: 0.0017 - val_loss: 1.2022 - val_tp: 245.0000 - val_fp: 462.0000 - val_tn: 2346.0000 - val_fn: 1159.0000 - val_accuracy: 0.3526 - val_precision: 0.3465 - val_precision-0.55: 0.3429 - val_precision-0.60: 0.3054 - val_precision-0.65: 0.2818 - val_precision-0.70: 0.2808 - val_precision-0.75: 0.2174 - val_precision-0.80: 0.2909 - val_precision-0.85: 0.2759 - val_precision-0.90: 0.0000e+00 - val_precision-0.95: 0.0000e+00 - val_recall: 0.1745 - val_recall-0.55: 0.1189 - val_recall-0.60: 0.0726 - val_recall-0.65: 0.0442 - val_recall-0.70: 0.0292 - val_recall-0.75: 0.0142 - val_recall-0.80: 0.0114 - val_recall-0.85: 0.0057 - val_recall-0.90: 0.0000e+00 - val_recall-0.95: 0.0000e+00\n",
      "Epoch 4/200\n",
      "328/328 [==============================] - 2s 7ms/step - loss: 0.9507 - tp: 2130.0000 - fp: 2041.0000 - tn: 11063.0000 - fn: 4422.0000 - accuracy: 0.4786 - precision: 0.5107 - precision-0.55: 0.5160 - precision-0.60: 0.5233 - precision-0.65: 0.5312 - precision-0.70: 0.5260 - precision-0.75: 0.5259 - precision-0.80: 0.5190 - precision-0.85: 0.5043 - precision-0.90: 0.4592 - precision-0.95: 0.3889 - recall: 0.3251 - recall-0.55: 0.2386 - recall-0.60: 0.1712 - recall-0.65: 0.1219 - recall-0.70: 0.0835 - recall-0.75: 0.0542 - recall-0.80: 0.0333 - recall-0.85: 0.0179 - recall-0.90: 0.0069 - recall-0.95: 0.0011 - val_loss: 1.1976 - val_tp: 239.0000 - val_fp: 458.0000 - val_tn: 2350.0000 - val_fn: 1165.0000 - val_accuracy: 0.3575 - val_precision: 0.3429 - val_precision-0.55: 0.3388 - val_precision-0.60: 0.3084 - val_precision-0.65: 0.3032 - val_precision-0.70: 0.2752 - val_precision-0.75: 0.2414 - val_precision-0.80: 0.2264 - val_precision-0.85: 0.2083 - val_precision-0.90: 0.0000e+00 - val_precision-0.95: 0.0000e+00 - val_recall: 0.1702 - val_recall-0.55: 0.1175 - val_recall-0.60: 0.0734 - val_recall-0.65: 0.0477 - val_recall-0.70: 0.0292 - val_recall-0.75: 0.0150 - val_recall-0.80: 0.0085 - val_recall-0.85: 0.0036 - val_recall-0.90: 0.0000e+00 - val_recall-0.95: 0.0000e+00\n",
      "Epoch 5/200\n",
      "328/328 [==============================] - 2s 7ms/step - loss: 0.9148 - tp: 2261.0000 - fp: 1953.0000 - tn: 11151.0000 - fn: 4291.0000 - accuracy: 0.5009 - precision: 0.5365 - precision-0.55: 0.5565 - precision-0.60: 0.5667 - precision-0.65: 0.5568 - precision-0.70: 0.5663 - precision-0.75: 0.5731 - precision-0.80: 0.6060 - precision-0.85: 0.5830 - precision-0.90: 0.5306 - precision-0.95: 0.6000 - recall: 0.3451 - recall-0.55: 0.2595 - recall-0.60: 0.1873 - recall-0.65: 0.1258 - recall-0.70: 0.0861 - recall-0.75: 0.0592 - recall-0.80: 0.0401 - recall-0.85: 0.0198 - recall-0.90: 0.0079 - recall-0.95: 0.0014 - val_loss: 1.2048 - val_tp: 246.0000 - val_fp: 476.0000 - val_tn: 2332.0000 - val_fn: 1158.0000 - val_accuracy: 0.3554 - val_precision: 0.3407 - val_precision-0.55: 0.3281 - val_precision-0.60: 0.3249 - val_precision-0.65: 0.3175 - val_precision-0.70: 0.2812 - val_precision-0.75: 0.2642 - val_precision-0.80: 0.2540 - val_precision-0.85: 0.2759 - val_precision-0.90: 0.1000 - val_precision-0.95: 0.0000e+00 - val_recall: 0.1752 - val_recall-0.55: 0.1197 - val_recall-0.60: 0.0819 - val_recall-0.65: 0.0570 - val_recall-0.70: 0.0321 - val_recall-0.75: 0.0199 - val_recall-0.80: 0.0114 - val_recall-0.85: 0.0057 - val_recall-0.90: 7.1225e-04 - val_recall-0.95: 0.0000e+00\n",
      "Epoch 6/200\n",
      "328/328 [==============================] - 2s 7ms/step - loss: 0.9011 - tp: 2362.0000 - fp: 1920.0000 - tn: 11184.0000 - fn: 4190.0000 - accuracy: 0.5185 - precision: 0.5516 - precision-0.55: 0.5677 - precision-0.60: 0.5719 - precision-0.65: 0.5787 - precision-0.70: 0.5827 - precision-0.75: 0.5840 - precision-0.80: 0.5711 - precision-0.85: 0.5863 - precision-0.90: 0.5741 - precision-0.95: 0.5000 - recall: 0.3605 - recall-0.55: 0.2724 - recall-0.60: 0.1984 - recall-0.65: 0.1392 - recall-0.70: 0.0957 - recall-0.75: 0.0636 - recall-0.80: 0.0392 - recall-0.85: 0.0223 - recall-0.90: 0.0095 - recall-0.95: 0.0017 - val_loss: 1.1992 - val_tp: 239.0000 - val_fp: 453.0000 - val_tn: 2355.0000 - val_fn: 1165.0000 - val_accuracy: 0.3504 - val_precision: 0.3454 - val_precision-0.55: 0.3465 - val_precision-0.60: 0.3392 - val_precision-0.65: 0.3188 - val_precision-0.70: 0.2756 - val_precision-0.75: 0.2800 - val_precision-0.80: 0.2656 - val_precision-0.85: 0.2069 - val_precision-0.90: 0.0000e+00 - val_precision-0.95: 0.0000e+00 - val_recall: 0.1702 - val_recall-0.55: 0.1246 - val_recall-0.60: 0.0826 - val_recall-0.65: 0.0520 - val_recall-0.70: 0.0306 - val_recall-0.75: 0.0199 - val_recall-0.80: 0.0121 - val_recall-0.85: 0.0043 - val_recall-0.90: 0.0000e+00 - val_recall-0.95: 0.0000e+00\n",
      "Epoch 7/200\n",
      "328/328 [==============================] - 2s 7ms/step - loss: 0.8759 - tp: 2461.0000 - fp: 1874.0000 - tn: 11230.0000 - fn: 4091.0000 - accuracy: 0.5322 - precision: 0.5677 - precision-0.55: 0.5812 - precision-0.60: 0.5996 - precision-0.65: 0.6243 - precision-0.70: 0.6325 - precision-0.75: 0.6397 - precision-0.80: 0.6419 - precision-0.85: 0.6454 - precision-0.90: 0.7041 - precision-0.95: 0.8000 - recall: 0.3756 - recall-0.55: 0.2836 - recall-0.60: 0.2089 - recall-0.65: 0.1506 - recall-0.70: 0.1027 - recall-0.75: 0.0699 - recall-0.80: 0.0421 - recall-0.85: 0.0247 - recall-0.90: 0.0105 - recall-0.95: 0.0018 - val_loss: 1.2059 - val_tp: 253.0000 - val_fp: 484.0000 - val_tn: 2324.0000 - val_fn: 1151.0000 - val_accuracy: 0.3526 - val_precision: 0.3433 - val_precision-0.55: 0.3340 - val_precision-0.60: 0.3273 - val_precision-0.65: 0.3125 - val_precision-0.70: 0.2798 - val_precision-0.75: 0.2800 - val_precision-0.80: 0.2982 - val_precision-0.85: 0.2258 - val_precision-0.90: 0.0000e+00 - val_precision-0.95: 0.0000e+00 - val_recall: 0.1802 - val_recall-0.55: 0.1282 - val_recall-0.60: 0.0897 - val_recall-0.65: 0.0570 - val_recall-0.70: 0.0335 - val_recall-0.75: 0.0199 - val_recall-0.80: 0.0121 - val_recall-0.85: 0.0050 - val_recall-0.90: 0.0000e+00 - val_recall-0.95: 0.0000e+00\n",
      "Epoch 8/200\n",
      "328/328 [==============================] - 2s 7ms/step - loss: 0.8591 - tp: 2549.0000 - fp: 1801.0000 - tn: 11303.0000 - fn: 4003.0000 - accuracy: 0.5459 - precision: 0.5860 - precision-0.55: 0.6031 - precision-0.60: 0.6207 - precision-0.65: 0.6407 - precision-0.70: 0.6570 - precision-0.75: 0.6612 - precision-0.80: 0.6620 - precision-0.85: 0.6855 - precision-0.90: 0.6807 - precision-0.95: 0.5200 - recall: 0.3890 - recall-0.55: 0.2946 - recall-0.60: 0.2201 - recall-0.65: 0.1609 - recall-0.70: 0.1143 - recall-0.75: 0.0739 - recall-0.80: 0.0433 - recall-0.85: 0.0259 - recall-0.90: 0.0124 - recall-0.95: 0.0020 - val_loss: 1.2044 - val_tp: 286.0000 - val_fp: 505.0000 - val_tn: 2303.0000 - val_fn: 1118.0000 - val_accuracy: 0.3526 - val_precision: 0.3616 - val_precision-0.55: 0.3533 - val_precision-0.60: 0.3333 - val_precision-0.65: 0.3514 - val_precision-0.70: 0.3135 - val_precision-0.75: 0.2743 - val_precision-0.80: 0.3279 - val_precision-0.85: 0.2778 - val_precision-0.90: 0.0000e+00 - val_precision-0.95: 0.0000e+00 - val_recall: 0.2037 - val_recall-0.55: 0.1432 - val_recall-0.60: 0.0933 - val_recall-0.65: 0.0691 - val_recall-0.70: 0.0413 - val_recall-0.75: 0.0221 - val_recall-0.80: 0.0142 - val_recall-0.85: 0.0071 - val_recall-0.90: 0.0000e+00 - val_recall-0.95: 0.0000e+00\n",
      "Epoch 9/200\n",
      "328/328 [==============================] - 2s 7ms/step - loss: 0.8416 - tp: 2643.0000 - fp: 1839.0000 - tn: 11265.0000 - fn: 3909.0000 - accuracy: 0.5514 - precision: 0.5897 - precision-0.55: 0.6094 - precision-0.60: 0.6258 - precision-0.65: 0.6459 - precision-0.70: 0.6642 - precision-0.75: 0.6691 - precision-0.80: 0.6916 - precision-0.85: 0.7428 - precision-0.90: 0.8000 - precision-0.95: 0.8235 - recall: 0.4034 - recall-0.55: 0.3153 - recall-0.60: 0.2376 - recall-0.65: 0.1723 - recall-0.70: 0.1241 - recall-0.75: 0.0827 - recall-0.80: 0.0565 - recall-0.85: 0.0353 - recall-0.90: 0.0171 - recall-0.95: 0.0043 - val_loss: 1.2072 - val_tp: 280.0000 - val_fp: 500.0000 - val_tn: 2308.0000 - val_fn: 1124.0000 - val_accuracy: 0.3540 - val_precision: 0.3590 - val_precision-0.55: 0.3429 - val_precision-0.60: 0.3292 - val_precision-0.65: 0.3139 - val_precision-0.70: 0.2890 - val_precision-0.75: 0.3056 - val_precision-0.80: 0.3030 - val_precision-0.85: 0.3103 - val_precision-0.90: 0.1667 - val_precision-0.95: 0.0000e+00 - val_recall: 0.1994 - val_recall-0.55: 0.1368 - val_recall-0.60: 0.0947 - val_recall-0.65: 0.0613 - val_recall-0.70: 0.0356 - val_recall-0.75: 0.0235 - val_recall-0.80: 0.0142 - val_recall-0.85: 0.0064 - val_recall-0.90: 7.1225e-04 - val_recall-0.95: 0.0000e+00\n",
      "Epoch 9: early stopping\n",
      "44/44 [==============================] - 0s 3ms/step\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 1.2418 - tp: 229.0000 - fp: 487.0000 - tn: 2321.0000 - fn: 1175.0000 - accuracy: 0.3283 - precision: 0.3198 - precision-0.55: 0.3159 - precision-0.60: 0.2932 - precision-0.65: 0.2952 - precision-0.70: 0.3814 - precision-0.75: 0.3768 - precision-0.80: 0.3333 - precision-0.85: 0.4667 - precision-0.90: 0.5000 - precision-0.95: 1.0000 - recall: 0.1631 - recall-0.55: 0.1118 - recall-0.60: 0.0677 - recall-0.65: 0.0442 - recall-0.70: 0.0321 - recall-0.75: 0.0185 - recall-0.80: 0.0078 - recall-0.85: 0.0050 - recall-0.90: 7.1225e-04 - recall-0.95: 7.1225e-04                            \n",
      "\n",
      "=============\n",
      "CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.44      0.34       366\n",
      "           1       0.33      0.25      0.29       483\n",
      "           2       0.38      0.32      0.35       555\n",
      "\n",
      "    accuracy                           0.33      1404\n",
      "   macro avg       0.33      0.34      0.33      1404\n",
      "weighted avg       0.34      0.33      0.33      1404\n",
      "\n",
      "\n",
      "=============\n",
      "CONFUSION MATRIX:\n",
      "       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2\n",
      "0      160   89  117    366  0.437  0.243  0.320\n",
      "1      191  122  170    483  0.395  0.253  0.352\n",
      "2      214  162  179    555  0.386  0.292  0.323\n",
      "Total  565  373  466   1404  0.402  0.266  0.332\n",
      "\n",
      ">>>>>> FOLD 5\n",
      "\n",
      "\n",
      "=============\n",
      "CLASSIFIER PARAMS:\n",
      "hu:100\n",
      "output_bias:[-0.06055337688273361, -0.02043338309330824, 0.08098679535259519]\n",
      "loss:categorical_crossentropy\n",
      "dropout:True\n",
      "dropout_rate:0.3\n",
      "learning_rate:0.0001\n",
      "gpu:False\n",
      "set_class_weight:True\n",
      "save_check_point:False\n",
      "early_stopping:True\n",
      "patience:5\n",
      "epochs:200\n",
      "shuffle_when_train:False\n",
      "batch_size:20\n",
      "\n",
      "=============\n",
      "CLASSIFIER PARAMS:\n",
      "hu:100\n",
      "output_bias:[-0.06055337688273361, -0.02043338309330824, 0.08098679535259519]\n",
      "loss:categorical_crossentropy\n",
      "dropout:True\n",
      "dropout_rate:0.3\n",
      "learning_rate:0.0001\n",
      "gpu:False\n",
      "set_class_weight:True\n",
      "save_check_point:False\n",
      "early_stopping:True\n",
      "patience:5\n",
      "epochs:200\n",
      "shuffle_when_train:False\n",
      "batch_size:20\n",
      "Epoch 1/200\n",
      "328/328 [==============================] - 7s 13ms/step - loss: 1.3388 - tp: 1899.0000 - fp: 3226.0000 - tn: 12686.0000 - fn: 6057.0000 - accuracy: 0.3630 - precision: 0.3705 - precision-0.55: 0.3728 - precision-0.60: 0.3649 - precision-0.65: 0.3629 - precision-0.70: 0.3639 - precision-0.75: 0.3721 - precision-0.80: 0.3551 - precision-0.85: 0.3677 - precision-0.90: 0.3670 - precision-0.95: 0.4032 - recall: 0.2387 - recall-0.55: 0.1855 - recall-0.60: 0.1376 - recall-0.65: 0.1022 - recall-0.70: 0.0749 - recall-0.75: 0.0556 - recall-0.80: 0.0362 - recall-0.85: 0.0238 - recall-0.90: 0.0137 - recall-0.95: 0.0063 - val_loss: 1.0738 - val_tp: 457.0000 - val_fp: 368.0000 - val_tn: 2440.0000 - val_fn: 947.0000 - val_accuracy: 0.4907 - val_precision: 0.5539 - val_precision-0.55: 0.5753 - val_precision-0.60: 0.5773 - val_precision-0.65: 0.5979 - val_precision-0.70: 0.6294 - val_precision-0.75: 0.5652 - val_precision-0.80: 0.5000 - val_precision-0.85: 0.2353 - val_precision-0.90: 0.0000e+00 - val_precision-0.95: 0.0000e+00 - val_recall: 0.3255 - val_recall-0.55: 0.2557 - val_recall-0.60: 0.1887 - val_recall-0.65: 0.1218 - val_recall-0.70: 0.0762 - val_recall-0.75: 0.0370 - val_recall-0.80: 0.0150 - val_recall-0.85: 0.0028 - val_recall-0.90: 0.0000e+00 - val_recall-0.95: 0.0000e+00\n",
      "Epoch 2/200\n",
      "328/328 [==============================] - 2s 7ms/step - loss: 1.1526 - tp: 1552.0000 - fp: 2071.0000 - tn: 11033.0000 - fn: 5000.0000 - accuracy: 0.4008 - precision: 0.4284 - precision-0.55: 0.4414 - precision-0.60: 0.4514 - precision-0.65: 0.4598 - precision-0.70: 0.4810 - precision-0.75: 0.4991 - precision-0.80: 0.5331 - precision-0.85: 0.5228 - precision-0.90: 0.4957 - precision-0.95: 0.5366 - recall: 0.2369 - recall-0.55: 0.1770 - recall-0.60: 0.1247 - recall-0.65: 0.0855 - recall-0.70: 0.0618 - recall-0.75: 0.0424 - recall-0.80: 0.0282 - recall-0.85: 0.0157 - recall-0.90: 0.0089 - recall-0.95: 0.0034 - val_loss: 1.0510 - val_tp: 457.0000 - val_fp: 359.0000 - val_tn: 2449.0000 - val_fn: 947.0000 - val_accuracy: 0.4950 - val_precision: 0.5600 - val_precision-0.55: 0.5856 - val_precision-0.60: 0.6019 - val_precision-0.65: 0.5885 - val_precision-0.70: 0.6284 - val_precision-0.75: 0.5238 - val_precision-0.80: 0.3913 - val_precision-0.85: 0.4000 - val_precision-0.90: 0.0000e+00 - val_precision-0.95: 0.0000e+00 - val_recall: 0.3255 - val_recall-0.55: 0.2486 - val_recall-0.60: 0.1766 - val_recall-0.65: 0.1090 - val_recall-0.70: 0.0662 - val_recall-0.75: 0.0235 - val_recall-0.80: 0.0064 - val_recall-0.85: 0.0028 - val_recall-0.90: 0.0000e+00 - val_recall-0.95: 0.0000e+00\n",
      "Epoch 3/200\n",
      "328/328 [==============================] - 2s 7ms/step - loss: 1.0745 - tp: 1560.0000 - fp: 1631.0000 - tn: 11473.0000 - fn: 4992.0000 - accuracy: 0.4429 - precision: 0.4889 - precision-0.55: 0.5116 - precision-0.60: 0.5489 - precision-0.65: 0.5683 - precision-0.70: 0.5958 - precision-0.75: 0.6010 - precision-0.80: 0.6186 - precision-0.85: 0.5645 - precision-0.90: 0.5593 - precision-0.95: 0.7333 - recall: 0.2381 - recall-0.55: 0.1711 - recall-0.60: 0.1233 - recall-0.65: 0.0826 - recall-0.70: 0.0560 - recall-0.75: 0.0368 - recall-0.80: 0.0203 - recall-0.85: 0.0107 - recall-0.90: 0.0050 - recall-0.95: 0.0017 - val_loss: 1.0665 - val_tp: 459.0000 - val_fp: 372.0000 - val_tn: 2436.0000 - val_fn: 945.0000 - val_accuracy: 0.4815 - val_precision: 0.5523 - val_precision-0.55: 0.5630 - val_precision-0.60: 0.5751 - val_precision-0.65: 0.5803 - val_precision-0.70: 0.5786 - val_precision-0.75: 0.5301 - val_precision-0.80: 0.4359 - val_precision-0.85: 0.2000 - val_precision-0.90: 0.0000e+00 - val_precision-0.95: 0.0000e+00 - val_recall: 0.3269 - val_recall-0.55: 0.2386 - val_recall-0.60: 0.1745 - val_recall-0.65: 0.1132 - val_recall-0.70: 0.0655 - val_recall-0.75: 0.0313 - val_recall-0.80: 0.0121 - val_recall-0.85: 0.0021 - val_recall-0.90: 0.0000e+00 - val_recall-0.95: 0.0000e+00\n",
      "Epoch 4/200\n",
      "328/328 [==============================] - 2s 7ms/step - loss: 1.0309 - tp: 1600.0000 - fp: 1421.0000 - tn: 11683.0000 - fn: 4952.0000 - accuracy: 0.4654 - precision: 0.5296 - precision-0.55: 0.5632 - precision-0.60: 0.5935 - precision-0.65: 0.6360 - precision-0.70: 0.6365 - precision-0.75: 0.6433 - precision-0.80: 0.6510 - precision-0.85: 0.6381 - precision-0.90: 0.6389 - precision-0.95: 0.7778 - recall: 0.2442 - recall-0.55: 0.1754 - recall-0.60: 0.1235 - recall-0.65: 0.0875 - recall-0.70: 0.0548 - recall-0.75: 0.0322 - recall-0.80: 0.0191 - recall-0.85: 0.0102 - recall-0.90: 0.0035 - recall-0.95: 0.0011 - val_loss: 1.0653 - val_tp: 459.0000 - val_fp: 376.0000 - val_tn: 2432.0000 - val_fn: 945.0000 - val_accuracy: 0.4879 - val_precision: 0.5497 - val_precision-0.55: 0.5679 - val_precision-0.60: 0.5967 - val_precision-0.65: 0.6250 - val_precision-0.70: 0.6102 - val_precision-0.75: 0.5625 - val_precision-0.80: 0.4375 - val_precision-0.85: 0.2000 - val_precision-0.90: 0.0000e+00 - val_precision-0.95: 0.0000e+00 - val_recall: 0.3269 - val_recall-0.55: 0.2443 - val_recall-0.60: 0.1781 - val_recall-0.65: 0.1282 - val_recall-0.70: 0.0769 - val_recall-0.75: 0.0385 - val_recall-0.80: 0.0150 - val_recall-0.85: 0.0021 - val_recall-0.90: 0.0000e+00 - val_recall-0.95: 0.0000e+00\n",
      "Epoch 5/200\n",
      "328/328 [==============================] - 2s 6ms/step - loss: 0.9992 - tp: 1694.0000 - fp: 1352.0000 - tn: 11752.0000 - fn: 4858.0000 - accuracy: 0.4866 - precision: 0.5561 - precision-0.55: 0.5854 - precision-0.60: 0.6249 - precision-0.65: 0.6712 - precision-0.70: 0.7077 - precision-0.75: 0.7515 - precision-0.80: 0.7542 - precision-0.85: 0.7531 - precision-0.90: 0.7500 - precision-0.95: 0.5000 - recall: 0.2585 - recall-0.55: 0.1810 - recall-0.60: 0.1302 - recall-0.65: 0.0910 - recall-0.70: 0.0614 - recall-0.75: 0.0383 - recall-0.80: 0.0206 - recall-0.85: 0.0093 - recall-0.90: 0.0037 - recall-0.95: 3.0525e-04 - val_loss: 1.0721 - val_tp: 464.0000 - val_fp: 384.0000 - val_tn: 2424.0000 - val_fn: 940.0000 - val_accuracy: 0.4858 - val_precision: 0.5472 - val_precision-0.55: 0.5660 - val_precision-0.60: 0.5830 - val_precision-0.65: 0.6113 - val_precision-0.70: 0.5938 - val_precision-0.75: 0.5505 - val_precision-0.80: 0.5152 - val_precision-0.85: 0.3103 - val_precision-0.90: 0.0769 - val_precision-0.95: 0.0000e+00 - val_recall: 0.3305 - val_recall-0.55: 0.2564 - val_recall-0.60: 0.1852 - val_recall-0.65: 0.1311 - val_recall-0.70: 0.0812 - val_recall-0.75: 0.0427 - val_recall-0.80: 0.0242 - val_recall-0.85: 0.0064 - val_recall-0.90: 7.1225e-04 - val_recall-0.95: 0.0000e+00\n",
      "Epoch 6/200\n",
      "328/328 [==============================] - 2s 7ms/step - loss: 0.9804 - tp: 1805.0000 - fp: 1196.0000 - tn: 11908.0000 - fn: 4747.0000 - accuracy: 0.5021 - precision: 0.6015 - precision-0.55: 0.6369 - precision-0.60: 0.6691 - precision-0.65: 0.7072 - precision-0.70: 0.7432 - precision-0.75: 0.7556 - precision-0.80: 0.7590 - precision-0.85: 0.7826 - precision-0.90: 0.7333 - precision-0.95: 0.6667 - recall: 0.2755 - recall-0.55: 0.1989 - recall-0.60: 0.1383 - recall-0.65: 0.0951 - recall-0.70: 0.0623 - recall-0.75: 0.0363 - recall-0.80: 0.0192 - recall-0.85: 0.0110 - recall-0.90: 0.0050 - recall-0.95: 6.1050e-04 - val_loss: 1.0839 - val_tp: 474.0000 - val_fp: 393.0000 - val_tn: 2415.0000 - val_fn: 930.0000 - val_accuracy: 0.4829 - val_precision: 0.5467 - val_precision-0.55: 0.5574 - val_precision-0.60: 0.5620 - val_precision-0.65: 0.5901 - val_precision-0.70: 0.5787 - val_precision-0.75: 0.5317 - val_precision-0.80: 0.5068 - val_precision-0.85: 0.3514 - val_precision-0.90: 0.1250 - val_precision-0.95: 0.0000e+00 - val_recall: 0.3376 - val_recall-0.55: 0.2628 - val_recall-0.60: 0.1937 - val_recall-0.65: 0.1353 - val_recall-0.70: 0.0890 - val_recall-0.75: 0.0477 - val_recall-0.80: 0.0264 - val_recall-0.85: 0.0093 - val_recall-0.90: 0.0014 - val_recall-0.95: 0.0000e+00\n",
      "Epoch 7/200\n",
      "328/328 [==============================] - 2s 7ms/step - loss: 0.9699 - tp: 1890.0000 - fp: 1248.0000 - tn: 11856.0000 - fn: 4662.0000 - accuracy: 0.5163 - precision: 0.6023 - precision-0.55: 0.6338 - precision-0.60: 0.6543 - precision-0.65: 0.6818 - precision-0.70: 0.7214 - precision-0.75: 0.7645 - precision-0.80: 0.7732 - precision-0.85: 0.8065 - precision-0.90: 0.8378 - precision-0.95: 0.8571 - recall: 0.2885 - recall-0.55: 0.2108 - recall-0.60: 0.1531 - recall-0.65: 0.1030 - recall-0.70: 0.0664 - recall-0.75: 0.0421 - recall-0.80: 0.0229 - recall-0.85: 0.0114 - recall-0.90: 0.0047 - recall-0.95: 9.1575e-04 - val_loss: 1.1076 - val_tp: 456.0000 - val_fp: 410.0000 - val_tn: 2398.0000 - val_fn: 948.0000 - val_accuracy: 0.4772 - val_precision: 0.5266 - val_precision-0.55: 0.5432 - val_precision-0.60: 0.5405 - val_precision-0.65: 0.5727 - val_precision-0.70: 0.5370 - val_precision-0.75: 0.5194 - val_precision-0.80: 0.5122 - val_precision-0.85: 0.3421 - val_precision-0.90: 0.0625 - val_precision-0.95: 0.0000e+00 - val_recall: 0.3248 - val_recall-0.55: 0.2507 - val_recall-0.60: 0.1759 - val_recall-0.65: 0.1346 - val_recall-0.70: 0.0826 - val_recall-0.75: 0.0477 - val_recall-0.80: 0.0299 - val_recall-0.85: 0.0093 - val_recall-0.90: 7.1225e-04 - val_recall-0.95: 0.0000e+00\n",
      "Epoch 7: early stopping\n",
      "44/44 [==============================] - 0s 3ms/step\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 1.1299 - tp: 466.0000 - fp: 495.0000 - tn: 2313.0000 - fn: 938.0000 - accuracy: 0.4601 - precision: 0.4849 - precision-0.55: 0.5072 - precision-0.60: 0.4931 - precision-0.65: 0.5111 - precision-0.70: 0.4872 - precision-0.75: 0.4432 - precision-0.80: 0.3936 - precision-0.85: 0.3182 - precision-0.90: 0.3333 - precision-0.95: 0.2500 - recall: 0.3319 - recall-0.55: 0.2756 - recall-0.60: 0.2030 - recall-0.65: 0.1474 - recall-0.70: 0.0947 - recall-0.75: 0.0556 - recall-0.80: 0.0264 - recall-0.85: 0.0100 - recall-0.90: 0.0071 - recall-0.95: 0.0014                      \n",
      "\n",
      "=============\n",
      "CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.57      0.63       924\n",
      "           1       0.17      0.44      0.24       204\n",
      "           2       0.21      0.10      0.13       276\n",
      "\n",
      "    accuracy                           0.46      1404\n",
      "   macro avg       0.36      0.37      0.34      1404\n",
      "weighted avg       0.53      0.46      0.48      1404\n",
      "\n",
      "\n",
      "=============\n",
      "CONFUSION MATRIX:\n",
      "       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2\n",
      "0      530  327   67    924  0.574  0.354  0.073\n",
      "1       79   89   36    204  0.387  0.436  0.176\n",
      "2      137  112   27    276  0.496  0.406  0.098\n",
      "Total  746  528  130   1404  0.531  0.376  0.093\n",
      "\n",
      ">>>>>>\n",
      "EVALUATION SUMMARY:\n",
      "       loss  accuracy  precision  recall  precision-0.65  recall-0.65  \\\n",
      "1     1.388     0.438      0.459   0.374           0.477        0.282   \n",
      "2     1.165     0.410      0.436   0.269           0.470        0.085   \n",
      "3     1.018     0.580      0.600   0.533           0.626        0.357   \n",
      "4     1.242     0.328      0.320   0.163           0.295        0.044   \n",
      "5     1.130     0.460      0.485   0.332           0.511        0.147   \n",
      "mean  1.189     0.443      0.460   0.334           0.476        0.183   \n",
      "std   0.138     0.091      0.101   0.137           0.119        0.132   \n",
      "min   1.018     0.328      0.320   0.163           0.295        0.044   \n",
      "max   1.388     0.580      0.600   0.533           0.626        0.357   \n",
      "\n",
      "      precision-0.80  recall-0.80  precision-0.95  recall-0.95  \n",
      "1              0.464        0.188           0.575        0.052  \n",
      "2              0.417        0.018           0.818        0.006  \n",
      "3              0.682        0.098           0.000        0.000  \n",
      "4              0.333        0.008           1.000        0.001  \n",
      "5              0.394        0.026           0.250        0.001  \n",
      "mean           0.458        0.068           0.529        0.012  \n",
      "std            0.134        0.076           0.408        0.022  \n",
      "min            0.333        0.008           0.000        0.000  \n",
      "max            0.682        0.188           1.000        0.052  \n"
     ]
    }
   ],
   "source": [
    "results_list = []\n",
    "results = dnn.evaluate_classifier_k_folds(\n",
    "    hu=100,\n",
    "    fm=fm,\n",
    "    fold_number=5,\n",
    "    batch_size=20,\n",
    "    set_class_weight=True,\n",
    "    save_check_point=False,\n",
    "    early_stopping=True,\n",
    "    dropout=True,\n",
    "    dropout_rate=0.3,\n",
    "    shuffle_when_train=False,\n",
    "    gpu=False,\n",
    "    metrics=metric_list,\n",
    "    write_to_file=True)\n",
    "results_list.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_list = [\n",
    "    \"loss\",\n",
    "    \"accuracy\",\n",
    "    \"precision\",\n",
    "    \"recall\",\n",
    "    \"precision-0.65\",\n",
    "    \"recall-0.65\",\n",
    "    \"precision-0.80\",\n",
    "    \"recall-0.80\",\n",
    "    \"precision-0.95\",\n",
    "    \"recall-0.95\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n",
    "results = dnn.evaluate_classifier(\n",
    "    hu=100,\n",
    "    fm=fm,\n",
    "    laps=1,\n",
    "    batch_size=20,\n",
    "    set_class_weight=False,\n",
    "    save_check_point=False,\n",
    "    early_stopping=True,\n",
    "    shuffle_before_split=False,\n",
    "    dropout=True,\n",
    "    dropout_rate=0.3,\n",
    "    gpu=False,\n",
    "    metrics=metric_list,\n",
    "    write_to_file=True)\n",
    "results_list.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(\"../out/evaluate.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = dnn.MultiDNNClassifer()\n",
    "\n",
    "dataset = classifier.prepare_data(\n",
    "    data = fm.df,\n",
    "    cols = fm.cols,\n",
    "    shuffle_before_split= False,\n",
    "    categorical_label=True,\n",
    "    target_col=\"trade_signal\"\n",
    ")\n",
    "\n",
    "initial_bias = tr_utils.init_imbalanced_bias(\n",
    "    y_train=dataset[1]\n",
    ")\n",
    "\n",
    "classifier.configure(\n",
    "    hu = 100, \n",
    "    dropout=True, \n",
    "    dropout_rate = 0.3,\n",
    "    input_dim=len(fm.cols),\n",
    "    output_bias=initial_bias,\n",
    "    class_num=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8b58b1819899e409cec63cea36e334f732dfc50db3a5ecdff48b63b0a8eb4970"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
