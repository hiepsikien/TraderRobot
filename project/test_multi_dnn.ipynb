{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-20 15:36:01.682522: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-20 15:36:01.682666: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-20 15:36:01.682678: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-01-20 15:36:03.193701: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2023-01-20 15:36:03.193788: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: andy-GA-970A-D3\n",
      "2023-01-20 15:36:03.193804: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: andy-GA-970A-D3\n",
      "2023-01-20 15:36:03.194039: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 520.61.5\n",
      "2023-01-20 15:36:03.194092: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5\n",
      "2023-01-20 15:36:03.194105: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 520.61.5\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import pandas as pd\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import feature_manager as fma\n",
    "import classifier.multi_dnn_classifier as dnn\n",
    "from random import randint\n",
    "from keras import callbacks\n",
    "from keras import losses\n",
    "import visualizer\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import importlib\n",
    "import utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeframe_in_ms = {\"15m\":15*60*1000,\"1h\":60*60*1000,\"1d\":24*60*60*1000}\n",
    "\n",
    "symbol = \"BTCUSDT\"\n",
    "trade_tf = \"1d\"\n",
    "granular_tf = \"1m\"\n",
    "\n",
    "data = pd.read_csv(\"../data/{}-{}.csv\".format(\n",
    "    symbol,trade_tf), \n",
    "    parse_dates=[\"Open Time\"], \n",
    "    index_col = \"Open Time\"\n",
    ")\n",
    "\n",
    "granular_data = pd.read_csv(\"../nocommit/{}-{}.csv\".format(\n",
    "    symbol,granular_tf), \n",
    "    parse_dates=[\"Open Time\"], \n",
    "    index_col = \"Open Time\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing timeframe 1/10\n",
      "Processing timeframe 2/10\n",
      "Processing timeframe 3/10\n",
      "Processing timeframe 4/10\n",
      "Processing timeframe 5/10\n",
      "Processing timeframe 6/10\n",
      "Processing timeframe 7/10\n",
      "Processing timeframe 8/10\n",
      "Processing timeframe 9/10\n",
      "Processing timeframe 10/10\n",
      "Completed. Value counts of target_col:\n",
      "0    783\n",
      "1    610\n",
      "2    570\n",
      "Name: trade_signal, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "tp = 0.1\n",
    "sl = 0.07\n",
    "md = 10\n",
    "\n",
    "fm = fma.FeatureManager(data=data,target_col=\"trade_signal\")\n",
    "\n",
    "fm.prepare_trade_forward_data(\n",
    "    granular_data=granular_data,\n",
    "    take_profit_rate=tp,\n",
    "    stop_loss_rate=sl,\n",
    "    max_duration=md,\n",
    "    timeframe_in_ms=timeframe_in_ms[trade_tf]\n",
    ")\n",
    "\n",
    "# loop_classifier(data=fm.df,cols=fm.cols,target_col=\"trade_signal\",laps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating features...\n",
      "\n",
      "Adding feature: returns, dir, sma, boll, boll7, boll14, boll21, min, min7, min14, min21, max, max7, max14, max21, mom, mom7, mom14, mom21, vol, vol7, vol14, vol21, obv, mfi7, mfi14, mfi21, rsi7, rsi14, rsi21, adx7, adx14, adx21, roc, roc7, roc14, roc21, atr7, atr14, atr21, bop, ad, adosc, trange, ado, willr7, willr14, willr21, dx7, dx14, dx21, trix, ultosc, high, low, \n",
      "\n",
      "Normalizing feature: returns_lag_1, returns_lag_2, returns_lag_3, returns_lag_4, returns_lag_5, returns_lag_6, returns_lag_7, returns_lag_8, returns_lag_9, returns_lag_10, returns_lag_11, returns_lag_12, returns_lag_13, returns_lag_14, returns_lag_15, returns_lag_16, returns_lag_17, returns_lag_18, returns_lag_19, returns_lag_20, returns_lag_21, returns_lag_22, returns_lag_23, returns_lag_24, returns_lag_25, returns_lag_26, returns_lag_27, returns_lag_28, returns_lag_29, returns_lag_30, returns_lag_31, returns_lag_32, returns_lag_33, returns_lag_34, returns_lag_35, returns_lag_36, returns_lag_37, returns_lag_38, returns_lag_39, returns_lag_40, returns_lag_41, returns_lag_42, returns_lag_43, returns_lag_44, returns_lag_45, returns_lag_46, returns_lag_47, returns_lag_48, returns_lag_49, returns_lag_50, returns_lag_51, returns_lag_52, returns_lag_53, returns_lag_54, returns_lag_55, returns_lag_56, returns_lag_57, returns_lag_58, returns_lag_59, returns_lag_60, dir_lag_1, dir_lag_2, dir_lag_3, dir_lag_4, dir_lag_5, dir_lag_6, dir_lag_7, dir_lag_8, dir_lag_9, dir_lag_10, dir_lag_11, dir_lag_12, dir_lag_13, dir_lag_14, dir_lag_15, dir_lag_16, dir_lag_17, dir_lag_18, dir_lag_19, dir_lag_20, dir_lag_21, dir_lag_22, dir_lag_23, dir_lag_24, dir_lag_25, dir_lag_26, dir_lag_27, dir_lag_28, dir_lag_29, dir_lag_30, dir_lag_31, dir_lag_32, dir_lag_33, dir_lag_34, dir_lag_35, dir_lag_36, dir_lag_37, dir_lag_38, dir_lag_39, dir_lag_40, dir_lag_41, dir_lag_42, dir_lag_43, dir_lag_44, dir_lag_45, dir_lag_46, dir_lag_47, dir_lag_48, dir_lag_49, dir_lag_50, dir_lag_51, dir_lag_52, dir_lag_53, dir_lag_54, dir_lag_55, dir_lag_56, dir_lag_57, dir_lag_58, dir_lag_59, dir_lag_60, sma_lag_1, sma_lag_2, sma_lag_3, sma_lag_4, sma_lag_5, sma_lag_6, sma_lag_7, sma_lag_8, sma_lag_9, sma_lag_10, sma_lag_11, sma_lag_12, sma_lag_13, sma_lag_14, sma_lag_15, sma_lag_16, sma_lag_17, sma_lag_18, sma_lag_19, sma_lag_20, sma_lag_21, sma_lag_22, sma_lag_23, sma_lag_24, sma_lag_25, sma_lag_26, sma_lag_27, sma_lag_28, sma_lag_29, sma_lag_30, sma_lag_31, sma_lag_32, sma_lag_33, sma_lag_34, sma_lag_35, sma_lag_36, sma_lag_37, sma_lag_38, sma_lag_39, sma_lag_40, sma_lag_41, sma_lag_42, sma_lag_43, sma_lag_44, sma_lag_45, sma_lag_46, sma_lag_47, sma_lag_48, sma_lag_49, sma_lag_50, sma_lag_51, sma_lag_52, sma_lag_53, sma_lag_54, sma_lag_55, sma_lag_56, sma_lag_57, sma_lag_58, sma_lag_59, sma_lag_60, boll_lag_1, boll_lag_2, boll_lag_3, boll_lag_4, boll_lag_5, boll_lag_6, boll_lag_7, boll_lag_8, boll_lag_9, boll_lag_10, boll_lag_11, boll_lag_12, boll_lag_13, boll_lag_14, boll_lag_15, boll_lag_16, boll_lag_17, boll_lag_18, boll_lag_19, boll_lag_20, boll_lag_21, boll_lag_22, boll_lag_23, boll_lag_24, boll_lag_25, boll_lag_26, boll_lag_27, boll_lag_28, boll_lag_29, boll_lag_30, boll_lag_31, boll_lag_32, boll_lag_33, boll_lag_34, boll_lag_35, boll_lag_36, boll_lag_37, boll_lag_38, boll_lag_39, boll_lag_40, boll_lag_41, boll_lag_42, boll_lag_43, boll_lag_44, boll_lag_45, boll_lag_46, boll_lag_47, boll_lag_48, boll_lag_49, boll_lag_50, boll_lag_51, boll_lag_52, boll_lag_53, boll_lag_54, boll_lag_55, boll_lag_56, boll_lag_57, boll_lag_58, boll_lag_59, boll_lag_60, boll7_lag_1, boll7_lag_2, boll7_lag_3, boll7_lag_4, boll7_lag_5, boll7_lag_6, boll7_lag_7, boll7_lag_8, boll7_lag_9, boll7_lag_10, boll7_lag_11, boll7_lag_12, boll7_lag_13, boll7_lag_14, boll7_lag_15, boll7_lag_16, boll7_lag_17, boll7_lag_18, boll7_lag_19, boll7_lag_20, boll7_lag_21, boll7_lag_22, boll7_lag_23, boll7_lag_24, boll7_lag_25, boll7_lag_26, boll7_lag_27, boll7_lag_28, boll7_lag_29, boll7_lag_30, boll7_lag_31, boll7_lag_32, boll7_lag_33, boll7_lag_34, boll7_lag_35, boll7_lag_36, boll7_lag_37, boll7_lag_38, boll7_lag_39, boll7_lag_40, boll7_lag_41, boll7_lag_42, boll7_lag_43, boll7_lag_44, boll7_lag_45, boll7_lag_46, boll7_lag_47, boll7_lag_48, boll7_lag_49, boll7_lag_50, boll7_lag_51, boll7_lag_52, boll7_lag_53, boll7_lag_54, boll7_lag_55, boll7_lag_56, boll7_lag_57, boll7_lag_58, boll7_lag_59, boll7_lag_60, boll14_lag_1, boll14_lag_2, boll14_lag_3, boll14_lag_4, boll14_lag_5, boll14_lag_6, boll14_lag_7, boll14_lag_8, boll14_lag_9, boll14_lag_10, boll14_lag_11, boll14_lag_12, boll14_lag_13, boll14_lag_14, boll14_lag_15, boll14_lag_16, boll14_lag_17, boll14_lag_18, boll14_lag_19, boll14_lag_20, boll14_lag_21, boll14_lag_22, boll14_lag_23, boll14_lag_24, boll14_lag_25, boll14_lag_26, boll14_lag_27, boll14_lag_28, boll14_lag_29, boll14_lag_30, boll14_lag_31, boll14_lag_32, boll14_lag_33, boll14_lag_34, boll14_lag_35, boll14_lag_36, boll14_lag_37, boll14_lag_38, boll14_lag_39, boll14_lag_40, boll14_lag_41, boll14_lag_42, boll14_lag_43, boll14_lag_44, boll14_lag_45, boll14_lag_46, boll14_lag_47, boll14_lag_48, boll14_lag_49, boll14_lag_50, boll14_lag_51, boll14_lag_52, boll14_lag_53, boll14_lag_54, boll14_lag_55, boll14_lag_56, boll14_lag_57, boll14_lag_58, boll14_lag_59, boll14_lag_60, boll21_lag_1, boll21_lag_2, boll21_lag_3, boll21_lag_4, boll21_lag_5, boll21_lag_6, boll21_lag_7, boll21_lag_8, boll21_lag_9, boll21_lag_10, boll21_lag_11, boll21_lag_12, boll21_lag_13, boll21_lag_14, boll21_lag_15, boll21_lag_16, boll21_lag_17, boll21_lag_18, boll21_lag_19, boll21_lag_20, boll21_lag_21, boll21_lag_22, boll21_lag_23, boll21_lag_24, boll21_lag_25, boll21_lag_26, boll21_lag_27, boll21_lag_28, boll21_lag_29, boll21_lag_30, boll21_lag_31, boll21_lag_32, boll21_lag_33, boll21_lag_34, boll21_lag_35, boll21_lag_36, boll21_lag_37, boll21_lag_38, boll21_lag_39, boll21_lag_40, boll21_lag_41, boll21_lag_42, boll21_lag_43, boll21_lag_44, boll21_lag_45, boll21_lag_46, boll21_lag_47, boll21_lag_48, boll21_lag_49, boll21_lag_50, boll21_lag_51, boll21_lag_52, boll21_lag_53, boll21_lag_54, boll21_lag_55, boll21_lag_56, boll21_lag_57, boll21_lag_58, boll21_lag_59, boll21_lag_60, min_lag_1, min_lag_2, min_lag_3, min_lag_4, min_lag_5, min_lag_6, min_lag_7, min_lag_8, min_lag_9, min_lag_10, min_lag_11, min_lag_12, min_lag_13, min_lag_14, min_lag_15, min_lag_16, min_lag_17, min_lag_18, min_lag_19, min_lag_20, min_lag_21, min_lag_22, min_lag_23, min_lag_24, min_lag_25, min_lag_26, min_lag_27, min_lag_28, min_lag_29, min_lag_30, min_lag_31, min_lag_32, min_lag_33, min_lag_34, min_lag_35, min_lag_36, min_lag_37, min_lag_38, min_lag_39, min_lag_40, min_lag_41, min_lag_42, min_lag_43, min_lag_44, min_lag_45, min_lag_46, min_lag_47, min_lag_48, min_lag_49, min_lag_50, min_lag_51, min_lag_52, min_lag_53, min_lag_54, min_lag_55, min_lag_56, min_lag_57, min_lag_58, min_lag_59, min_lag_60, min7_lag_1, min7_lag_2, min7_lag_3, min7_lag_4, min7_lag_5, min7_lag_6, min7_lag_7, min7_lag_8, min7_lag_9, min7_lag_10, min7_lag_11, min7_lag_12, min7_lag_13, min7_lag_14, min7_lag_15, min7_lag_16, min7_lag_17, min7_lag_18, min7_lag_19, min7_lag_20, min7_lag_21, min7_lag_22, min7_lag_23, min7_lag_24, min7_lag_25, min7_lag_26, min7_lag_27, min7_lag_28, min7_lag_29, min7_lag_30, min7_lag_31, min7_lag_32, min7_lag_33, min7_lag_34, min7_lag_35, min7_lag_36, min7_lag_37, min7_lag_38, min7_lag_39, min7_lag_40, min7_lag_41, min7_lag_42, min7_lag_43, min7_lag_44, min7_lag_45, min7_lag_46, min7_lag_47, min7_lag_48, min7_lag_49, min7_lag_50, min7_lag_51, min7_lag_52, min7_lag_53, min7_lag_54, min7_lag_55, min7_lag_56, min7_lag_57, min7_lag_58, min7_lag_59, min7_lag_60, min14_lag_1, min14_lag_2, min14_lag_3, min14_lag_4, min14_lag_5, min14_lag_6, min14_lag_7, min14_lag_8, min14_lag_9, min14_lag_10, min14_lag_11, min14_lag_12, min14_lag_13, min14_lag_14, min14_lag_15, min14_lag_16, min14_lag_17, min14_lag_18, min14_lag_19, min14_lag_20, min14_lag_21, min14_lag_22, min14_lag_23, min14_lag_24, min14_lag_25, min14_lag_26, min14_lag_27, min14_lag_28, min14_lag_29, min14_lag_30, min14_lag_31, min14_lag_32, min14_lag_33, min14_lag_34, min14_lag_35, min14_lag_36, min14_lag_37, min14_lag_38, min14_lag_39, min14_lag_40, min14_lag_41, min14_lag_42, min14_lag_43, min14_lag_44, min14_lag_45, min14_lag_46, min14_lag_47, min14_lag_48, min14_lag_49, min14_lag_50, min14_lag_51, min14_lag_52, min14_lag_53, min14_lag_54, min14_lag_55, min14_lag_56, min14_lag_57, min14_lag_58, min14_lag_59, min14_lag_60, min21_lag_1, min21_lag_2, min21_lag_3, min21_lag_4, min21_lag_5, min21_lag_6, min21_lag_7, min21_lag_8, min21_lag_9, min21_lag_10, min21_lag_11, min21_lag_12, min21_lag_13, min21_lag_14, min21_lag_15, min21_lag_16, min21_lag_17, min21_lag_18, min21_lag_19, min21_lag_20, min21_lag_21, min21_lag_22, min21_lag_23, min21_lag_24, min21_lag_25, min21_lag_26, min21_lag_27, min21_lag_28, min21_lag_29, min21_lag_30, min21_lag_31, min21_lag_32, min21_lag_33, min21_lag_34, min21_lag_35, min21_lag_36, min21_lag_37, min21_lag_38, min21_lag_39, min21_lag_40, min21_lag_41, min21_lag_42, min21_lag_43, min21_lag_44, min21_lag_45, min21_lag_46, min21_lag_47, min21_lag_48, min21_lag_49, min21_lag_50, min21_lag_51, min21_lag_52, min21_lag_53, min21_lag_54, min21_lag_55, min21_lag_56, min21_lag_57, min21_lag_58, min21_lag_59, min21_lag_60, max_lag_1, max_lag_2, max_lag_3, max_lag_4, max_lag_5, max_lag_6, max_lag_7, max_lag_8, max_lag_9, max_lag_10, max_lag_11, max_lag_12, max_lag_13, max_lag_14, max_lag_15, max_lag_16, max_lag_17, max_lag_18, max_lag_19, max_lag_20, max_lag_21, max_lag_22, max_lag_23, max_lag_24, max_lag_25, max_lag_26, max_lag_27, max_lag_28, max_lag_29, max_lag_30, max_lag_31, max_lag_32, max_lag_33, max_lag_34, max_lag_35, max_lag_36, max_lag_37, max_lag_38, max_lag_39, max_lag_40, max_lag_41, max_lag_42, max_lag_43, max_lag_44, max_lag_45, max_lag_46, max_lag_47, max_lag_48, max_lag_49, max_lag_50, max_lag_51, max_lag_52, max_lag_53, max_lag_54, max_lag_55, max_lag_56, max_lag_57, max_lag_58, max_lag_59, max_lag_60, max7_lag_1, max7_lag_2, max7_lag_3, max7_lag_4, max7_lag_5, max7_lag_6, max7_lag_7, max7_lag_8, max7_lag_9, max7_lag_10, max7_lag_11, max7_lag_12, max7_lag_13, max7_lag_14, max7_lag_15, max7_lag_16, max7_lag_17, max7_lag_18, max7_lag_19, max7_lag_20, max7_lag_21, max7_lag_22, max7_lag_23, max7_lag_24, max7_lag_25, max7_lag_26, max7_lag_27, max7_lag_28, max7_lag_29, max7_lag_30, max7_lag_31, max7_lag_32, max7_lag_33, max7_lag_34, max7_lag_35, max7_lag_36, max7_lag_37, max7_lag_38, max7_lag_39, max7_lag_40, max7_lag_41, max7_lag_42, max7_lag_43, max7_lag_44, max7_lag_45, max7_lag_46, max7_lag_47, max7_lag_48, max7_lag_49, max7_lag_50, max7_lag_51, max7_lag_52, max7_lag_53, max7_lag_54, max7_lag_55, max7_lag_56, max7_lag_57, max7_lag_58, max7_lag_59, max7_lag_60, max14_lag_1, max14_lag_2, max14_lag_3, max14_lag_4, max14_lag_5, max14_lag_6, max14_lag_7, max14_lag_8, max14_lag_9, max14_lag_10, max14_lag_11, max14_lag_12, max14_lag_13, max14_lag_14, max14_lag_15, max14_lag_16, max14_lag_17, max14_lag_18, max14_lag_19, max14_lag_20, max14_lag_21, max14_lag_22, max14_lag_23, max14_lag_24, max14_lag_25, max14_lag_26, max14_lag_27, max14_lag_28, max14_lag_29, max14_lag_30, max14_lag_31, max14_lag_32, max14_lag_33, max14_lag_34, max14_lag_35, max14_lag_36, max14_lag_37, max14_lag_38, max14_lag_39, max14_lag_40, max14_lag_41, max14_lag_42, max14_lag_43, max14_lag_44, max14_lag_45, max14_lag_46, max14_lag_47, max14_lag_48, max14_lag_49, max14_lag_50, max14_lag_51, max14_lag_52, max14_lag_53, max14_lag_54, max14_lag_55, max14_lag_56, max14_lag_57, max14_lag_58, max14_lag_59, max14_lag_60, max21_lag_1, max21_lag_2, max21_lag_3, max21_lag_4, max21_lag_5, max21_lag_6, max21_lag_7, max21_lag_8, max21_lag_9, max21_lag_10, max21_lag_11, max21_lag_12, max21_lag_13, max21_lag_14, max21_lag_15, max21_lag_16, max21_lag_17, max21_lag_18, max21_lag_19, max21_lag_20, max21_lag_21, max21_lag_22, max21_lag_23, max21_lag_24, max21_lag_25, max21_lag_26, max21_lag_27, max21_lag_28, max21_lag_29, max21_lag_30, max21_lag_31, max21_lag_32, max21_lag_33, max21_lag_34, max21_lag_35, max21_lag_36, max21_lag_37, max21_lag_38, max21_lag_39, max21_lag_40, max21_lag_41, max21_lag_42, max21_lag_43, max21_lag_44, max21_lag_45, max21_lag_46, max21_lag_47, max21_lag_48, max21_lag_49, max21_lag_50, max21_lag_51, max21_lag_52, max21_lag_53, max21_lag_54, max21_lag_55, max21_lag_56, max21_lag_57, max21_lag_58, max21_lag_59, max21_lag_60, mom_lag_1, mom_lag_2, mom_lag_3, mom_lag_4, mom_lag_5, mom_lag_6, mom_lag_7, mom_lag_8, mom_lag_9, mom_lag_10, mom_lag_11, mom_lag_12, mom_lag_13, mom_lag_14, mom_lag_15, mom_lag_16, mom_lag_17, mom_lag_18, mom_lag_19, mom_lag_20, mom_lag_21, mom_lag_22, mom_lag_23, mom_lag_24, mom_lag_25, mom_lag_26, mom_lag_27, mom_lag_28, mom_lag_29, mom_lag_30, mom_lag_31, mom_lag_32, mom_lag_33, mom_lag_34, mom_lag_35, mom_lag_36, mom_lag_37, mom_lag_38, mom_lag_39, mom_lag_40, mom_lag_41, mom_lag_42, mom_lag_43, mom_lag_44, mom_lag_45, mom_lag_46, mom_lag_47, mom_lag_48, mom_lag_49, mom_lag_50, mom_lag_51, mom_lag_52, mom_lag_53, mom_lag_54, mom_lag_55, mom_lag_56, mom_lag_57, mom_lag_58, mom_lag_59, mom_lag_60, mom7_lag_1, mom7_lag_2, mom7_lag_3, mom7_lag_4, mom7_lag_5, mom7_lag_6, mom7_lag_7, mom7_lag_8, mom7_lag_9, mom7_lag_10, mom7_lag_11, mom7_lag_12, mom7_lag_13, mom7_lag_14, mom7_lag_15, mom7_lag_16, mom7_lag_17, mom7_lag_18, mom7_lag_19, mom7_lag_20, mom7_lag_21, mom7_lag_22, mom7_lag_23, mom7_lag_24, mom7_lag_25, mom7_lag_26, mom7_lag_27, mom7_lag_28, mom7_lag_29, mom7_lag_30, mom7_lag_31, mom7_lag_32, mom7_lag_33, mom7_lag_34, mom7_lag_35, mom7_lag_36, mom7_lag_37, mom7_lag_38, mom7_lag_39, mom7_lag_40, mom7_lag_41, mom7_lag_42, mom7_lag_43, mom7_lag_44, mom7_lag_45, mom7_lag_46, mom7_lag_47, mom7_lag_48, mom7_lag_49, mom7_lag_50, mom7_lag_51, mom7_lag_52, mom7_lag_53, mom7_lag_54, mom7_lag_55, mom7_lag_56, mom7_lag_57, mom7_lag_58, mom7_lag_59, mom7_lag_60, mom14_lag_1, mom14_lag_2, mom14_lag_3, mom14_lag_4, mom14_lag_5, mom14_lag_6, mom14_lag_7, mom14_lag_8, mom14_lag_9, mom14_lag_10, mom14_lag_11, mom14_lag_12, mom14_lag_13, mom14_lag_14, mom14_lag_15, mom14_lag_16, mom14_lag_17, mom14_lag_18, mom14_lag_19, mom14_lag_20, mom14_lag_21, mom14_lag_22, mom14_lag_23, mom14_lag_24, mom14_lag_25, mom14_lag_26, mom14_lag_27, mom14_lag_28, mom14_lag_29, mom14_lag_30, mom14_lag_31, mom14_lag_32, mom14_lag_33, mom14_lag_34, mom14_lag_35, mom14_lag_36, mom14_lag_37, mom14_lag_38, mom14_lag_39, mom14_lag_40, mom14_lag_41, mom14_lag_42, mom14_lag_43, mom14_lag_44, mom14_lag_45, mom14_lag_46, mom14_lag_47, mom14_lag_48, mom14_lag_49, mom14_lag_50, mom14_lag_51, mom14_lag_52, mom14_lag_53, mom14_lag_54, mom14_lag_55, mom14_lag_56, mom14_lag_57, mom14_lag_58, mom14_lag_59, mom14_lag_60, mom21_lag_1, mom21_lag_2, mom21_lag_3, mom21_lag_4, mom21_lag_5, mom21_lag_6, mom21_lag_7, mom21_lag_8, mom21_lag_9, mom21_lag_10, mom21_lag_11, mom21_lag_12, mom21_lag_13, mom21_lag_14, mom21_lag_15, mom21_lag_16, mom21_lag_17, mom21_lag_18, mom21_lag_19, mom21_lag_20, mom21_lag_21, mom21_lag_22, mom21_lag_23, mom21_lag_24, mom21_lag_25, mom21_lag_26, mom21_lag_27, mom21_lag_28, mom21_lag_29, mom21_lag_30, mom21_lag_31, mom21_lag_32, mom21_lag_33, mom21_lag_34, mom21_lag_35, mom21_lag_36, mom21_lag_37, mom21_lag_38, mom21_lag_39, mom21_lag_40, mom21_lag_41, mom21_lag_42, mom21_lag_43, mom21_lag_44, mom21_lag_45, mom21_lag_46, mom21_lag_47, mom21_lag_48, mom21_lag_49, mom21_lag_50, mom21_lag_51, mom21_lag_52, mom21_lag_53, mom21_lag_54, mom21_lag_55, mom21_lag_56, mom21_lag_57, mom21_lag_58, mom21_lag_59, mom21_lag_60, vol_lag_1, vol_lag_2, vol_lag_3, vol_lag_4, vol_lag_5, vol_lag_6, vol_lag_7, vol_lag_8, vol_lag_9, vol_lag_10, vol_lag_11, vol_lag_12, vol_lag_13, vol_lag_14, vol_lag_15, vol_lag_16, vol_lag_17, vol_lag_18, vol_lag_19, vol_lag_20, vol_lag_21, vol_lag_22, vol_lag_23, vol_lag_24, vol_lag_25, vol_lag_26, vol_lag_27, vol_lag_28, vol_lag_29, vol_lag_30, vol_lag_31, vol_lag_32, vol_lag_33, vol_lag_34, vol_lag_35, vol_lag_36, vol_lag_37, vol_lag_38, vol_lag_39, vol_lag_40, vol_lag_41, vol_lag_42, vol_lag_43, vol_lag_44, vol_lag_45, vol_lag_46, vol_lag_47, vol_lag_48, vol_lag_49, vol_lag_50, vol_lag_51, vol_lag_52, vol_lag_53, vol_lag_54, vol_lag_55, vol_lag_56, vol_lag_57, vol_lag_58, vol_lag_59, vol_lag_60, vol7_lag_1, vol7_lag_2, vol7_lag_3, vol7_lag_4, vol7_lag_5, vol7_lag_6, vol7_lag_7, vol7_lag_8, vol7_lag_9, vol7_lag_10, vol7_lag_11, vol7_lag_12, vol7_lag_13, vol7_lag_14, vol7_lag_15, vol7_lag_16, vol7_lag_17, vol7_lag_18, vol7_lag_19, vol7_lag_20, vol7_lag_21, vol7_lag_22, vol7_lag_23, vol7_lag_24, vol7_lag_25, vol7_lag_26, vol7_lag_27, vol7_lag_28, vol7_lag_29, vol7_lag_30, vol7_lag_31, vol7_lag_32, vol7_lag_33, vol7_lag_34, vol7_lag_35, vol7_lag_36, vol7_lag_37, vol7_lag_38, vol7_lag_39, vol7_lag_40, vol7_lag_41, vol7_lag_42, vol7_lag_43, vol7_lag_44, vol7_lag_45, vol7_lag_46, vol7_lag_47, vol7_lag_48, vol7_lag_49, vol7_lag_50, vol7_lag_51, vol7_lag_52, vol7_lag_53, vol7_lag_54, vol7_lag_55, vol7_lag_56, vol7_lag_57, vol7_lag_58, vol7_lag_59, vol7_lag_60, vol14_lag_1, vol14_lag_2, vol14_lag_3, vol14_lag_4, vol14_lag_5, vol14_lag_6, vol14_lag_7, vol14_lag_8, vol14_lag_9, vol14_lag_10, vol14_lag_11, vol14_lag_12, vol14_lag_13, vol14_lag_14, vol14_lag_15, vol14_lag_16, vol14_lag_17, vol14_lag_18, vol14_lag_19, vol14_lag_20, vol14_lag_21, vol14_lag_22, vol14_lag_23, vol14_lag_24, vol14_lag_25, vol14_lag_26, vol14_lag_27, vol14_lag_28, vol14_lag_29, vol14_lag_30, vol14_lag_31, vol14_lag_32, vol14_lag_33, vol14_lag_34, vol14_lag_35, vol14_lag_36, vol14_lag_37, vol14_lag_38, vol14_lag_39, vol14_lag_40, vol14_lag_41, vol14_lag_42, vol14_lag_43, vol14_lag_44, vol14_lag_45, vol14_lag_46, vol14_lag_47, vol14_lag_48, vol14_lag_49, vol14_lag_50, vol14_lag_51, vol14_lag_52, vol14_lag_53, vol14_lag_54, vol14_lag_55, vol14_lag_56, vol14_lag_57, vol14_lag_58, vol14_lag_59, vol14_lag_60, vol21_lag_1, vol21_lag_2, vol21_lag_3, vol21_lag_4, vol21_lag_5, vol21_lag_6, vol21_lag_7, vol21_lag_8, vol21_lag_9, vol21_lag_10, vol21_lag_11, vol21_lag_12, vol21_lag_13, vol21_lag_14, vol21_lag_15, vol21_lag_16, vol21_lag_17, vol21_lag_18, vol21_lag_19, vol21_lag_20, vol21_lag_21, vol21_lag_22, vol21_lag_23, vol21_lag_24, vol21_lag_25, vol21_lag_26, vol21_lag_27, vol21_lag_28, vol21_lag_29, vol21_lag_30, vol21_lag_31, vol21_lag_32, vol21_lag_33, vol21_lag_34, vol21_lag_35, vol21_lag_36, vol21_lag_37, vol21_lag_38, vol21_lag_39, vol21_lag_40, vol21_lag_41, vol21_lag_42, vol21_lag_43, vol21_lag_44, vol21_lag_45, vol21_lag_46, vol21_lag_47, vol21_lag_48, vol21_lag_49, vol21_lag_50, vol21_lag_51, vol21_lag_52, vol21_lag_53, vol21_lag_54, vol21_lag_55, vol21_lag_56, vol21_lag_57, vol21_lag_58, vol21_lag_59, vol21_lag_60, obv_lag_1, obv_lag_2, obv_lag_3, obv_lag_4, obv_lag_5, obv_lag_6, obv_lag_7, obv_lag_8, obv_lag_9, obv_lag_10, obv_lag_11, obv_lag_12, obv_lag_13, obv_lag_14, obv_lag_15, obv_lag_16, obv_lag_17, obv_lag_18, obv_lag_19, obv_lag_20, obv_lag_21, obv_lag_22, obv_lag_23, obv_lag_24, obv_lag_25, obv_lag_26, obv_lag_27, obv_lag_28, obv_lag_29, obv_lag_30, obv_lag_31, obv_lag_32, obv_lag_33, obv_lag_34, obv_lag_35, obv_lag_36, obv_lag_37, obv_lag_38, obv_lag_39, obv_lag_40, obv_lag_41, obv_lag_42, obv_lag_43, obv_lag_44, obv_lag_45, obv_lag_46, obv_lag_47, obv_lag_48, obv_lag_49, obv_lag_50, obv_lag_51, obv_lag_52, obv_lag_53, obv_lag_54, obv_lag_55, obv_lag_56, obv_lag_57, obv_lag_58, obv_lag_59, obv_lag_60, mfi7_lag_1, mfi7_lag_2, mfi7_lag_3, mfi7_lag_4, mfi7_lag_5, mfi7_lag_6, mfi7_lag_7, mfi7_lag_8, mfi7_lag_9, mfi7_lag_10, mfi7_lag_11, mfi7_lag_12, mfi7_lag_13, mfi7_lag_14, mfi7_lag_15, mfi7_lag_16, mfi7_lag_17, mfi7_lag_18, mfi7_lag_19, mfi7_lag_20, mfi7_lag_21, mfi7_lag_22, mfi7_lag_23, mfi7_lag_24, mfi7_lag_25, mfi7_lag_26, mfi7_lag_27, mfi7_lag_28, mfi7_lag_29, mfi7_lag_30, mfi7_lag_31, mfi7_lag_32, mfi7_lag_33, mfi7_lag_34, mfi7_lag_35, mfi7_lag_36, mfi7_lag_37, mfi7_lag_38, mfi7_lag_39, mfi7_lag_40, mfi7_lag_41, mfi7_lag_42, mfi7_lag_43, mfi7_lag_44, mfi7_lag_45, mfi7_lag_46, mfi7_lag_47, mfi7_lag_48, mfi7_lag_49, mfi7_lag_50, mfi7_lag_51, mfi7_lag_52, mfi7_lag_53, mfi7_lag_54, mfi7_lag_55, mfi7_lag_56, mfi7_lag_57, mfi7_lag_58, mfi7_lag_59, mfi7_lag_60, mfi14_lag_1, mfi14_lag_2, mfi14_lag_3, mfi14_lag_4, mfi14_lag_5, mfi14_lag_6, mfi14_lag_7, mfi14_lag_8, mfi14_lag_9, mfi14_lag_10, mfi14_lag_11, mfi14_lag_12, mfi14_lag_13, mfi14_lag_14, mfi14_lag_15, mfi14_lag_16, mfi14_lag_17, mfi14_lag_18, mfi14_lag_19, mfi14_lag_20, mfi14_lag_21, mfi14_lag_22, mfi14_lag_23, mfi14_lag_24, mfi14_lag_25, mfi14_lag_26, mfi14_lag_27, mfi14_lag_28, mfi14_lag_29, mfi14_lag_30, mfi14_lag_31, mfi14_lag_32, mfi14_lag_33, mfi14_lag_34, mfi14_lag_35, mfi14_lag_36, mfi14_lag_37, mfi14_lag_38, mfi14_lag_39, mfi14_lag_40, mfi14_lag_41, mfi14_lag_42, mfi14_lag_43, mfi14_lag_44, mfi14_lag_45, mfi14_lag_46, mfi14_lag_47, mfi14_lag_48, mfi14_lag_49, mfi14_lag_50, mfi14_lag_51, mfi14_lag_52, mfi14_lag_53, mfi14_lag_54, mfi14_lag_55, mfi14_lag_56, mfi14_lag_57, mfi14_lag_58, mfi14_lag_59, mfi14_lag_60, mfi21_lag_1, mfi21_lag_2, mfi21_lag_3, mfi21_lag_4, mfi21_lag_5, mfi21_lag_6, mfi21_lag_7, mfi21_lag_8, mfi21_lag_9, mfi21_lag_10, mfi21_lag_11, mfi21_lag_12, mfi21_lag_13, mfi21_lag_14, mfi21_lag_15, mfi21_lag_16, mfi21_lag_17, mfi21_lag_18, mfi21_lag_19, mfi21_lag_20, mfi21_lag_21, mfi21_lag_22, mfi21_lag_23, mfi21_lag_24, mfi21_lag_25, mfi21_lag_26, mfi21_lag_27, mfi21_lag_28, mfi21_lag_29, mfi21_lag_30, mfi21_lag_31, mfi21_lag_32, mfi21_lag_33, mfi21_lag_34, mfi21_lag_35, mfi21_lag_36, mfi21_lag_37, mfi21_lag_38, mfi21_lag_39, mfi21_lag_40, mfi21_lag_41, mfi21_lag_42, mfi21_lag_43, mfi21_lag_44, mfi21_lag_45, mfi21_lag_46, mfi21_lag_47, mfi21_lag_48, mfi21_lag_49, mfi21_lag_50, mfi21_lag_51, mfi21_lag_52, mfi21_lag_53, mfi21_lag_54, mfi21_lag_55, mfi21_lag_56, mfi21_lag_57, mfi21_lag_58, mfi21_lag_59, mfi21_lag_60, rsi7_lag_1, rsi7_lag_2, rsi7_lag_3, rsi7_lag_4, rsi7_lag_5, rsi7_lag_6, rsi7_lag_7, rsi7_lag_8, rsi7_lag_9, rsi7_lag_10, rsi7_lag_11, rsi7_lag_12, rsi7_lag_13, rsi7_lag_14, rsi7_lag_15, rsi7_lag_16, rsi7_lag_17, rsi7_lag_18, rsi7_lag_19, rsi7_lag_20, rsi7_lag_21, rsi7_lag_22, rsi7_lag_23, rsi7_lag_24, rsi7_lag_25, rsi7_lag_26, rsi7_lag_27, rsi7_lag_28, rsi7_lag_29, rsi7_lag_30, rsi7_lag_31, rsi7_lag_32, rsi7_lag_33, rsi7_lag_34, rsi7_lag_35, rsi7_lag_36, rsi7_lag_37, rsi7_lag_38, rsi7_lag_39, rsi7_lag_40, rsi7_lag_41, rsi7_lag_42, rsi7_lag_43, rsi7_lag_44, rsi7_lag_45, rsi7_lag_46, rsi7_lag_47, rsi7_lag_48, rsi7_lag_49, rsi7_lag_50, rsi7_lag_51, rsi7_lag_52, rsi7_lag_53, rsi7_lag_54, rsi7_lag_55, rsi7_lag_56, rsi7_lag_57, rsi7_lag_58, rsi7_lag_59, rsi7_lag_60, rsi14_lag_1, rsi14_lag_2, rsi14_lag_3, rsi14_lag_4, rsi14_lag_5, rsi14_lag_6, rsi14_lag_7, rsi14_lag_8, rsi14_lag_9, rsi14_lag_10, rsi14_lag_11, rsi14_lag_12, rsi14_lag_13, rsi14_lag_14, rsi14_lag_15, rsi14_lag_16, rsi14_lag_17, rsi14_lag_18, rsi14_lag_19, rsi14_lag_20, rsi14_lag_21, rsi14_lag_22, rsi14_lag_23, rsi14_lag_24, rsi14_lag_25, rsi14_lag_26, rsi14_lag_27, rsi14_lag_28, rsi14_lag_29, rsi14_lag_30, rsi14_lag_31, rsi14_lag_32, rsi14_lag_33, rsi14_lag_34, rsi14_lag_35, rsi14_lag_36, rsi14_lag_37, rsi14_lag_38, rsi14_lag_39, rsi14_lag_40, rsi14_lag_41, rsi14_lag_42, rsi14_lag_43, rsi14_lag_44, rsi14_lag_45, rsi14_lag_46, rsi14_lag_47, rsi14_lag_48, rsi14_lag_49, rsi14_lag_50, rsi14_lag_51, rsi14_lag_52, rsi14_lag_53, rsi14_lag_54, rsi14_lag_55, rsi14_lag_56, rsi14_lag_57, rsi14_lag_58, rsi14_lag_59, rsi14_lag_60, rsi21_lag_1, rsi21_lag_2, rsi21_lag_3, rsi21_lag_4, rsi21_lag_5, rsi21_lag_6, rsi21_lag_7, rsi21_lag_8, rsi21_lag_9, rsi21_lag_10, rsi21_lag_11, rsi21_lag_12, rsi21_lag_13, rsi21_lag_14, rsi21_lag_15, rsi21_lag_16, rsi21_lag_17, rsi21_lag_18, rsi21_lag_19, rsi21_lag_20, rsi21_lag_21, rsi21_lag_22, rsi21_lag_23, rsi21_lag_24, rsi21_lag_25, rsi21_lag_26, rsi21_lag_27, rsi21_lag_28, rsi21_lag_29, rsi21_lag_30, rsi21_lag_31, rsi21_lag_32, rsi21_lag_33, rsi21_lag_34, rsi21_lag_35, rsi21_lag_36, rsi21_lag_37, rsi21_lag_38, rsi21_lag_39, rsi21_lag_40, rsi21_lag_41, rsi21_lag_42, rsi21_lag_43, rsi21_lag_44, rsi21_lag_45, rsi21_lag_46, rsi21_lag_47, rsi21_lag_48, rsi21_lag_49, rsi21_lag_50, rsi21_lag_51, rsi21_lag_52, rsi21_lag_53, rsi21_lag_54, rsi21_lag_55, rsi21_lag_56, rsi21_lag_57, rsi21_lag_58, rsi21_lag_59, rsi21_lag_60, adx7_lag_1, adx7_lag_2, adx7_lag_3, adx7_lag_4, adx7_lag_5, adx7_lag_6, adx7_lag_7, adx7_lag_8, adx7_lag_9, adx7_lag_10, adx7_lag_11, adx7_lag_12, adx7_lag_13, adx7_lag_14, adx7_lag_15, adx7_lag_16, adx7_lag_17, adx7_lag_18, adx7_lag_19, adx7_lag_20, adx7_lag_21, adx7_lag_22, adx7_lag_23, adx7_lag_24, adx7_lag_25, adx7_lag_26, adx7_lag_27, adx7_lag_28, adx7_lag_29, adx7_lag_30, adx7_lag_31, adx7_lag_32, adx7_lag_33, adx7_lag_34, adx7_lag_35, adx7_lag_36, adx7_lag_37, adx7_lag_38, adx7_lag_39, adx7_lag_40, adx7_lag_41, adx7_lag_42, adx7_lag_43, adx7_lag_44, adx7_lag_45, adx7_lag_46, adx7_lag_47, adx7_lag_48, adx7_lag_49, adx7_lag_50, adx7_lag_51, adx7_lag_52, adx7_lag_53, adx7_lag_54, adx7_lag_55, adx7_lag_56, adx7_lag_57, adx7_lag_58, adx7_lag_59, adx7_lag_60, adx14_lag_1, adx14_lag_2, adx14_lag_3, adx14_lag_4, adx14_lag_5, adx14_lag_6, adx14_lag_7, adx14_lag_8, adx14_lag_9, adx14_lag_10, adx14_lag_11, adx14_lag_12, adx14_lag_13, adx14_lag_14, adx14_lag_15, adx14_lag_16, adx14_lag_17, adx14_lag_18, adx14_lag_19, adx14_lag_20, adx14_lag_21, adx14_lag_22, adx14_lag_23, adx14_lag_24, adx14_lag_25, adx14_lag_26, adx14_lag_27, adx14_lag_28, adx14_lag_29, adx14_lag_30, adx14_lag_31, adx14_lag_32, adx14_lag_33, adx14_lag_34, adx14_lag_35, adx14_lag_36, adx14_lag_37, adx14_lag_38, adx14_lag_39, adx14_lag_40, adx14_lag_41, adx14_lag_42, adx14_lag_43, adx14_lag_44, adx14_lag_45, adx14_lag_46, adx14_lag_47, adx14_lag_48, adx14_lag_49, adx14_lag_50, adx14_lag_51, adx14_lag_52, adx14_lag_53, adx14_lag_54, adx14_lag_55, adx14_lag_56, adx14_lag_57, adx14_lag_58, adx14_lag_59, adx14_lag_60, adx21_lag_1, adx21_lag_2, adx21_lag_3, adx21_lag_4, adx21_lag_5, adx21_lag_6, adx21_lag_7, adx21_lag_8, adx21_lag_9, adx21_lag_10, adx21_lag_11, adx21_lag_12, adx21_lag_13, adx21_lag_14, adx21_lag_15, adx21_lag_16, adx21_lag_17, adx21_lag_18, adx21_lag_19, adx21_lag_20, adx21_lag_21, adx21_lag_22, adx21_lag_23, adx21_lag_24, adx21_lag_25, adx21_lag_26, adx21_lag_27, adx21_lag_28, adx21_lag_29, adx21_lag_30, adx21_lag_31, adx21_lag_32, adx21_lag_33, adx21_lag_34, adx21_lag_35, adx21_lag_36, adx21_lag_37, adx21_lag_38, adx21_lag_39, adx21_lag_40, adx21_lag_41, adx21_lag_42, adx21_lag_43, adx21_lag_44, adx21_lag_45, adx21_lag_46, adx21_lag_47, adx21_lag_48, adx21_lag_49, adx21_lag_50, adx21_lag_51, adx21_lag_52, adx21_lag_53, adx21_lag_54, adx21_lag_55, adx21_lag_56, adx21_lag_57, adx21_lag_58, adx21_lag_59, adx21_lag_60, roc_lag_1, roc_lag_2, roc_lag_3, roc_lag_4, roc_lag_5, roc_lag_6, roc_lag_7, roc_lag_8, roc_lag_9, roc_lag_10, roc_lag_11, roc_lag_12, roc_lag_13, roc_lag_14, roc_lag_15, roc_lag_16, roc_lag_17, roc_lag_18, roc_lag_19, roc_lag_20, roc_lag_21, roc_lag_22, roc_lag_23, roc_lag_24, roc_lag_25, roc_lag_26, roc_lag_27, roc_lag_28, roc_lag_29, roc_lag_30, roc_lag_31, roc_lag_32, roc_lag_33, roc_lag_34, roc_lag_35, roc_lag_36, roc_lag_37, roc_lag_38, roc_lag_39, roc_lag_40, roc_lag_41, roc_lag_42, roc_lag_43, roc_lag_44, roc_lag_45, roc_lag_46, roc_lag_47, roc_lag_48, roc_lag_49, roc_lag_50, roc_lag_51, roc_lag_52, roc_lag_53, roc_lag_54, roc_lag_55, roc_lag_56, roc_lag_57, roc_lag_58, roc_lag_59, roc_lag_60, roc7_lag_1, roc7_lag_2, roc7_lag_3, roc7_lag_4, roc7_lag_5, roc7_lag_6, roc7_lag_7, roc7_lag_8, roc7_lag_9, roc7_lag_10, roc7_lag_11, roc7_lag_12, roc7_lag_13, roc7_lag_14, roc7_lag_15, roc7_lag_16, roc7_lag_17, roc7_lag_18, roc7_lag_19, roc7_lag_20, roc7_lag_21, roc7_lag_22, roc7_lag_23, roc7_lag_24, roc7_lag_25, roc7_lag_26, roc7_lag_27, roc7_lag_28, roc7_lag_29, roc7_lag_30, roc7_lag_31, roc7_lag_32, roc7_lag_33, roc7_lag_34, roc7_lag_35, roc7_lag_36, roc7_lag_37, roc7_lag_38, roc7_lag_39, roc7_lag_40, roc7_lag_41, roc7_lag_42, roc7_lag_43, roc7_lag_44, roc7_lag_45, roc7_lag_46, roc7_lag_47, roc7_lag_48, roc7_lag_49, roc7_lag_50, roc7_lag_51, roc7_lag_52, roc7_lag_53, roc7_lag_54, roc7_lag_55, roc7_lag_56, roc7_lag_57, roc7_lag_58, roc7_lag_59, roc7_lag_60, roc14_lag_1, roc14_lag_2, roc14_lag_3, roc14_lag_4, roc14_lag_5, roc14_lag_6, roc14_lag_7, roc14_lag_8, roc14_lag_9, roc14_lag_10, roc14_lag_11, roc14_lag_12, roc14_lag_13, roc14_lag_14, roc14_lag_15, roc14_lag_16, roc14_lag_17, roc14_lag_18, roc14_lag_19, roc14_lag_20, roc14_lag_21, roc14_lag_22, roc14_lag_23, roc14_lag_24, roc14_lag_25, roc14_lag_26, roc14_lag_27, roc14_lag_28, roc14_lag_29, roc14_lag_30, roc14_lag_31, roc14_lag_32, roc14_lag_33, roc14_lag_34, roc14_lag_35, roc14_lag_36, roc14_lag_37, roc14_lag_38, roc14_lag_39, roc14_lag_40, roc14_lag_41, roc14_lag_42, roc14_lag_43, roc14_lag_44, roc14_lag_45, roc14_lag_46, roc14_lag_47, roc14_lag_48, roc14_lag_49, roc14_lag_50, roc14_lag_51, roc14_lag_52, roc14_lag_53, roc14_lag_54, roc14_lag_55, roc14_lag_56, roc14_lag_57, roc14_lag_58, roc14_lag_59, roc14_lag_60, roc21_lag_1, roc21_lag_2, roc21_lag_3, roc21_lag_4, roc21_lag_5, roc21_lag_6, roc21_lag_7, roc21_lag_8, roc21_lag_9, roc21_lag_10, roc21_lag_11, roc21_lag_12, roc21_lag_13, roc21_lag_14, roc21_lag_15, roc21_lag_16, roc21_lag_17, roc21_lag_18, roc21_lag_19, roc21_lag_20, roc21_lag_21, roc21_lag_22, roc21_lag_23, roc21_lag_24, roc21_lag_25, roc21_lag_26, roc21_lag_27, roc21_lag_28, roc21_lag_29, roc21_lag_30, roc21_lag_31, roc21_lag_32, roc21_lag_33, roc21_lag_34, roc21_lag_35, roc21_lag_36, roc21_lag_37, roc21_lag_38, roc21_lag_39, roc21_lag_40, roc21_lag_41, roc21_lag_42, roc21_lag_43, roc21_lag_44, roc21_lag_45, roc21_lag_46, roc21_lag_47, roc21_lag_48, roc21_lag_49, roc21_lag_50, roc21_lag_51, roc21_lag_52, roc21_lag_53, roc21_lag_54, roc21_lag_55, roc21_lag_56, roc21_lag_57, roc21_lag_58, roc21_lag_59, roc21_lag_60, atr7_lag_1, atr7_lag_2, atr7_lag_3, atr7_lag_4, atr7_lag_5, atr7_lag_6, atr7_lag_7, atr7_lag_8, atr7_lag_9, atr7_lag_10, atr7_lag_11, atr7_lag_12, atr7_lag_13, atr7_lag_14, atr7_lag_15, atr7_lag_16, atr7_lag_17, atr7_lag_18, atr7_lag_19, atr7_lag_20, atr7_lag_21, atr7_lag_22, atr7_lag_23, atr7_lag_24, atr7_lag_25, atr7_lag_26, atr7_lag_27, atr7_lag_28, atr7_lag_29, atr7_lag_30, atr7_lag_31, atr7_lag_32, atr7_lag_33, atr7_lag_34, atr7_lag_35, atr7_lag_36, atr7_lag_37, atr7_lag_38, atr7_lag_39, atr7_lag_40, atr7_lag_41, atr7_lag_42, atr7_lag_43, atr7_lag_44, atr7_lag_45, atr7_lag_46, atr7_lag_47, atr7_lag_48, atr7_lag_49, atr7_lag_50, atr7_lag_51, atr7_lag_52, atr7_lag_53, atr7_lag_54, atr7_lag_55, atr7_lag_56, atr7_lag_57, atr7_lag_58, atr7_lag_59, atr7_lag_60, atr14_lag_1, atr14_lag_2, atr14_lag_3, atr14_lag_4, atr14_lag_5, atr14_lag_6, atr14_lag_7, atr14_lag_8, atr14_lag_9, atr14_lag_10, atr14_lag_11, atr14_lag_12, atr14_lag_13, atr14_lag_14, atr14_lag_15, atr14_lag_16, atr14_lag_17, atr14_lag_18, atr14_lag_19, atr14_lag_20, atr14_lag_21, atr14_lag_22, atr14_lag_23, atr14_lag_24, atr14_lag_25, atr14_lag_26, atr14_lag_27, atr14_lag_28, atr14_lag_29, atr14_lag_30, atr14_lag_31, atr14_lag_32, atr14_lag_33, atr14_lag_34, atr14_lag_35, atr14_lag_36, atr14_lag_37, atr14_lag_38, atr14_lag_39, atr14_lag_40, atr14_lag_41, atr14_lag_42, atr14_lag_43, atr14_lag_44, atr14_lag_45, atr14_lag_46, atr14_lag_47, atr14_lag_48, atr14_lag_49, atr14_lag_50, atr14_lag_51, atr14_lag_52, atr14_lag_53, atr14_lag_54, atr14_lag_55, atr14_lag_56, atr14_lag_57, atr14_lag_58, atr14_lag_59, atr14_lag_60, atr21_lag_1, atr21_lag_2, atr21_lag_3, atr21_lag_4, atr21_lag_5, atr21_lag_6, atr21_lag_7, atr21_lag_8, atr21_lag_9, atr21_lag_10, atr21_lag_11, atr21_lag_12, atr21_lag_13, atr21_lag_14, atr21_lag_15, atr21_lag_16, atr21_lag_17, atr21_lag_18, atr21_lag_19, atr21_lag_20, atr21_lag_21, atr21_lag_22, atr21_lag_23, atr21_lag_24, atr21_lag_25, atr21_lag_26, atr21_lag_27, atr21_lag_28, atr21_lag_29, atr21_lag_30, atr21_lag_31, atr21_lag_32, atr21_lag_33, atr21_lag_34, atr21_lag_35, atr21_lag_36, atr21_lag_37, atr21_lag_38, atr21_lag_39, atr21_lag_40, atr21_lag_41, atr21_lag_42, atr21_lag_43, atr21_lag_44, atr21_lag_45, atr21_lag_46, atr21_lag_47, atr21_lag_48, atr21_lag_49, atr21_lag_50, atr21_lag_51, atr21_lag_52, atr21_lag_53, atr21_lag_54, atr21_lag_55, atr21_lag_56, atr21_lag_57, atr21_lag_58, atr21_lag_59, atr21_lag_60, bop_lag_1, bop_lag_2, bop_lag_3, bop_lag_4, bop_lag_5, bop_lag_6, bop_lag_7, bop_lag_8, bop_lag_9, bop_lag_10, bop_lag_11, bop_lag_12, bop_lag_13, bop_lag_14, bop_lag_15, bop_lag_16, bop_lag_17, bop_lag_18, bop_lag_19, bop_lag_20, bop_lag_21, bop_lag_22, bop_lag_23, bop_lag_24, bop_lag_25, bop_lag_26, bop_lag_27, bop_lag_28, bop_lag_29, bop_lag_30, bop_lag_31, bop_lag_32, bop_lag_33, bop_lag_34, bop_lag_35, bop_lag_36, bop_lag_37, bop_lag_38, bop_lag_39, bop_lag_40, bop_lag_41, bop_lag_42, bop_lag_43, bop_lag_44, bop_lag_45, bop_lag_46, bop_lag_47, bop_lag_48, bop_lag_49, bop_lag_50, bop_lag_51, bop_lag_52, bop_lag_53, bop_lag_54, bop_lag_55, bop_lag_56, bop_lag_57, bop_lag_58, bop_lag_59, bop_lag_60, ad_lag_1, ad_lag_2, ad_lag_3, ad_lag_4, ad_lag_5, ad_lag_6, ad_lag_7, ad_lag_8, ad_lag_9, ad_lag_10, ad_lag_11, ad_lag_12, ad_lag_13, ad_lag_14, ad_lag_15, ad_lag_16, ad_lag_17, ad_lag_18, ad_lag_19, ad_lag_20, ad_lag_21, ad_lag_22, ad_lag_23, ad_lag_24, ad_lag_25, ad_lag_26, ad_lag_27, ad_lag_28, ad_lag_29, ad_lag_30, ad_lag_31, ad_lag_32, ad_lag_33, ad_lag_34, ad_lag_35, ad_lag_36, ad_lag_37, ad_lag_38, ad_lag_39, ad_lag_40, ad_lag_41, ad_lag_42, ad_lag_43, ad_lag_44, ad_lag_45, ad_lag_46, ad_lag_47, ad_lag_48, ad_lag_49, ad_lag_50, ad_lag_51, ad_lag_52, ad_lag_53, ad_lag_54, ad_lag_55, ad_lag_56, ad_lag_57, ad_lag_58, ad_lag_59, ad_lag_60, adosc_lag_1, adosc_lag_2, adosc_lag_3, adosc_lag_4, adosc_lag_5, adosc_lag_6, adosc_lag_7, adosc_lag_8, adosc_lag_9, adosc_lag_10, adosc_lag_11, adosc_lag_12, adosc_lag_13, adosc_lag_14, adosc_lag_15, adosc_lag_16, adosc_lag_17, adosc_lag_18, adosc_lag_19, adosc_lag_20, adosc_lag_21, adosc_lag_22, adosc_lag_23, adosc_lag_24, adosc_lag_25, adosc_lag_26, adosc_lag_27, adosc_lag_28, adosc_lag_29, adosc_lag_30, adosc_lag_31, adosc_lag_32, adosc_lag_33, adosc_lag_34, adosc_lag_35, adosc_lag_36, adosc_lag_37, adosc_lag_38, adosc_lag_39, adosc_lag_40, adosc_lag_41, adosc_lag_42, adosc_lag_43, adosc_lag_44, adosc_lag_45, adosc_lag_46, adosc_lag_47, adosc_lag_48, adosc_lag_49, adosc_lag_50, adosc_lag_51, adosc_lag_52, adosc_lag_53, adosc_lag_54, adosc_lag_55, adosc_lag_56, adosc_lag_57, adosc_lag_58, adosc_lag_59, adosc_lag_60, trange_lag_1, trange_lag_2, trange_lag_3, trange_lag_4, trange_lag_5, trange_lag_6, trange_lag_7, trange_lag_8, trange_lag_9, trange_lag_10, trange_lag_11, trange_lag_12, trange_lag_13, trange_lag_14, trange_lag_15, trange_lag_16, trange_lag_17, trange_lag_18, trange_lag_19, trange_lag_20, trange_lag_21, trange_lag_22, trange_lag_23, trange_lag_24, trange_lag_25, trange_lag_26, trange_lag_27, trange_lag_28, trange_lag_29, trange_lag_30, trange_lag_31, trange_lag_32, trange_lag_33, trange_lag_34, trange_lag_35, trange_lag_36, trange_lag_37, trange_lag_38, trange_lag_39, trange_lag_40, trange_lag_41, trange_lag_42, trange_lag_43, trange_lag_44, trange_lag_45, trange_lag_46, trange_lag_47, trange_lag_48, trange_lag_49, trange_lag_50, trange_lag_51, trange_lag_52, trange_lag_53, trange_lag_54, trange_lag_55, trange_lag_56, trange_lag_57, trange_lag_58, trange_lag_59, trange_lag_60, ado_lag_1, ado_lag_2, ado_lag_3, ado_lag_4, ado_lag_5, ado_lag_6, ado_lag_7, ado_lag_8, ado_lag_9, ado_lag_10, ado_lag_11, ado_lag_12, ado_lag_13, ado_lag_14, ado_lag_15, ado_lag_16, ado_lag_17, ado_lag_18, ado_lag_19, ado_lag_20, ado_lag_21, ado_lag_22, ado_lag_23, ado_lag_24, ado_lag_25, ado_lag_26, ado_lag_27, ado_lag_28, ado_lag_29, ado_lag_30, ado_lag_31, ado_lag_32, ado_lag_33, ado_lag_34, ado_lag_35, ado_lag_36, ado_lag_37, ado_lag_38, ado_lag_39, ado_lag_40, ado_lag_41, ado_lag_42, ado_lag_43, ado_lag_44, ado_lag_45, ado_lag_46, ado_lag_47, ado_lag_48, ado_lag_49, ado_lag_50, ado_lag_51, ado_lag_52, ado_lag_53, ado_lag_54, ado_lag_55, ado_lag_56, ado_lag_57, ado_lag_58, ado_lag_59, ado_lag_60, willr7_lag_1, willr7_lag_2, willr7_lag_3, willr7_lag_4, willr7_lag_5, willr7_lag_6, willr7_lag_7, willr7_lag_8, willr7_lag_9, willr7_lag_10, willr7_lag_11, willr7_lag_12, willr7_lag_13, willr7_lag_14, willr7_lag_15, willr7_lag_16, willr7_lag_17, willr7_lag_18, willr7_lag_19, willr7_lag_20, willr7_lag_21, willr7_lag_22, willr7_lag_23, willr7_lag_24, willr7_lag_25, willr7_lag_26, willr7_lag_27, willr7_lag_28, willr7_lag_29, willr7_lag_30, willr7_lag_31, willr7_lag_32, willr7_lag_33, willr7_lag_34, willr7_lag_35, willr7_lag_36, willr7_lag_37, willr7_lag_38, willr7_lag_39, willr7_lag_40, willr7_lag_41, willr7_lag_42, willr7_lag_43, willr7_lag_44, willr7_lag_45, willr7_lag_46, willr7_lag_47, willr7_lag_48, willr7_lag_49, willr7_lag_50, willr7_lag_51, willr7_lag_52, willr7_lag_53, willr7_lag_54, willr7_lag_55, willr7_lag_56, willr7_lag_57, willr7_lag_58, willr7_lag_59, willr7_lag_60, willr14_lag_1, willr14_lag_2, willr14_lag_3, willr14_lag_4, willr14_lag_5, willr14_lag_6, willr14_lag_7, willr14_lag_8, willr14_lag_9, willr14_lag_10, willr14_lag_11, willr14_lag_12, willr14_lag_13, willr14_lag_14, willr14_lag_15, willr14_lag_16, willr14_lag_17, willr14_lag_18, willr14_lag_19, willr14_lag_20, willr14_lag_21, willr14_lag_22, willr14_lag_23, willr14_lag_24, willr14_lag_25, willr14_lag_26, willr14_lag_27, willr14_lag_28, willr14_lag_29, willr14_lag_30, willr14_lag_31, willr14_lag_32, willr14_lag_33, willr14_lag_34, willr14_lag_35, willr14_lag_36, willr14_lag_37, willr14_lag_38, willr14_lag_39, willr14_lag_40, willr14_lag_41, willr14_lag_42, willr14_lag_43, willr14_lag_44, willr14_lag_45, willr14_lag_46, willr14_lag_47, willr14_lag_48, willr14_lag_49, willr14_lag_50, willr14_lag_51, willr14_lag_52, willr14_lag_53, willr14_lag_54, willr14_lag_55, willr14_lag_56, willr14_lag_57, willr14_lag_58, willr14_lag_59, willr14_lag_60, willr21_lag_1, willr21_lag_2, willr21_lag_3, willr21_lag_4, willr21_lag_5, willr21_lag_6, willr21_lag_7, willr21_lag_8, willr21_lag_9, willr21_lag_10, willr21_lag_11, willr21_lag_12, willr21_lag_13, willr21_lag_14, willr21_lag_15, willr21_lag_16, willr21_lag_17, willr21_lag_18, willr21_lag_19, willr21_lag_20, willr21_lag_21, willr21_lag_22, willr21_lag_23, willr21_lag_24, willr21_lag_25, willr21_lag_26, willr21_lag_27, willr21_lag_28, willr21_lag_29, willr21_lag_30, willr21_lag_31, willr21_lag_32, willr21_lag_33, willr21_lag_34, willr21_lag_35, willr21_lag_36, willr21_lag_37, willr21_lag_38, willr21_lag_39, willr21_lag_40, willr21_lag_41, willr21_lag_42, willr21_lag_43, willr21_lag_44, willr21_lag_45, willr21_lag_46, willr21_lag_47, willr21_lag_48, willr21_lag_49, willr21_lag_50, willr21_lag_51, willr21_lag_52, willr21_lag_53, willr21_lag_54, willr21_lag_55, willr21_lag_56, willr21_lag_57, willr21_lag_58, willr21_lag_59, willr21_lag_60, dx7_lag_1, dx7_lag_2, dx7_lag_3, dx7_lag_4, dx7_lag_5, dx7_lag_6, dx7_lag_7, dx7_lag_8, dx7_lag_9, dx7_lag_10, dx7_lag_11, dx7_lag_12, dx7_lag_13, dx7_lag_14, dx7_lag_15, dx7_lag_16, dx7_lag_17, dx7_lag_18, dx7_lag_19, dx7_lag_20, dx7_lag_21, dx7_lag_22, dx7_lag_23, dx7_lag_24, dx7_lag_25, dx7_lag_26, dx7_lag_27, dx7_lag_28, dx7_lag_29, dx7_lag_30, dx7_lag_31, dx7_lag_32, dx7_lag_33, dx7_lag_34, dx7_lag_35, dx7_lag_36, dx7_lag_37, dx7_lag_38, dx7_lag_39, dx7_lag_40, dx7_lag_41, dx7_lag_42, dx7_lag_43, dx7_lag_44, dx7_lag_45, dx7_lag_46, dx7_lag_47, dx7_lag_48, dx7_lag_49, dx7_lag_50, dx7_lag_51, dx7_lag_52, dx7_lag_53, dx7_lag_54, dx7_lag_55, dx7_lag_56, dx7_lag_57, dx7_lag_58, dx7_lag_59, dx7_lag_60, dx14_lag_1, dx14_lag_2, dx14_lag_3, dx14_lag_4, dx14_lag_5, dx14_lag_6, dx14_lag_7, dx14_lag_8, dx14_lag_9, dx14_lag_10, dx14_lag_11, dx14_lag_12, dx14_lag_13, dx14_lag_14, dx14_lag_15, dx14_lag_16, dx14_lag_17, dx14_lag_18, dx14_lag_19, dx14_lag_20, dx14_lag_21, dx14_lag_22, dx14_lag_23, dx14_lag_24, dx14_lag_25, dx14_lag_26, dx14_lag_27, dx14_lag_28, dx14_lag_29, dx14_lag_30, dx14_lag_31, dx14_lag_32, dx14_lag_33, dx14_lag_34, dx14_lag_35, dx14_lag_36, dx14_lag_37, dx14_lag_38, dx14_lag_39, dx14_lag_40, dx14_lag_41, dx14_lag_42, dx14_lag_43, dx14_lag_44, dx14_lag_45, dx14_lag_46, dx14_lag_47, dx14_lag_48, dx14_lag_49, dx14_lag_50, dx14_lag_51, dx14_lag_52, dx14_lag_53, dx14_lag_54, dx14_lag_55, dx14_lag_56, dx14_lag_57, dx14_lag_58, dx14_lag_59, dx14_lag_60, dx21_lag_1, dx21_lag_2, dx21_lag_3, dx21_lag_4, dx21_lag_5, dx21_lag_6, dx21_lag_7, dx21_lag_8, dx21_lag_9, dx21_lag_10, dx21_lag_11, dx21_lag_12, dx21_lag_13, dx21_lag_14, dx21_lag_15, dx21_lag_16, dx21_lag_17, dx21_lag_18, dx21_lag_19, dx21_lag_20, dx21_lag_21, dx21_lag_22, dx21_lag_23, dx21_lag_24, dx21_lag_25, dx21_lag_26, dx21_lag_27, dx21_lag_28, dx21_lag_29, dx21_lag_30, dx21_lag_31, dx21_lag_32, dx21_lag_33, dx21_lag_34, dx21_lag_35, dx21_lag_36, dx21_lag_37, dx21_lag_38, dx21_lag_39, dx21_lag_40, dx21_lag_41, dx21_lag_42, dx21_lag_43, dx21_lag_44, dx21_lag_45, dx21_lag_46, dx21_lag_47, dx21_lag_48, dx21_lag_49, dx21_lag_50, dx21_lag_51, dx21_lag_52, dx21_lag_53, dx21_lag_54, dx21_lag_55, dx21_lag_56, dx21_lag_57, dx21_lag_58, dx21_lag_59, dx21_lag_60, trix_lag_1, trix_lag_2, trix_lag_3, trix_lag_4, trix_lag_5, trix_lag_6, trix_lag_7, trix_lag_8, trix_lag_9, trix_lag_10, trix_lag_11, trix_lag_12, trix_lag_13, trix_lag_14, trix_lag_15, trix_lag_16, trix_lag_17, trix_lag_18, trix_lag_19, trix_lag_20, trix_lag_21, trix_lag_22, trix_lag_23, trix_lag_24, trix_lag_25, trix_lag_26, trix_lag_27, trix_lag_28, trix_lag_29, trix_lag_30, trix_lag_31, trix_lag_32, trix_lag_33, trix_lag_34, trix_lag_35, trix_lag_36, trix_lag_37, trix_lag_38, trix_lag_39, trix_lag_40, trix_lag_41, trix_lag_42, trix_lag_43, trix_lag_44, trix_lag_45, trix_lag_46, trix_lag_47, trix_lag_48, trix_lag_49, trix_lag_50, trix_lag_51, trix_lag_52, trix_lag_53, trix_lag_54, trix_lag_55, trix_lag_56, trix_lag_57, trix_lag_58, trix_lag_59, trix_lag_60, ultosc_lag_1, ultosc_lag_2, ultosc_lag_3, ultosc_lag_4, ultosc_lag_5, ultosc_lag_6, ultosc_lag_7, ultosc_lag_8, ultosc_lag_9, ultosc_lag_10, ultosc_lag_11, ultosc_lag_12, ultosc_lag_13, ultosc_lag_14, ultosc_lag_15, ultosc_lag_16, ultosc_lag_17, ultosc_lag_18, ultosc_lag_19, ultosc_lag_20, ultosc_lag_21, ultosc_lag_22, ultosc_lag_23, ultosc_lag_24, ultosc_lag_25, ultosc_lag_26, ultosc_lag_27, ultosc_lag_28, ultosc_lag_29, ultosc_lag_30, ultosc_lag_31, ultosc_lag_32, ultosc_lag_33, ultosc_lag_34, ultosc_lag_35, ultosc_lag_36, ultosc_lag_37, ultosc_lag_38, ultosc_lag_39, ultosc_lag_40, ultosc_lag_41, ultosc_lag_42, ultosc_lag_43, ultosc_lag_44, ultosc_lag_45, ultosc_lag_46, ultosc_lag_47, ultosc_lag_48, ultosc_lag_49, ultosc_lag_50, ultosc_lag_51, ultosc_lag_52, ultosc_lag_53, ultosc_lag_54, ultosc_lag_55, ultosc_lag_56, ultosc_lag_57, ultosc_lag_58, ultosc_lag_59, ultosc_lag_60, high_lag_1, high_lag_2, high_lag_3, high_lag_4, high_lag_5, high_lag_6, high_lag_7, high_lag_8, high_lag_9, high_lag_10, high_lag_11, high_lag_12, high_lag_13, high_lag_14, high_lag_15, high_lag_16, high_lag_17, high_lag_18, high_lag_19, high_lag_20, high_lag_21, high_lag_22, high_lag_23, high_lag_24, high_lag_25, high_lag_26, high_lag_27, high_lag_28, high_lag_29, high_lag_30, high_lag_31, high_lag_32, high_lag_33, high_lag_34, high_lag_35, high_lag_36, high_lag_37, high_lag_38, high_lag_39, high_lag_40, high_lag_41, high_lag_42, high_lag_43, high_lag_44, high_lag_45, high_lag_46, high_lag_47, high_lag_48, high_lag_49, high_lag_50, high_lag_51, high_lag_52, high_lag_53, high_lag_54, high_lag_55, high_lag_56, high_lag_57, high_lag_58, high_lag_59, high_lag_60, low_lag_1, low_lag_2, low_lag_3, low_lag_4, low_lag_5, low_lag_6, low_lag_7, low_lag_8, low_lag_9, low_lag_10, low_lag_11, low_lag_12, low_lag_13, low_lag_14, low_lag_15, low_lag_16, low_lag_17, low_lag_18, low_lag_19, low_lag_20, low_lag_21, low_lag_22, low_lag_23, low_lag_24, low_lag_25, low_lag_26, low_lag_27, low_lag_28, low_lag_29, low_lag_30, low_lag_31, low_lag_32, low_lag_33, low_lag_34, low_lag_35, low_lag_36, low_lag_37, low_lag_38, low_lag_39, low_lag_40, low_lag_41, low_lag_42, low_lag_43, low_lag_44, low_lag_45, low_lag_46, low_lag_47, low_lag_48, low_lag_49, low_lag_50, low_lag_51, low_lag_52, low_lag_53, low_lag_54, low_lag_55, low_lag_56, low_lag_57, low_lag_58, low_lag_59, low_lag_60, \n",
      "Total 3300 features added.\n"
     ]
    }
   ],
   "source": [
    "fm.build_features(lags=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(dnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = dnn.MultiClassDNNClassifer()\n",
    "\n",
    "classifier.prepare_data(\n",
    "    data = fm.df,\n",
    "    cols = fm.cols,\n",
    "    is_shuffle = True,\n",
    "    y_to_categorical=True,\n",
    "    rebalance = None,\n",
    "    random_state=100,\n",
    "    target_col=\"trade_signal\"\n",
    ")\n",
    "\n",
    "initial_bias = utils.init_imbalanced_bias(\n",
    "    data=classifier.data_train,\n",
    "    target_col=\"trade_signal\"\n",
    ")\n",
    "\n",
    "classifier.configure(\n",
    "    hu = 200, \n",
    "    dropout=True, \n",
    "    input_dim=len(fm.cols),\n",
    "    output_bias=initial_bias,\n",
    "    class_num=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======= Lap 1 =======\n",
      "Shuffling the data\n",
      "Completed. Train: 1227, Validation: 263, Test: 264\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-20 15:38:07.875457: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1584000000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 - 10s - loss: 1.5050 - tp: 565.0000 - fp: 551.0000 - tn: 1903.0000 - fn: 662.0000 - accuracy: 0.6705 - precision: 0.5063 - recall: 0.4605 - auc: 0.6762 - prc: 0.5018 - val_loss: 1.0353 - val_tp: 148.0000 - val_fp: 88.0000 - val_tn: 438.0000 - val_fn: 115.0000 - val_accuracy: 0.7427 - val_precision: 0.6271 - val_recall: 0.5627 - val_auc: 0.7779 - val_prc: 0.6363 - 10s/epoch - 94ms/step\n",
      "Epoch 2/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-20 15:38:16.006534: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1584000000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 - 8s - loss: 0.7154 - tp: 868.0000 - fp: 287.0000 - tn: 2167.0000 - fn: 359.0000 - accuracy: 0.8245 - precision: 0.7515 - recall: 0.7074 - auc: 0.8789 - prc: 0.7955 - val_loss: 0.9724 - val_tp: 164.0000 - val_fp: 86.0000 - val_tn: 440.0000 - val_fn: 99.0000 - val_accuracy: 0.7655 - val_precision: 0.6560 - val_recall: 0.6236 - val_auc: 0.8077 - val_prc: 0.6830 - 8s/epoch - 78ms/step\n",
      "Epoch 3/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-20 15:38:24.106907: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1584000000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 - 8s - loss: 0.5647 - tp: 957.0000 - fp: 217.0000 - tn: 2237.0000 - fn: 270.0000 - accuracy: 0.8677 - precision: 0.8152 - recall: 0.7800 - auc: 0.9220 - prc: 0.8576 - val_loss: 0.9839 - val_tp: 169.0000 - val_fp: 84.0000 - val_tn: 442.0000 - val_fn: 94.0000 - val_accuracy: 0.7744 - val_precision: 0.6680 - val_recall: 0.6426 - val_auc: 0.8320 - val_prc: 0.7211 - 8s/epoch - 80ms/step\n",
      "Epoch 4/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-20 15:38:32.415005: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1584000000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 - 8s - loss: 0.4627 - tp: 1015.0000 - fp: 179.0000 - tn: 2275.0000 - fn: 212.0000 - accuracy: 0.8938 - precision: 0.8501 - recall: 0.8272 - auc: 0.9461 - prc: 0.9053 - val_loss: 1.1955 - val_tp: 163.0000 - val_fp: 88.0000 - val_tn: 438.0000 - val_fn: 100.0000 - val_accuracy: 0.7617 - val_precision: 0.6494 - val_recall: 0.6198 - val_auc: 0.8173 - val_prc: 0.7006 - 8s/epoch - 80ms/step\n",
      "Epoch 5/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-20 15:38:39.640713: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1584000000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 - 7s - loss: 0.4140 - tp: 1041.0000 - fp: 156.0000 - tn: 2298.0000 - fn: 186.0000 - accuracy: 0.9071 - precision: 0.8697 - recall: 0.8484 - auc: 0.9566 - prc: 0.9191 - val_loss: 0.8405 - val_tp: 193.0000 - val_fp: 64.0000 - val_tn: 462.0000 - val_fn: 70.0000 - val_accuracy: 0.8302 - val_precision: 0.7510 - val_recall: 0.7338 - val_auc: 0.8725 - val_prc: 0.7958 - 7s/epoch - 69ms/step\n",
      "Epoch 6/200\n",
      "103/103 - 7s - loss: 0.2389 - tp: 1119.0000 - fp: 87.0000 - tn: 2367.0000 - fn: 108.0000 - accuracy: 0.9470 - precision: 0.9279 - recall: 0.9120 - auc: 0.9835 - prc: 0.9692 - val_loss: 0.9206 - val_tp: 181.0000 - val_fp: 75.0000 - val_tn: 451.0000 - val_fn: 82.0000 - val_accuracy: 0.8010 - val_precision: 0.7070 - val_recall: 0.6882 - val_auc: 0.8529 - val_prc: 0.7336 - 7s/epoch - 69ms/step\n",
      "Epoch 7/200\n",
      "103/103 - 7s - loss: 0.2037 - tp: 1132.0000 - fp: 74.0000 - tn: 2380.0000 - fn: 95.0000 - accuracy: 0.9541 - precision: 0.9386 - recall: 0.9226 - auc: 0.9883 - prc: 0.9787 - val_loss: 0.7421 - val_tp: 192.0000 - val_fp: 63.0000 - val_tn: 463.0000 - val_fn: 71.0000 - val_accuracy: 0.8302 - val_precision: 0.7529 - val_recall: 0.7300 - val_auc: 0.8924 - val_prc: 0.8199 - 7s/epoch - 72ms/step\n",
      "Epoch 8/200\n",
      "103/103 - 8s - loss: 0.1560 - tp: 1157.0000 - fp: 58.0000 - tn: 2396.0000 - fn: 70.0000 - accuracy: 0.9652 - precision: 0.9523 - recall: 0.9430 - auc: 0.9929 - prc: 0.9868 - val_loss: 0.9146 - val_tp: 186.0000 - val_fp: 64.0000 - val_tn: 462.0000 - val_fn: 77.0000 - val_accuracy: 0.8213 - val_precision: 0.7440 - val_recall: 0.7072 - val_auc: 0.8725 - val_prc: 0.8024 - 8s/epoch - 74ms/step\n",
      "Epoch 9/200\n",
      "103/103 - 8s - loss: 0.1708 - tp: 1153.0000 - fp: 66.0000 - tn: 2388.0000 - fn: 74.0000 - accuracy: 0.9620 - precision: 0.9459 - recall: 0.9397 - auc: 0.9911 - prc: 0.9839 - val_loss: 0.9676 - val_tp: 187.0000 - val_fp: 68.0000 - val_tn: 458.0000 - val_fn: 76.0000 - val_accuracy: 0.8175 - val_precision: 0.7333 - val_recall: 0.7110 - val_auc: 0.8621 - val_prc: 0.7721 - 8s/epoch - 82ms/step\n",
      "Epoch 10/200\n",
      "103/103 - 7s - loss: 0.1499 - tp: 1164.0000 - fp: 57.0000 - tn: 2397.0000 - fn: 63.0000 - accuracy: 0.9674 - precision: 0.9533 - recall: 0.9487 - auc: 0.9931 - prc: 0.9870 - val_loss: 1.0409 - val_tp: 186.0000 - val_fp: 65.0000 - val_tn: 461.0000 - val_fn: 77.0000 - val_accuracy: 0.8200 - val_precision: 0.7410 - val_recall: 0.7072 - val_auc: 0.8558 - val_prc: 0.7595 - 7s/epoch - 68ms/step\n",
      "Epoch 11/200\n",
      "103/103 - 7s - loss: 0.1079 - tp: 1179.0000 - fp: 41.0000 - tn: 2413.0000 - fn: 48.0000 - accuracy: 0.9758 - precision: 0.9664 - recall: 0.9609 - auc: 0.9967 - prc: 0.9936 - val_loss: 0.9375 - val_tp: 188.0000 - val_fp: 65.0000 - val_tn: 461.0000 - val_fn: 75.0000 - val_accuracy: 0.8226 - val_precision: 0.7431 - val_recall: 0.7148 - val_auc: 0.8743 - val_prc: 0.7857 - 7s/epoch - 67ms/step\n",
      "Epoch 12/200\n",
      "103/103 - 7s - loss: 0.1126 - tp: 1184.0000 - fp: 39.0000 - tn: 2415.0000 - fn: 43.0000 - accuracy: 0.9777 - precision: 0.9681 - recall: 0.9650 - auc: 0.9959 - prc: 0.9931 - val_loss: 0.9939 - val_tp: 186.0000 - val_fp: 73.0000 - val_tn: 453.0000 - val_fn: 77.0000 - val_accuracy: 0.8099 - val_precision: 0.7181 - val_recall: 0.7072 - val_auc: 0.8594 - val_prc: 0.7513 - 7s/epoch - 69ms/step\n",
      "Epoch 12: early stopping\n",
      "9/9 [==============================] - 0s 21ms/step\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 1.1037 - tp: 191.0000 - fp: 65.0000 - tn: 463.0000 - fn: 73.0000 - accuracy: 0.8258 - precision: 0.7461 - recall: 0.7235 - auc: 0.8506 - prc: 0.7482\n",
      "\n",
      "=============\n",
      "PARAMS:\n",
      "random_state:1\n",
      "is_shuffle:True\n",
      "y_to_categorical:True\n",
      "rebalance:None\n",
      "cat_lentgh:1000\n",
      "hu:2000\n",
      "output_bias:[0.29277847488414926, -0.1391532064121786, -0.1536252390207128]\n",
      "loss:categorical_crossentropy\n",
      "dropout:True\n",
      "gpu:False\n",
      "set_class_weight:False\n",
      "save_check_point:False\n",
      "early_stopping:True\n",
      "patience:5\n",
      "epochs:200\n",
      "batch_size:12\n",
      "timeframe_in_ms:86400000\n",
      "take_profit_rate:0.1\n",
      "stop_loss_rate:0.07\n",
      "max_duration:10\n",
      "lags:60\n",
      "\n",
      "\n",
      "\n",
      "=============\n",
      "CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.77      0.76       116\n",
      "           1       0.70      0.68      0.69        77\n",
      "           2       0.71      0.73      0.72        71\n",
      "\n",
      "    accuracy                           0.73       264\n",
      "   macro avg       0.73      0.72      0.72       264\n",
      "weighted avg       0.73      0.73      0.73       264\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "=============\n",
      "CONFUSION MATRIX:\n",
      "[[89 16 11]\n",
      " [15 52 10]\n",
      " [13  6 52]]\n",
      "\n",
      "\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 1.1037 - tp: 191.0000 - fp: 65.0000 - tn: 463.0000 - fn: 73.0000 - accuracy: 0.8258 - precision: 0.7461 - recall: 0.7235 - auc: 0.8506 - prc: 0.7482\n",
      "\n",
      "======= Lap 2 =======\n",
      "Shuffling the data\n",
      "Completed. Train: 1227, Validation: 263, Test: 264\n",
      "Epoch 1/200\n",
      "103/103 - 9s - loss: 1.3628 - tp: 804.0000 - fp: 562.0000 - tn: 2420.0000 - fn: 687.0000 - accuracy: 0.7208 - precision: 0.5886 - recall: 0.5392 - auc: 0.7367 - prc: 0.5875 - val_loss: 1.2737 - val_tp: 142.0000 - val_fp: 94.0000 - val_tn: 432.0000 - val_fn: 121.0000 - val_accuracy: 0.7275 - val_precision: 0.6017 - val_recall: 0.5399 - val_auc: 0.7330 - val_prc: 0.5788 - 9s/epoch - 91ms/step\n",
      "Epoch 2/200\n",
      "103/103 - 7s - loss: 0.7839 - tp: 872.0000 - fp: 299.0000 - tn: 2155.0000 - fn: 355.0000 - accuracy: 0.8223 - precision: 0.7447 - recall: 0.7107 - auc: 0.8784 - prc: 0.7873 - val_loss: 1.1193 - val_tp: 156.0000 - val_fp: 90.0000 - val_tn: 436.0000 - val_fn: 107.0000 - val_accuracy: 0.7503 - val_precision: 0.6341 - val_recall: 0.5932 - val_auc: 0.7926 - val_prc: 0.6634 - 7s/epoch - 71ms/step\n",
      "Epoch 3/200\n",
      "103/103 - 7s - loss: 0.5760 - tp: 969.0000 - fp: 210.0000 - tn: 2244.0000 - fn: 258.0000 - accuracy: 0.8729 - precision: 0.8219 - recall: 0.7897 - auc: 0.9259 - prc: 0.8713 - val_loss: 0.9804 - val_tp: 167.0000 - val_fp: 79.0000 - val_tn: 447.0000 - val_fn: 96.0000 - val_accuracy: 0.7782 - val_precision: 0.6789 - val_recall: 0.6350 - val_auc: 0.8267 - val_prc: 0.7046 - 7s/epoch - 72ms/step\n",
      "Epoch 4/200\n",
      "103/103 - 7s - loss: 0.4180 - tp: 1022.0000 - fp: 172.0000 - tn: 2282.0000 - fn: 205.0000 - accuracy: 0.8976 - precision: 0.8559 - recall: 0.8329 - auc: 0.9529 - prc: 0.9134 - val_loss: 1.0290 - val_tp: 172.0000 - val_fp: 78.0000 - val_tn: 448.0000 - val_fn: 91.0000 - val_accuracy: 0.7858 - val_precision: 0.6880 - val_recall: 0.6540 - val_auc: 0.8311 - val_prc: 0.7083 - 7s/epoch - 68ms/step\n",
      "Epoch 5/200\n",
      "103/103 - 7s - loss: 0.3384 - tp: 1066.0000 - fp: 144.0000 - tn: 2310.0000 - fn: 161.0000 - accuracy: 0.9171 - precision: 0.8810 - recall: 0.8688 - auc: 0.9688 - prc: 0.9446 - val_loss: 1.0533 - val_tp: 166.0000 - val_fp: 83.0000 - val_tn: 443.0000 - val_fn: 97.0000 - val_accuracy: 0.7719 - val_precision: 0.6667 - val_recall: 0.6312 - val_auc: 0.8326 - val_prc: 0.7173 - 7s/epoch - 67ms/step\n",
      "Epoch 6/200\n",
      "103/103 - 7s - loss: 0.2406 - tp: 1116.0000 - fp: 100.0000 - tn: 2354.0000 - fn: 111.0000 - accuracy: 0.9427 - precision: 0.9178 - recall: 0.9095 - auc: 0.9831 - prc: 0.9692 - val_loss: 0.9822 - val_tp: 175.0000 - val_fp: 78.0000 - val_tn: 448.0000 - val_fn: 88.0000 - val_accuracy: 0.7896 - val_precision: 0.6917 - val_recall: 0.6654 - val_auc: 0.8461 - val_prc: 0.7412 - 7s/epoch - 67ms/step\n",
      "Epoch 7/200\n",
      "103/103 - 7s - loss: 0.1817 - tp: 1150.0000 - fp: 62.0000 - tn: 2392.0000 - fn: 77.0000 - accuracy: 0.9622 - precision: 0.9488 - recall: 0.9372 - auc: 0.9907 - prc: 0.9825 - val_loss: 1.0068 - val_tp: 180.0000 - val_fp: 78.0000 - val_tn: 448.0000 - val_fn: 83.0000 - val_accuracy: 0.7959 - val_precision: 0.6977 - val_recall: 0.6844 - val_auc: 0.8550 - val_prc: 0.7590 - 7s/epoch - 67ms/step\n",
      "Epoch 8/200\n",
      "103/103 - 7s - loss: 0.1639 - tp: 1149.0000 - fp: 64.0000 - tn: 2390.0000 - fn: 78.0000 - accuracy: 0.9614 - precision: 0.9472 - recall: 0.9364 - auc: 0.9931 - prc: 0.9872 - val_loss: 1.0426 - val_tp: 177.0000 - val_fp: 71.0000 - val_tn: 455.0000 - val_fn: 86.0000 - val_accuracy: 0.8010 - val_precision: 0.7137 - val_recall: 0.6730 - val_auc: 0.8536 - val_prc: 0.7504 - 7s/epoch - 67ms/step\n",
      "Epoch 8: early stopping\n",
      "9/9 [==============================] - 0s 18ms/step\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 1.1590 - tp: 169.0000 - fp: 82.0000 - tn: 446.0000 - fn: 95.0000 - accuracy: 0.7765 - precision: 0.6733 - recall: 0.6402 - auc: 0.8169 - prc: 0.6863\n",
      "\n",
      "=============\n",
      "PARAMS:\n",
      "random_state:2\n",
      "is_shuffle:True\n",
      "y_to_categorical:True\n",
      "rebalance:None\n",
      "cat_lentgh:1000\n",
      "hu:2000\n",
      "output_bias:[0.3063722415364799, -0.11959684676939017, -0.18677538452610526]\n",
      "loss:categorical_crossentropy\n",
      "dropout:True\n",
      "gpu:False\n",
      "set_class_weight:False\n",
      "save_check_point:False\n",
      "early_stopping:True\n",
      "patience:5\n",
      "epochs:200\n",
      "batch_size:12\n",
      "timeframe_in_ms:86400000\n",
      "take_profit_rate:0.1\n",
      "stop_loss_rate:0.07\n",
      "max_duration:10\n",
      "lags:60\n",
      "\n",
      "\n",
      "\n",
      "=============\n",
      "CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.62      0.66       112\n",
      "           1       0.66      0.79      0.72        76\n",
      "           2       0.66      0.64      0.65        76\n",
      "\n",
      "    accuracy                           0.68       264\n",
      "   macro avg       0.68      0.69      0.68       264\n",
      "weighted avg       0.68      0.68      0.68       264\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "=============\n",
      "CONFUSION MATRIX:\n",
      "[[70 20 22]\n",
      " [13 60  3]\n",
      " [16 11 49]]\n",
      "\n",
      "\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 1.1590 - tp: 169.0000 - fp: 82.0000 - tn: 446.0000 - fn: 95.0000 - accuracy: 0.7765 - precision: 0.6733 - recall: 0.6402 - auc: 0.8169 - prc: 0.6863\n",
      "\n",
      "======= Lap 3 =======\n",
      "Shuffling the data\n",
      "Completed. Train: 1227, Validation: 263, Test: 264\n",
      "Epoch 1/200\n",
      "103/103 - 10s - loss: 1.3741 - tp: 762.0000 - fp: 592.0000 - tn: 2390.0000 - fn: 729.0000 - accuracy: 0.7047 - precision: 0.5628 - recall: 0.5111 - auc: 0.7235 - prc: 0.5675 - val_loss: 1.3940 - val_tp: 151.0000 - val_fp: 101.0000 - val_tn: 425.0000 - val_fn: 112.0000 - val_accuracy: 0.7300 - val_precision: 0.5992 - val_recall: 0.5741 - val_auc: 0.7496 - val_prc: 0.6168 - 10s/epoch - 94ms/step\n",
      "Epoch 2/200\n",
      "103/103 - 7s - loss: 0.7732 - tp: 882.0000 - fp: 285.0000 - tn: 2169.0000 - fn: 345.0000 - accuracy: 0.8289 - precision: 0.7558 - recall: 0.7188 - auc: 0.8766 - prc: 0.7908 - val_loss: 0.9613 - val_tp: 164.0000 - val_fp: 81.0000 - val_tn: 445.0000 - val_fn: 99.0000 - val_accuracy: 0.7719 - val_precision: 0.6694 - val_recall: 0.6236 - val_auc: 0.8122 - val_prc: 0.6895 - 7s/epoch - 69ms/step\n",
      "Epoch 3/200\n",
      "103/103 - 7s - loss: 0.6310 - tp: 943.0000 - fp: 236.0000 - tn: 2218.0000 - fn: 284.0000 - accuracy: 0.8587 - precision: 0.7998 - recall: 0.7685 - auc: 0.9101 - prc: 0.8450 - val_loss: 0.9365 - val_tp: 171.0000 - val_fp: 81.0000 - val_tn: 445.0000 - val_fn: 92.0000 - val_accuracy: 0.7807 - val_precision: 0.6786 - val_recall: 0.6502 - val_auc: 0.8323 - val_prc: 0.7386 - 7s/epoch - 70ms/step\n",
      "Epoch 4/200\n",
      "103/103 - 7s - loss: 0.4015 - tp: 1034.0000 - fp: 156.0000 - tn: 2298.0000 - fn: 193.0000 - accuracy: 0.9052 - precision: 0.8689 - recall: 0.8427 - auc: 0.9569 - prc: 0.9251 - val_loss: 0.9861 - val_tp: 174.0000 - val_fp: 76.0000 - val_tn: 450.0000 - val_fn: 89.0000 - val_accuracy: 0.7909 - val_precision: 0.6960 - val_recall: 0.6616 - val_auc: 0.8286 - val_prc: 0.7124 - 7s/epoch - 69ms/step\n",
      "Epoch 5/200\n",
      "103/103 - 7s - loss: 0.2978 - tp: 1072.0000 - fp: 124.0000 - tn: 2330.0000 - fn: 155.0000 - accuracy: 0.9242 - precision: 0.8963 - recall: 0.8737 - auc: 0.9749 - prc: 0.9554 - val_loss: 0.8179 - val_tp: 183.0000 - val_fp: 69.0000 - val_tn: 457.0000 - val_fn: 80.0000 - val_accuracy: 0.8112 - val_precision: 0.7262 - val_recall: 0.6958 - val_auc: 0.8654 - val_prc: 0.7660 - 7s/epoch - 70ms/step\n",
      "Epoch 6/200\n",
      "103/103 - 7s - loss: 0.2301 - tp: 1119.0000 - fp: 90.0000 - tn: 2364.0000 - fn: 108.0000 - accuracy: 0.9462 - precision: 0.9256 - recall: 0.9120 - auc: 0.9850 - prc: 0.9722 - val_loss: 0.8944 - val_tp: 180.0000 - val_fp: 73.0000 - val_tn: 453.0000 - val_fn: 83.0000 - val_accuracy: 0.8023 - val_precision: 0.7115 - val_recall: 0.6844 - val_auc: 0.8600 - val_prc: 0.7745 - 7s/epoch - 70ms/step\n",
      "Epoch 7/200\n",
      "103/103 - 7s - loss: 0.1766 - tp: 1159.0000 - fp: 56.0000 - tn: 2398.0000 - fn: 68.0000 - accuracy: 0.9663 - precision: 0.9539 - recall: 0.9446 - auc: 0.9897 - prc: 0.9808 - val_loss: 1.0587 - val_tp: 180.0000 - val_fp: 72.0000 - val_tn: 454.0000 - val_fn: 83.0000 - val_accuracy: 0.8035 - val_precision: 0.7143 - val_recall: 0.6844 - val_auc: 0.8487 - val_prc: 0.7490 - 7s/epoch - 70ms/step\n",
      "Epoch 8/200\n",
      "103/103 - 7s - loss: 0.1846 - tp: 1148.0000 - fp: 69.0000 - tn: 2385.0000 - fn: 79.0000 - accuracy: 0.9598 - precision: 0.9433 - recall: 0.9356 - auc: 0.9888 - prc: 0.9773 - val_loss: 1.1334 - val_tp: 170.0000 - val_fp: 81.0000 - val_tn: 445.0000 - val_fn: 93.0000 - val_accuracy: 0.7795 - val_precision: 0.6773 - val_recall: 0.6464 - val_auc: 0.8237 - val_prc: 0.7076 - 7s/epoch - 70ms/step\n",
      "Epoch 9/200\n",
      "103/103 - 7s - loss: 0.1680 - tp: 1146.0000 - fp: 72.0000 - tn: 2382.0000 - fn: 81.0000 - accuracy: 0.9584 - precision: 0.9409 - recall: 0.9340 - auc: 0.9917 - prc: 0.9850 - val_loss: 0.9364 - val_tp: 190.0000 - val_fp: 68.0000 - val_tn: 458.0000 - val_fn: 73.0000 - val_accuracy: 0.8213 - val_precision: 0.7364 - val_recall: 0.7224 - val_auc: 0.8707 - val_prc: 0.7778 - 7s/epoch - 69ms/step\n",
      "Epoch 10/200\n",
      "103/103 - 7s - loss: 0.1240 - tp: 1173.0000 - fp: 48.0000 - tn: 2406.0000 - fn: 54.0000 - accuracy: 0.9723 - precision: 0.9607 - recall: 0.9560 - auc: 0.9961 - prc: 0.9928 - val_loss: 1.2149 - val_tp: 176.0000 - val_fp: 79.0000 - val_tn: 447.0000 - val_fn: 87.0000 - val_accuracy: 0.7896 - val_precision: 0.6902 - val_recall: 0.6692 - val_auc: 0.8358 - val_prc: 0.7287 - 7s/epoch - 69ms/step\n",
      "Epoch 10: early stopping\n",
      "9/9 [==============================] - 0s 17ms/step\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 1.0759 - tp: 178.0000 - fp: 79.0000 - tn: 449.0000 - fn: 86.0000 - accuracy: 0.7917 - precision: 0.6926 - recall: 0.6742 - auc: 0.8533 - prc: 0.7362\n",
      "\n",
      "=============\n",
      "PARAMS:\n",
      "random_state:3\n",
      "is_shuffle:True\n",
      "y_to_categorical:True\n",
      "rebalance:None\n",
      "cat_lentgh:1000\n",
      "hu:2000\n",
      "output_bias:[0.2884982207863342, -0.12260070504005838, -0.16589751079344447]\n",
      "loss:categorical_crossentropy\n",
      "dropout:True\n",
      "gpu:False\n",
      "set_class_weight:False\n",
      "save_check_point:False\n",
      "early_stopping:True\n",
      "patience:5\n",
      "epochs:200\n",
      "batch_size:12\n",
      "timeframe_in_ms:86400000\n",
      "take_profit_rate:0.1\n",
      "stop_loss_rate:0.07\n",
      "max_duration:10\n",
      "lags:60\n",
      "\n",
      "\n",
      "\n",
      "=============\n",
      "CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.78      0.72       117\n",
      "           1       0.67      0.67      0.67        84\n",
      "           2       0.74      0.54      0.62        63\n",
      "\n",
      "    accuracy                           0.69       264\n",
      "   macro avg       0.70      0.66      0.67       264\n",
      "weighted avg       0.69      0.69      0.68       264\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "=============\n",
      "CONFUSION MATRIX:\n",
      "[[91 19  7]\n",
      " [23 56  5]\n",
      " [21  8 34]]\n",
      "\n",
      "\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 1.0759 - tp: 178.0000 - fp: 79.0000 - tn: 449.0000 - fn: 86.0000 - accuracy: 0.7917 - precision: 0.6926 - recall: 0.6742 - auc: 0.8533 - prc: 0.7362\n",
      "\n",
      "======= Lap 4 =======\n",
      "Shuffling the data\n",
      "Completed. Train: 1227, Validation: 263, Test: 264\n",
      "Epoch 1/200\n",
      "103/103 - 9s - loss: 1.4449 - tp: 752.0000 - fp: 626.0000 - tn: 2356.0000 - fn: 739.0000 - accuracy: 0.6948 - precision: 0.5457 - recall: 0.5044 - auc: 0.7179 - prc: 0.5610 - val_loss: 1.0222 - val_tp: 159.0000 - val_fp: 86.0000 - val_tn: 440.0000 - val_fn: 104.0000 - val_accuracy: 0.7592 - val_precision: 0.6490 - val_recall: 0.6046 - val_auc: 0.8052 - val_prc: 0.6814 - 9s/epoch - 91ms/step\n",
      "Epoch 2/200\n",
      "103/103 - 7s - loss: 0.7761 - tp: 863.0000 - fp: 301.0000 - tn: 2153.0000 - fn: 364.0000 - accuracy: 0.8193 - precision: 0.7414 - recall: 0.7033 - auc: 0.8686 - prc: 0.7769 - val_loss: 0.9154 - val_tp: 156.0000 - val_fp: 86.0000 - val_tn: 440.0000 - val_fn: 107.0000 - val_accuracy: 0.7554 - val_precision: 0.6446 - val_recall: 0.5932 - val_auc: 0.8218 - val_prc: 0.7139 - 7s/epoch - 69ms/step\n",
      "Epoch 3/200\n",
      "103/103 - 7s - loss: 0.5069 - tp: 980.0000 - fp: 198.0000 - tn: 2256.0000 - fn: 247.0000 - accuracy: 0.8791 - precision: 0.8319 - recall: 0.7987 - auc: 0.9343 - prc: 0.8831 - val_loss: 0.8784 - val_tp: 173.0000 - val_fp: 77.0000 - val_tn: 449.0000 - val_fn: 90.0000 - val_accuracy: 0.7883 - val_precision: 0.6920 - val_recall: 0.6578 - val_auc: 0.8517 - val_prc: 0.7731 - 7s/epoch - 69ms/step\n",
      "Epoch 4/200\n",
      "103/103 - 7s - loss: 0.4240 - tp: 1023.0000 - fp: 174.0000 - tn: 2280.0000 - fn: 204.0000 - accuracy: 0.8973 - precision: 0.8546 - recall: 0.8337 - auc: 0.9533 - prc: 0.9174 - val_loss: 0.9040 - val_tp: 173.0000 - val_fp: 75.0000 - val_tn: 451.0000 - val_fn: 90.0000 - val_accuracy: 0.7909 - val_precision: 0.6976 - val_recall: 0.6578 - val_auc: 0.8486 - val_prc: 0.7508 - 7s/epoch - 70ms/step\n",
      "Epoch 5/200\n",
      "103/103 - 7s - loss: 0.3828 - tp: 1048.0000 - fp: 150.0000 - tn: 2304.0000 - fn: 179.0000 - accuracy: 0.9106 - precision: 0.8748 - recall: 0.8541 - auc: 0.9611 - prc: 0.9318 - val_loss: 0.9065 - val_tp: 180.0000 - val_fp: 78.0000 - val_tn: 448.0000 - val_fn: 83.0000 - val_accuracy: 0.7959 - val_precision: 0.6977 - val_recall: 0.6844 - val_auc: 0.8572 - val_prc: 0.7557 - 7s/epoch - 69ms/step\n",
      "Epoch 6/200\n",
      "103/103 - 7s - loss: 0.2794 - tp: 1091.0000 - fp: 115.0000 - tn: 2339.0000 - fn: 136.0000 - accuracy: 0.9318 - precision: 0.9046 - recall: 0.8892 - auc: 0.9776 - prc: 0.9597 - val_loss: 0.9474 - val_tp: 177.0000 - val_fp: 73.0000 - val_tn: 453.0000 - val_fn: 86.0000 - val_accuracy: 0.7985 - val_precision: 0.7080 - val_recall: 0.6730 - val_auc: 0.8517 - val_prc: 0.7376 - 7s/epoch - 70ms/step\n",
      "Epoch 7/200\n",
      "103/103 - 7s - loss: 0.2404 - tp: 1110.0000 - fp: 100.0000 - tn: 2354.0000 - fn: 117.0000 - accuracy: 0.9410 - precision: 0.9174 - recall: 0.9046 - auc: 0.9830 - prc: 0.9675 - val_loss: 1.0168 - val_tp: 181.0000 - val_fp: 70.0000 - val_tn: 456.0000 - val_fn: 82.0000 - val_accuracy: 0.8074 - val_precision: 0.7211 - val_recall: 0.6882 - val_auc: 0.8485 - val_prc: 0.7380 - 7s/epoch - 71ms/step\n",
      "Epoch 8/200\n",
      "103/103 - 7s - loss: 0.2033 - tp: 1139.0000 - fp: 75.0000 - tn: 2379.0000 - fn: 88.0000 - accuracy: 0.9557 - precision: 0.9382 - recall: 0.9283 - auc: 0.9877 - prc: 0.9781 - val_loss: 0.9078 - val_tp: 188.0000 - val_fp: 63.0000 - val_tn: 463.0000 - val_fn: 75.0000 - val_accuracy: 0.8251 - val_precision: 0.7490 - val_recall: 0.7148 - val_auc: 0.8664 - val_prc: 0.7828 - 7s/epoch - 70ms/step\n",
      "Epoch 8: early stopping\n",
      "9/9 [==============================] - 0s 20ms/step\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.9634 - tp: 183.0000 - fp: 77.0000 - tn: 451.0000 - fn: 81.0000 - accuracy: 0.8005 - precision: 0.7038 - recall: 0.6932 - auc: 0.8540 - prc: 0.7482\n",
      "\n",
      "=============\n",
      "PARAMS:\n",
      "random_state:4\n",
      "is_shuffle:True\n",
      "y_to_categorical:True\n",
      "rebalance:None\n",
      "cat_lentgh:1000\n",
      "hu:2000\n",
      "output_bias:[0.3247041633390123, -0.10468112848688473, -0.22002301210252034]\n",
      "loss:categorical_crossentropy\n",
      "dropout:True\n",
      "gpu:False\n",
      "set_class_weight:False\n",
      "save_check_point:False\n",
      "early_stopping:True\n",
      "patience:5\n",
      "epochs:200\n",
      "batch_size:12\n",
      "timeframe_in_ms:86400000\n",
      "take_profit_rate:0.1\n",
      "stop_loss_rate:0.07\n",
      "max_duration:10\n",
      "lags:60\n",
      "\n",
      "\n",
      "\n",
      "=============\n",
      "CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.74      0.70       110\n",
      "           1       0.74      0.65      0.69        69\n",
      "           2       0.72      0.68      0.70        85\n",
      "\n",
      "    accuracy                           0.70       264\n",
      "   macro avg       0.71      0.69      0.70       264\n",
      "weighted avg       0.70      0.70      0.70       264\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "=============\n",
      "CONFUSION MATRIX:\n",
      "[[81  9 20]\n",
      " [21 45  3]\n",
      " [20  7 58]]\n",
      "\n",
      "\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.9634 - tp: 183.0000 - fp: 77.0000 - tn: 451.0000 - fn: 81.0000 - accuracy: 0.8005 - precision: 0.7038 - recall: 0.6932 - auc: 0.8540 - prc: 0.7482\n",
      "\n",
      "======= Lap 5 =======\n",
      "Shuffling the data\n",
      "Completed. Train: 1227, Validation: 263, Test: 264\n",
      "Epoch 1/200\n",
      "103/103 - 9s - loss: 1.4743 - tp: 790.0000 - fp: 579.0000 - tn: 2403.0000 - fn: 701.0000 - accuracy: 0.7138 - precision: 0.5771 - recall: 0.5298 - auc: 0.7261 - prc: 0.5775 - val_loss: 1.1238 - val_tp: 146.0000 - val_fp: 93.0000 - val_tn: 433.0000 - val_fn: 117.0000 - val_accuracy: 0.7338 - val_precision: 0.6109 - val_recall: 0.5551 - val_auc: 0.7511 - val_prc: 0.6301 - 9s/epoch - 91ms/step\n",
      "Epoch 2/200\n",
      "103/103 - 8s - loss: 0.7635 - tp: 869.0000 - fp: 289.0000 - tn: 2165.0000 - fn: 358.0000 - accuracy: 0.8242 - precision: 0.7504 - recall: 0.7082 - auc: 0.8742 - prc: 0.7840 - val_loss: 1.0854 - val_tp: 152.0000 - val_fp: 93.0000 - val_tn: 433.0000 - val_fn: 111.0000 - val_accuracy: 0.7414 - val_precision: 0.6204 - val_recall: 0.5779 - val_auc: 0.7906 - val_prc: 0.6626 - 8s/epoch - 79ms/step\n",
      "Epoch 3/200\n",
      "103/103 - 8s - loss: 0.5302 - tp: 969.0000 - fp: 215.0000 - tn: 2239.0000 - fn: 258.0000 - accuracy: 0.8715 - precision: 0.8184 - recall: 0.7897 - auc: 0.9297 - prc: 0.8758 - val_loss: 0.9976 - val_tp: 163.0000 - val_fp: 84.0000 - val_tn: 442.0000 - val_fn: 100.0000 - val_accuracy: 0.7668 - val_precision: 0.6599 - val_recall: 0.6198 - val_auc: 0.8151 - val_prc: 0.6950 - 8s/epoch - 76ms/step\n",
      "Epoch 4/200\n",
      "103/103 - 8s - loss: 0.3455 - tp: 1048.0000 - fp: 145.0000 - tn: 2309.0000 - fn: 179.0000 - accuracy: 0.9120 - precision: 0.8785 - recall: 0.8541 - auc: 0.9663 - prc: 0.9385 - val_loss: 1.0533 - val_tp: 164.0000 - val_fp: 83.0000 - val_tn: 443.0000 - val_fn: 99.0000 - val_accuracy: 0.7693 - val_precision: 0.6640 - val_recall: 0.6236 - val_auc: 0.8118 - val_prc: 0.7026 - 8s/epoch - 76ms/step\n",
      "Epoch 5/200\n",
      "103/103 - 7s - loss: 0.3303 - tp: 1066.0000 - fp: 136.0000 - tn: 2318.0000 - fn: 161.0000 - accuracy: 0.9193 - precision: 0.8869 - recall: 0.8688 - auc: 0.9692 - prc: 0.9455 - val_loss: 1.0499 - val_tp: 175.0000 - val_fp: 79.0000 - val_tn: 447.0000 - val_fn: 88.0000 - val_accuracy: 0.7883 - val_precision: 0.6890 - val_recall: 0.6654 - val_auc: 0.8296 - val_prc: 0.7043 - 7s/epoch - 71ms/step\n",
      "Epoch 6/200\n",
      "103/103 - 7s - loss: 0.2970 - tp: 1095.0000 - fp: 113.0000 - tn: 2341.0000 - fn: 132.0000 - accuracy: 0.9334 - precision: 0.9065 - recall: 0.8924 - auc: 0.9746 - prc: 0.9502 - val_loss: 0.8792 - val_tp: 180.0000 - val_fp: 72.0000 - val_tn: 454.0000 - val_fn: 83.0000 - val_accuracy: 0.8035 - val_precision: 0.7143 - val_recall: 0.6844 - val_auc: 0.8616 - val_prc: 0.7636 - 7s/epoch - 73ms/step\n",
      "Epoch 7/200\n",
      "103/103 - 7s - loss: 0.1984 - tp: 1139.0000 - fp: 72.0000 - tn: 2382.0000 - fn: 88.0000 - accuracy: 0.9565 - precision: 0.9405 - recall: 0.9283 - auc: 0.9878 - prc: 0.9779 - val_loss: 1.1690 - val_tp: 174.0000 - val_fp: 82.0000 - val_tn: 444.0000 - val_fn: 89.0000 - val_accuracy: 0.7833 - val_precision: 0.6797 - val_recall: 0.6616 - val_auc: 0.8246 - val_prc: 0.7135 - 7s/epoch - 67ms/step\n",
      "Epoch 8/200\n",
      "103/103 - 7s - loss: 0.2015 - tp: 1134.0000 - fp: 83.0000 - tn: 2371.0000 - fn: 93.0000 - accuracy: 0.9522 - precision: 0.9318 - recall: 0.9242 - auc: 0.9877 - prc: 0.9766 - val_loss: 1.0608 - val_tp: 180.0000 - val_fp: 77.0000 - val_tn: 449.0000 - val_fn: 83.0000 - val_accuracy: 0.7972 - val_precision: 0.7004 - val_recall: 0.6844 - val_auc: 0.8322 - val_prc: 0.7201 - 7s/epoch - 67ms/step\n",
      "Epoch 9/200\n",
      "103/103 - 7s - loss: 0.1652 - tp: 1158.0000 - fp: 57.0000 - tn: 2397.0000 - fn: 69.0000 - accuracy: 0.9658 - precision: 0.9531 - recall: 0.9438 - auc: 0.9915 - prc: 0.9835 - val_loss: 1.0833 - val_tp: 171.0000 - val_fp: 83.0000 - val_tn: 443.0000 - val_fn: 92.0000 - val_accuracy: 0.7782 - val_precision: 0.6732 - val_recall: 0.6502 - val_auc: 0.8451 - val_prc: 0.7523 - 7s/epoch - 67ms/step\n",
      "Epoch 10/200\n",
      "103/103 - 7s - loss: 0.1467 - tp: 1172.0000 - fp: 51.0000 - tn: 2403.0000 - fn: 55.0000 - accuracy: 0.9712 - precision: 0.9583 - recall: 0.9552 - auc: 0.9930 - prc: 0.9865 - val_loss: 1.0859 - val_tp: 176.0000 - val_fp: 78.0000 - val_tn: 448.0000 - val_fn: 87.0000 - val_accuracy: 0.7909 - val_precision: 0.6929 - val_recall: 0.6692 - val_auc: 0.8456 - val_prc: 0.7316 - 7s/epoch - 65ms/step\n",
      "Epoch 11/200\n",
      "103/103 - 7s - loss: 0.0889 - tp: 1187.0000 - fp: 34.0000 - tn: 2420.0000 - fn: 40.0000 - accuracy: 0.9799 - precision: 0.9722 - recall: 0.9674 - auc: 0.9983 - prc: 0.9967 - val_loss: 1.0231 - val_tp: 184.0000 - val_fp: 74.0000 - val_tn: 452.0000 - val_fn: 79.0000 - val_accuracy: 0.8061 - val_precision: 0.7132 - val_recall: 0.6996 - val_auc: 0.8611 - val_prc: 0.7666 - 7s/epoch - 67ms/step\n",
      "Epoch 11: early stopping\n",
      "9/9 [==============================] - 0s 18ms/step\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 1.3430 - tp: 177.0000 - fp: 80.0000 - tn: 448.0000 - fn: 87.0000 - accuracy: 0.7891 - precision: 0.6887 - recall: 0.6705 - auc: 0.8193 - prc: 0.6984\n",
      "\n",
      "=============\n",
      "PARAMS:\n",
      "random_state:5\n",
      "is_shuffle:True\n",
      "y_to_categorical:True\n",
      "rebalance:None\n",
      "cat_lentgh:1000\n",
      "hu:2000\n",
      "output_bias:[0.30158872547364496, -0.14933875671132119, -0.1522499669186897]\n",
      "loss:categorical_crossentropy\n",
      "dropout:True\n",
      "gpu:False\n",
      "set_class_weight:False\n",
      "save_check_point:False\n",
      "early_stopping:True\n",
      "patience:5\n",
      "epochs:200\n",
      "batch_size:12\n",
      "timeframe_in_ms:86400000\n",
      "take_profit_rate:0.1\n",
      "stop_loss_rate:0.07\n",
      "max_duration:10\n",
      "lags:60\n",
      "\n",
      "\n",
      "\n",
      "=============\n",
      "CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.77      0.72       106\n",
      "           1       0.68      0.60      0.64        81\n",
      "           2       0.74      0.66      0.70        77\n",
      "\n",
      "    accuracy                           0.69       264\n",
      "   macro avg       0.70      0.68      0.69       264\n",
      "weighted avg       0.69      0.69      0.69       264\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "=============\n",
      "CONFUSION MATRIX:\n",
      "[[82 12 12]\n",
      " [26 49  6]\n",
      " [15 11 51]]\n",
      "\n",
      "\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 1.3430 - tp: 177.0000 - fp: 80.0000 - tn: 448.0000 - fn: 87.0000 - accuracy: 0.7891 - precision: 0.6887 - recall: 0.6705 - auc: 0.8193 - prc: 0.6984\n",
      "\n",
      "======= Lap 6 =======\n",
      "Shuffling the data\n",
      "Completed. Train: 1227, Validation: 263, Test: 264\n",
      "Epoch 1/200\n",
      "103/103 - 9s - loss: 1.4419 - tp: 777.0000 - fp: 616.0000 - tn: 2366.0000 - fn: 714.0000 - accuracy: 0.7027 - precision: 0.5578 - recall: 0.5211 - auc: 0.7182 - prc: 0.5643 - val_loss: 1.2873 - val_tp: 135.0000 - val_fp: 110.0000 - val_tn: 416.0000 - val_fn: 128.0000 - val_accuracy: 0.6984 - val_precision: 0.5510 - val_recall: 0.5133 - val_auc: 0.7345 - val_prc: 0.5713 - 9s/epoch - 90ms/step\n",
      "Epoch 2/200\n",
      "103/103 - 7s - loss: 0.7186 - tp: 880.0000 - fp: 284.0000 - tn: 2170.0000 - fn: 347.0000 - accuracy: 0.8286 - precision: 0.7560 - recall: 0.7172 - auc: 0.8837 - prc: 0.7926 - val_loss: 1.1191 - val_tp: 158.0000 - val_fp: 91.0000 - val_tn: 435.0000 - val_fn: 105.0000 - val_accuracy: 0.7516 - val_precision: 0.6345 - val_recall: 0.6008 - val_auc: 0.7940 - val_prc: 0.6813 - 7s/epoch - 69ms/step\n",
      "Epoch 3/200\n",
      "103/103 - 7s - loss: 0.5030 - tp: 982.0000 - fp: 208.0000 - tn: 2246.0000 - fn: 245.0000 - accuracy: 0.8769 - precision: 0.8252 - recall: 0.8003 - auc: 0.9359 - prc: 0.8836 - val_loss: 0.8936 - val_tp: 172.0000 - val_fp: 74.0000 - val_tn: 452.0000 - val_fn: 91.0000 - val_accuracy: 0.7909 - val_precision: 0.6992 - val_recall: 0.6540 - val_auc: 0.8433 - val_prc: 0.7608 - 7s/epoch - 73ms/step\n",
      "Epoch 4/200\n",
      "103/103 - 7s - loss: 0.4109 - tp: 1046.0000 - fp: 157.0000 - tn: 2297.0000 - fn: 181.0000 - accuracy: 0.9082 - precision: 0.8695 - recall: 0.8525 - auc: 0.9545 - prc: 0.9154 - val_loss: 1.0008 - val_tp: 165.0000 - val_fp: 86.0000 - val_tn: 440.0000 - val_fn: 98.0000 - val_accuracy: 0.7668 - val_precision: 0.6574 - val_recall: 0.6274 - val_auc: 0.8299 - val_prc: 0.7296 - 7s/epoch - 70ms/step\n",
      "Epoch 5/200\n",
      "103/103 - 7s - loss: 0.3417 - tp: 1056.0000 - fp: 135.0000 - tn: 2319.0000 - fn: 171.0000 - accuracy: 0.9169 - precision: 0.8866 - recall: 0.8606 - auc: 0.9680 - prc: 0.9430 - val_loss: 1.1110 - val_tp: 173.0000 - val_fp: 84.0000 - val_tn: 442.0000 - val_fn: 90.0000 - val_accuracy: 0.7795 - val_precision: 0.6732 - val_recall: 0.6578 - val_auc: 0.8253 - val_prc: 0.7044 - 7s/epoch - 69ms/step\n",
      "Epoch 6/200\n",
      "103/103 - 7s - loss: 0.2845 - tp: 1086.0000 - fp: 123.0000 - tn: 2331.0000 - fn: 141.0000 - accuracy: 0.9283 - precision: 0.8983 - recall: 0.8851 - auc: 0.9768 - prc: 0.9581 - val_loss: 1.0374 - val_tp: 189.0000 - val_fp: 70.0000 - val_tn: 456.0000 - val_fn: 74.0000 - val_accuracy: 0.8175 - val_precision: 0.7297 - val_recall: 0.7186 - val_auc: 0.8454 - val_prc: 0.7391 - 7s/epoch - 69ms/step\n",
      "Epoch 7/200\n",
      "103/103 - 7s - loss: 0.2246 - tp: 1119.0000 - fp: 98.0000 - tn: 2356.0000 - fn: 108.0000 - accuracy: 0.9440 - precision: 0.9195 - recall: 0.9120 - auc: 0.9852 - prc: 0.9724 - val_loss: 0.9561 - val_tp: 188.0000 - val_fp: 67.0000 - val_tn: 459.0000 - val_fn: 75.0000 - val_accuracy: 0.8200 - val_precision: 0.7373 - val_recall: 0.7148 - val_auc: 0.8620 - val_prc: 0.7758 - 7s/epoch - 70ms/step\n",
      "Epoch 8/200\n",
      "103/103 - 7s - loss: 0.1534 - tp: 1153.0000 - fp: 67.0000 - tn: 2387.0000 - fn: 74.0000 - accuracy: 0.9617 - precision: 0.9451 - recall: 0.9397 - auc: 0.9939 - prc: 0.9883 - val_loss: 0.9481 - val_tp: 187.0000 - val_fp: 63.0000 - val_tn: 463.0000 - val_fn: 76.0000 - val_accuracy: 0.8238 - val_precision: 0.7480 - val_recall: 0.7110 - val_auc: 0.8583 - val_prc: 0.7613 - 7s/epoch - 71ms/step\n",
      "Epoch 8: early stopping\n",
      "9/9 [==============================] - 0s 17ms/step\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 1.0639 - tp: 177.0000 - fp: 82.0000 - tn: 446.0000 - fn: 87.0000 - accuracy: 0.7866 - precision: 0.6834 - recall: 0.6705 - auc: 0.8340 - prc: 0.7135\n",
      "\n",
      "=============\n",
      "PARAMS:\n",
      "random_state:6\n",
      "is_shuffle:True\n",
      "y_to_categorical:True\n",
      "rebalance:None\n",
      "cat_lentgh:1000\n",
      "hu:2000\n",
      "output_bias:[0.29952065873726885, -0.1294086887058625, -0.17011198181936676]\n",
      "loss:categorical_crossentropy\n",
      "dropout:True\n",
      "gpu:False\n",
      "set_class_weight:False\n",
      "save_check_point:False\n",
      "early_stopping:True\n",
      "patience:5\n",
      "epochs:200\n",
      "batch_size:12\n",
      "timeframe_in_ms:86400000\n",
      "take_profit_rate:0.1\n",
      "stop_loss_rate:0.07\n",
      "max_duration:10\n",
      "lags:60\n",
      "\n",
      "\n",
      "\n",
      "=============\n",
      "CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.69      0.68       114\n",
      "           1       0.72      0.68      0.70        74\n",
      "           2       0.64      0.66      0.65        76\n",
      "\n",
      "    accuracy                           0.68       264\n",
      "   macro avg       0.68      0.68      0.68       264\n",
      "weighted avg       0.68      0.68      0.68       264\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "=============\n",
      "CONFUSION MATRIX:\n",
      "[[79 15 20]\n",
      " [16 50  8]\n",
      " [22  4 50]]\n",
      "\n",
      "\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 1.0639 - tp: 177.0000 - fp: 82.0000 - tn: 446.0000 - fn: 87.0000 - accuracy: 0.7866 - precision: 0.6834 - recall: 0.6705 - auc: 0.8340 - prc: 0.7135\n",
      "\n",
      "======= Lap 7 =======\n",
      "Shuffling the data\n",
      "Completed. Train: 1227, Validation: 263, Test: 264\n",
      "Epoch 1/200\n",
      "103/103 - 9s - loss: 1.4087 - tp: 761.0000 - fp: 610.0000 - tn: 2372.0000 - fn: 730.0000 - accuracy: 0.7004 - precision: 0.5551 - recall: 0.5104 - auc: 0.7175 - prc: 0.5574 - val_loss: 1.2250 - val_tp: 135.0000 - val_fp: 90.0000 - val_tn: 436.0000 - val_fn: 128.0000 - val_accuracy: 0.7237 - val_precision: 0.6000 - val_recall: 0.5133 - val_auc: 0.7349 - val_prc: 0.5745 - 9s/epoch - 91ms/step\n",
      "Epoch 2/200\n",
      "103/103 - 7s - loss: 0.7142 - tp: 875.0000 - fp: 275.0000 - tn: 2179.0000 - fn: 352.0000 - accuracy: 0.8297 - precision: 0.7609 - recall: 0.7131 - auc: 0.8865 - prc: 0.8029 - val_loss: 1.0490 - val_tp: 162.0000 - val_fp: 84.0000 - val_tn: 442.0000 - val_fn: 101.0000 - val_accuracy: 0.7655 - val_precision: 0.6585 - val_recall: 0.6160 - val_auc: 0.7978 - val_prc: 0.6617 - 7s/epoch - 69ms/step\n",
      "Epoch 3/200\n",
      "103/103 - 7s - loss: 0.6073 - tp: 949.0000 - fp: 228.0000 - tn: 2226.0000 - fn: 278.0000 - accuracy: 0.8625 - precision: 0.8063 - recall: 0.7734 - auc: 0.9165 - prc: 0.8511 - val_loss: 1.1495 - val_tp: 162.0000 - val_fp: 87.0000 - val_tn: 439.0000 - val_fn: 101.0000 - val_accuracy: 0.7617 - val_precision: 0.6506 - val_recall: 0.6160 - val_auc: 0.7952 - val_prc: 0.6665 - 7s/epoch - 70ms/step\n",
      "Epoch 4/200\n",
      "103/103 - 7s - loss: 0.4007 - tp: 1040.0000 - fp: 150.0000 - tn: 2304.0000 - fn: 187.0000 - accuracy: 0.9084 - precision: 0.8739 - recall: 0.8476 - auc: 0.9566 - prc: 0.9226 - val_loss: 1.1152 - val_tp: 172.0000 - val_fp: 78.0000 - val_tn: 448.0000 - val_fn: 91.0000 - val_accuracy: 0.7858 - val_precision: 0.6880 - val_recall: 0.6540 - val_auc: 0.8170 - val_prc: 0.6678 - 7s/epoch - 70ms/step\n",
      "Epoch 5/200\n",
      "103/103 - 7s - loss: 0.2713 - tp: 1090.0000 - fp: 113.0000 - tn: 2341.0000 - fn: 137.0000 - accuracy: 0.9321 - precision: 0.9061 - recall: 0.8883 - auc: 0.9795 - prc: 0.9629 - val_loss: 1.1205 - val_tp: 167.0000 - val_fp: 82.0000 - val_tn: 444.0000 - val_fn: 96.0000 - val_accuracy: 0.7744 - val_precision: 0.6707 - val_recall: 0.6350 - val_auc: 0.8180 - val_prc: 0.6859 - 7s/epoch - 69ms/step\n",
      "Epoch 6/200\n",
      "103/103 - 7s - loss: 0.2420 - tp: 1108.0000 - fp: 105.0000 - tn: 2349.0000 - fn: 119.0000 - accuracy: 0.9391 - precision: 0.9134 - recall: 0.9030 - auc: 0.9826 - prc: 0.9678 - val_loss: 0.9864 - val_tp: 188.0000 - val_fp: 62.0000 - val_tn: 464.0000 - val_fn: 75.0000 - val_accuracy: 0.8264 - val_precision: 0.7520 - val_recall: 0.7148 - val_auc: 0.8550 - val_prc: 0.7642 - 7s/epoch - 70ms/step\n",
      "Epoch 7/200\n",
      "103/103 - 7s - loss: 0.2272 - tp: 1119.0000 - fp: 99.0000 - tn: 2355.0000 - fn: 108.0000 - accuracy: 0.9438 - precision: 0.9187 - recall: 0.9120 - auc: 0.9845 - prc: 0.9714 - val_loss: 1.1519 - val_tp: 162.0000 - val_fp: 84.0000 - val_tn: 442.0000 - val_fn: 101.0000 - val_accuracy: 0.7655 - val_precision: 0.6585 - val_recall: 0.6160 - val_auc: 0.8200 - val_prc: 0.7012 - 7s/epoch - 69ms/step\n",
      "Epoch 8/200\n",
      "103/103 - 7s - loss: 0.1683 - tp: 1152.0000 - fp: 66.0000 - tn: 2388.0000 - fn: 75.0000 - accuracy: 0.9617 - precision: 0.9458 - recall: 0.9389 - auc: 0.9920 - prc: 0.9850 - val_loss: 1.0585 - val_tp: 186.0000 - val_fp: 71.0000 - val_tn: 455.0000 - val_fn: 77.0000 - val_accuracy: 0.8124 - val_precision: 0.7237 - val_recall: 0.7072 - val_auc: 0.8447 - val_prc: 0.7410 - 7s/epoch - 70ms/step\n",
      "Epoch 9/200\n",
      "103/103 - 7s - loss: 0.1299 - tp: 1169.0000 - fp: 48.0000 - tn: 2406.0000 - fn: 58.0000 - accuracy: 0.9712 - precision: 0.9606 - recall: 0.9527 - auc: 0.9950 - prc: 0.9907 - val_loss: 0.9934 - val_tp: 179.0000 - val_fp: 71.0000 - val_tn: 455.0000 - val_fn: 84.0000 - val_accuracy: 0.8035 - val_precision: 0.7160 - val_recall: 0.6806 - val_auc: 0.8615 - val_prc: 0.7564 - 7s/epoch - 70ms/step\n",
      "Epoch 10/200\n",
      "103/103 - 7s - loss: 0.1366 - tp: 1161.0000 - fp: 58.0000 - tn: 2396.0000 - fn: 66.0000 - accuracy: 0.9663 - precision: 0.9524 - recall: 0.9462 - auc: 0.9943 - prc: 0.9889 - val_loss: 1.1307 - val_tp: 183.0000 - val_fp: 69.0000 - val_tn: 457.0000 - val_fn: 80.0000 - val_accuracy: 0.8112 - val_precision: 0.7262 - val_recall: 0.6958 - val_auc: 0.8491 - val_prc: 0.7541 - 7s/epoch - 69ms/step\n",
      "Epoch 11/200\n",
      "103/103 - 7s - loss: 0.0882 - tp: 1195.0000 - fp: 25.0000 - tn: 2429.0000 - fn: 32.0000 - accuracy: 0.9845 - precision: 0.9795 - recall: 0.9739 - auc: 0.9979 - prc: 0.9960 - val_loss: 1.1016 - val_tp: 186.0000 - val_fp: 73.0000 - val_tn: 453.0000 - val_fn: 77.0000 - val_accuracy: 0.8099 - val_precision: 0.7181 - val_recall: 0.7072 - val_auc: 0.8577 - val_prc: 0.7660 - 7s/epoch - 70ms/step\n",
      "Epoch 11: early stopping\n",
      "9/9 [==============================] - 0s 19ms/step\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 1.1952 - tp: 189.0000 - fp: 69.0000 - tn: 459.0000 - fn: 75.0000 - accuracy: 0.8182 - precision: 0.7326 - recall: 0.7159 - auc: 0.8427 - prc: 0.7439\n",
      "\n",
      "=============\n",
      "PARAMS:\n",
      "random_state:7\n",
      "is_shuffle:True\n",
      "y_to_categorical:True\n",
      "rebalance:None\n",
      "cat_lentgh:1000\n",
      "hu:2000\n",
      "output_bias:[0.2662696174562858, -0.1174862850851126, -0.14878329180703792]\n",
      "loss:categorical_crossentropy\n",
      "dropout:True\n",
      "gpu:False\n",
      "set_class_weight:False\n",
      "save_check_point:False\n",
      "early_stopping:True\n",
      "patience:5\n",
      "epochs:200\n",
      "batch_size:12\n",
      "timeframe_in_ms:86400000\n",
      "take_profit_rate:0.1\n",
      "stop_loss_rate:0.07\n",
      "max_duration:10\n",
      "lags:60\n",
      "\n",
      "\n",
      "\n",
      "=============\n",
      "CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.74      0.76       122\n",
      "           1       0.69      0.68      0.68        78\n",
      "           2       0.69      0.77      0.73        64\n",
      "\n",
      "    accuracy                           0.73       264\n",
      "   macro avg       0.72      0.73      0.72       264\n",
      "weighted avg       0.73      0.73      0.73       264\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "=============\n",
      "CONFUSION MATRIX:\n",
      "[[90 18 14]\n",
      " [17 53  8]\n",
      " [ 9  6 49]]\n",
      "\n",
      "\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 1.1952 - tp: 189.0000 - fp: 69.0000 - tn: 459.0000 - fn: 75.0000 - accuracy: 0.8182 - precision: 0.7326 - recall: 0.7159 - auc: 0.8427 - prc: 0.7439\n",
      "\n",
      "======= Lap 8 =======\n",
      "Shuffling the data\n",
      "Completed. Train: 1227, Validation: 263, Test: 264\n",
      "Epoch 1/200\n",
      "103/103 - 10s - loss: 1.6527 - tp: 745.0000 - fp: 632.0000 - tn: 2350.0000 - fn: 746.0000 - accuracy: 0.6919 - precision: 0.5410 - recall: 0.4997 - auc: 0.6998 - prc: 0.5436 - val_loss: 1.0154 - val_tp: 157.0000 - val_fp: 86.0000 - val_tn: 440.0000 - val_fn: 106.0000 - val_accuracy: 0.7567 - val_precision: 0.6461 - val_recall: 0.5970 - val_auc: 0.7948 - val_prc: 0.6631 - 10s/epoch - 94ms/step\n",
      "Epoch 2/200\n",
      "103/103 - 7s - loss: 0.7784 - tp: 851.0000 - fp: 305.0000 - tn: 2149.0000 - fn: 376.0000 - accuracy: 0.8150 - precision: 0.7362 - recall: 0.6936 - auc: 0.8701 - prc: 0.7723 - val_loss: 0.9859 - val_tp: 167.0000 - val_fp: 80.0000 - val_tn: 446.0000 - val_fn: 96.0000 - val_accuracy: 0.7769 - val_precision: 0.6761 - val_recall: 0.6350 - val_auc: 0.8152 - val_prc: 0.6871 - 7s/epoch - 69ms/step\n",
      "Epoch 3/200\n",
      "103/103 - 7s - loss: 0.5545 - tp: 956.0000 - fp: 226.0000 - tn: 2228.0000 - fn: 271.0000 - accuracy: 0.8650 - precision: 0.8088 - recall: 0.7791 - auc: 0.9258 - prc: 0.8713 - val_loss: 0.8802 - val_tp: 170.0000 - val_fp: 71.0000 - val_tn: 455.0000 - val_fn: 93.0000 - val_accuracy: 0.7921 - val_precision: 0.7054 - val_recall: 0.6464 - val_auc: 0.8401 - val_prc: 0.7410 - 7s/epoch - 70ms/step\n",
      "Epoch 4/200\n",
      "103/103 - 7s - loss: 0.4273 - tp: 1017.0000 - fp: 173.0000 - tn: 2281.0000 - fn: 210.0000 - accuracy: 0.8960 - precision: 0.8546 - recall: 0.8289 - auc: 0.9522 - prc: 0.9182 - val_loss: 1.0987 - val_tp: 165.0000 - val_fp: 87.0000 - val_tn: 439.0000 - val_fn: 98.0000 - val_accuracy: 0.7655 - val_precision: 0.6548 - val_recall: 0.6274 - val_auc: 0.8141 - val_prc: 0.6705 - 7s/epoch - 69ms/step\n",
      "Epoch 5/200\n",
      "103/103 - 7s - loss: 0.3429 - tp: 1066.0000 - fp: 138.0000 - tn: 2316.0000 - fn: 161.0000 - accuracy: 0.9188 - precision: 0.8854 - recall: 0.8688 - auc: 0.9681 - prc: 0.9408 - val_loss: 1.0718 - val_tp: 176.0000 - val_fp: 78.0000 - val_tn: 448.0000 - val_fn: 87.0000 - val_accuracy: 0.7909 - val_precision: 0.6929 - val_recall: 0.6692 - val_auc: 0.8295 - val_prc: 0.6867 - 7s/epoch - 70ms/step\n",
      "Epoch 6/200\n",
      "103/103 - 7s - loss: 0.1934 - tp: 1134.0000 - fp: 75.0000 - tn: 2379.0000 - fn: 93.0000 - accuracy: 0.9544 - precision: 0.9380 - recall: 0.9242 - auc: 0.9899 - prc: 0.9814 - val_loss: 0.8961 - val_tp: 180.0000 - val_fp: 74.0000 - val_tn: 452.0000 - val_fn: 83.0000 - val_accuracy: 0.8010 - val_precision: 0.7087 - val_recall: 0.6844 - val_auc: 0.8551 - val_prc: 0.7493 - 7s/epoch - 70ms/step\n",
      "Epoch 7/200\n",
      "103/103 - 7s - loss: 0.2012 - tp: 1138.0000 - fp: 77.0000 - tn: 2377.0000 - fn: 89.0000 - accuracy: 0.9549 - precision: 0.9366 - recall: 0.9275 - auc: 0.9881 - prc: 0.9782 - val_loss: 1.1912 - val_tp: 177.0000 - val_fp: 81.0000 - val_tn: 445.0000 - val_fn: 86.0000 - val_accuracy: 0.7883 - val_precision: 0.6860 - val_recall: 0.6730 - val_auc: 0.8242 - val_prc: 0.7014 - 7s/epoch - 69ms/step\n",
      "Epoch 8/200\n",
      "103/103 - 7s - loss: 0.2175 - tp: 1120.0000 - fp: 96.0000 - tn: 2358.0000 - fn: 107.0000 - accuracy: 0.9449 - precision: 0.9211 - recall: 0.9128 - auc: 0.9860 - prc: 0.9731 - val_loss: 0.9736 - val_tp: 184.0000 - val_fp: 73.0000 - val_tn: 453.0000 - val_fn: 79.0000 - val_accuracy: 0.8074 - val_precision: 0.7160 - val_recall: 0.6996 - val_auc: 0.8523 - val_prc: 0.7345 - 7s/epoch - 68ms/step\n",
      "Epoch 8: early stopping\n",
      "9/9 [==============================] - 0s 18ms/step\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.9982 - tp: 181.0000 - fp: 74.0000 - tn: 454.0000 - fn: 83.0000 - accuracy: 0.8018 - precision: 0.7098 - recall: 0.6856 - auc: 0.8422 - prc: 0.7314\n",
      "\n",
      "=============\n",
      "PARAMS:\n",
      "random_state:8\n",
      "is_shuffle:True\n",
      "y_to_categorical:True\n",
      "rebalance:None\n",
      "cat_lentgh:1000\n",
      "hu:2000\n",
      "output_bias:[0.3457998805157116, -0.14440645604732114, -0.2013933874214353]\n",
      "loss:categorical_crossentropy\n",
      "dropout:True\n",
      "gpu:False\n",
      "set_class_weight:False\n",
      "save_check_point:False\n",
      "early_stopping:True\n",
      "patience:5\n",
      "epochs:200\n",
      "batch_size:12\n",
      "timeframe_in_ms:86400000\n",
      "take_profit_rate:0.1\n",
      "stop_loss_rate:0.07\n",
      "max_duration:10\n",
      "lags:60\n",
      "\n",
      "\n",
      "\n",
      "=============\n",
      "CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.76      0.70        98\n",
      "           1       0.72      0.74      0.73        87\n",
      "           2       0.75      0.59      0.66        79\n",
      "\n",
      "    accuracy                           0.70       264\n",
      "   macro avg       0.71      0.70      0.70       264\n",
      "weighted avg       0.71      0.70      0.70       264\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "=============\n",
      "CONFUSION MATRIX:\n",
      "[[74 16  8]\n",
      " [15 64  8]\n",
      " [23  9 47]]\n",
      "\n",
      "\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.9982 - tp: 181.0000 - fp: 74.0000 - tn: 454.0000 - fn: 83.0000 - accuracy: 0.8018 - precision: 0.7098 - recall: 0.6856 - auc: 0.8422 - prc: 0.7314\n",
      "\n",
      "======= Lap 9 =======\n",
      "Shuffling the data\n",
      "Completed. Train: 1227, Validation: 263, Test: 264\n",
      "Epoch 1/200\n",
      "103/103 - 9s - loss: 1.4877 - tp: 778.0000 - fp: 615.0000 - tn: 2367.0000 - fn: 713.0000 - accuracy: 0.7031 - precision: 0.5585 - recall: 0.5218 - auc: 0.7160 - prc: 0.5609 - val_loss: 1.1432 - val_tp: 139.0000 - val_fp: 103.0000 - val_tn: 423.0000 - val_fn: 124.0000 - val_accuracy: 0.7123 - val_precision: 0.5744 - val_recall: 0.5285 - val_auc: 0.7484 - val_prc: 0.6297 - 9s/epoch - 89ms/step\n",
      "Epoch 2/200\n",
      "103/103 - 7s - loss: 0.7455 - tp: 851.0000 - fp: 316.0000 - tn: 2138.0000 - fn: 376.0000 - accuracy: 0.8120 - precision: 0.7292 - recall: 0.6936 - auc: 0.8727 - prc: 0.7827 - val_loss: 0.9931 - val_tp: 165.0000 - val_fp: 80.0000 - val_tn: 446.0000 - val_fn: 98.0000 - val_accuracy: 0.7744 - val_precision: 0.6735 - val_recall: 0.6274 - val_auc: 0.8166 - val_prc: 0.7176 - 7s/epoch - 69ms/step\n",
      "Epoch 3/200\n",
      "103/103 - 7s - loss: 0.4934 - tp: 983.0000 - fp: 205.0000 - tn: 2249.0000 - fn: 244.0000 - accuracy: 0.8780 - precision: 0.8274 - recall: 0.8011 - auc: 0.9368 - prc: 0.8896 - val_loss: 0.9817 - val_tp: 157.0000 - val_fp: 89.0000 - val_tn: 437.0000 - val_fn: 106.0000 - val_accuracy: 0.7529 - val_precision: 0.6382 - val_recall: 0.5970 - val_auc: 0.8109 - val_prc: 0.6888 - 7s/epoch - 68ms/step\n",
      "Epoch 4/200\n",
      "103/103 - 7s - loss: 0.3913 - tp: 1022.0000 - fp: 164.0000 - tn: 2290.0000 - fn: 205.0000 - accuracy: 0.8998 - precision: 0.8617 - recall: 0.8329 - auc: 0.9580 - prc: 0.9219 - val_loss: 1.0523 - val_tp: 179.0000 - val_fp: 72.0000 - val_tn: 454.0000 - val_fn: 84.0000 - val_accuracy: 0.8023 - val_precision: 0.7131 - val_recall: 0.6806 - val_auc: 0.8378 - val_prc: 0.7511 - 7s/epoch - 70ms/step\n",
      "Epoch 5/200\n",
      "103/103 - 7s - loss: 0.3254 - tp: 1062.0000 - fp: 138.0000 - tn: 2316.0000 - fn: 165.0000 - accuracy: 0.9177 - precision: 0.8850 - recall: 0.8655 - auc: 0.9709 - prc: 0.9488 - val_loss: 1.1068 - val_tp: 173.0000 - val_fp: 77.0000 - val_tn: 449.0000 - val_fn: 90.0000 - val_accuracy: 0.7883 - val_precision: 0.6920 - val_recall: 0.6578 - val_auc: 0.8203 - val_prc: 0.6898 - 7s/epoch - 68ms/step\n",
      "Epoch 6/200\n",
      "103/103 - 7s - loss: 0.2681 - tp: 1098.0000 - fp: 105.0000 - tn: 2349.0000 - fn: 129.0000 - accuracy: 0.9364 - precision: 0.9127 - recall: 0.8949 - auc: 0.9789 - prc: 0.9616 - val_loss: 1.0993 - val_tp: 176.0000 - val_fp: 80.0000 - val_tn: 446.0000 - val_fn: 87.0000 - val_accuracy: 0.7883 - val_precision: 0.6875 - val_recall: 0.6692 - val_auc: 0.8249 - val_prc: 0.7112 - 7s/epoch - 68ms/step\n",
      "Epoch 7/200\n",
      "103/103 - 7s - loss: 0.2092 - tp: 1134.0000 - fp: 78.0000 - tn: 2376.0000 - fn: 93.0000 - accuracy: 0.9535 - precision: 0.9356 - recall: 0.9242 - auc: 0.9868 - prc: 0.9737 - val_loss: 1.0304 - val_tp: 178.0000 - val_fp: 72.0000 - val_tn: 454.0000 - val_fn: 85.0000 - val_accuracy: 0.8010 - val_precision: 0.7120 - val_recall: 0.6768 - val_auc: 0.8436 - val_prc: 0.7635 - 7s/epoch - 68ms/step\n",
      "Epoch 8/200\n",
      "103/103 - 7s - loss: 0.2138 - tp: 1127.0000 - fp: 88.0000 - tn: 2366.0000 - fn: 100.0000 - accuracy: 0.9489 - precision: 0.9276 - recall: 0.9185 - auc: 0.9865 - prc: 0.9757 - val_loss: 1.2621 - val_tp: 181.0000 - val_fp: 73.0000 - val_tn: 453.0000 - val_fn: 82.0000 - val_accuracy: 0.8035 - val_precision: 0.7126 - val_recall: 0.6882 - val_auc: 0.8302 - val_prc: 0.7063 - 7s/epoch - 68ms/step\n",
      "Epoch 8: early stopping\n",
      "9/9 [==============================] - 0s 18ms/step\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.8784 - tp: 182.0000 - fp: 68.0000 - tn: 460.0000 - fn: 82.0000 - accuracy: 0.8106 - precision: 0.7280 - recall: 0.6894 - auc: 0.8757 - prc: 0.7876\n",
      "\n",
      "=============\n",
      "PARAMS:\n",
      "random_state:9\n",
      "is_shuffle:True\n",
      "y_to_categorical:True\n",
      "rebalance:None\n",
      "cat_lentgh:1000\n",
      "hu:2000\n",
      "output_bias:[0.2774528716400047, -0.11579977843865, -0.16165308293478461]\n",
      "loss:categorical_crossentropy\n",
      "dropout:True\n",
      "gpu:False\n",
      "set_class_weight:False\n",
      "save_check_point:False\n",
      "early_stopping:True\n",
      "patience:5\n",
      "epochs:200\n",
      "batch_size:12\n",
      "timeframe_in_ms:86400000\n",
      "take_profit_rate:0.1\n",
      "stop_loss_rate:0.07\n",
      "max_duration:10\n",
      "lags:60\n",
      "\n",
      "\n",
      "\n",
      "=============\n",
      "CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.76      0.75       120\n",
      "           1       0.68      0.73      0.71        75\n",
      "           2       0.70      0.61      0.65        69\n",
      "\n",
      "    accuracy                           0.71       264\n",
      "   macro avg       0.71      0.70      0.70       264\n",
      "weighted avg       0.71      0.71      0.71       264\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "=============\n",
      "CONFUSION MATRIX:\n",
      "[[91 16 13]\n",
      " [15 55  5]\n",
      " [17 10 42]]\n",
      "\n",
      "\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.8784 - tp: 182.0000 - fp: 68.0000 - tn: 460.0000 - fn: 82.0000 - accuracy: 0.8106 - precision: 0.7280 - recall: 0.6894 - auc: 0.8757 - prc: 0.7876\n",
      "\n",
      "======= Lap 10 =======\n",
      "Shuffling the data\n",
      "Completed. Train: 1227, Validation: 263, Test: 264\n",
      "Epoch 1/200\n",
      "103/103 - 9s - loss: 1.4936 - tp: 762.0000 - fp: 612.0000 - tn: 2370.0000 - fn: 729.0000 - accuracy: 0.7002 - precision: 0.5546 - recall: 0.5111 - auc: 0.7198 - prc: 0.5698 - val_loss: 1.1965 - val_tp: 149.0000 - val_fp: 88.0000 - val_tn: 438.0000 - val_fn: 114.0000 - val_accuracy: 0.7440 - val_precision: 0.6287 - val_recall: 0.5665 - val_auc: 0.7599 - val_prc: 0.5925 - 9s/epoch - 91ms/step\n",
      "Epoch 2/200\n",
      "103/103 - 7s - loss: 0.9145 - tp: 833.0000 - fp: 328.0000 - tn: 2126.0000 - fn: 394.0000 - accuracy: 0.8039 - precision: 0.7175 - recall: 0.6789 - auc: 0.8475 - prc: 0.7352 - val_loss: 0.9510 - val_tp: 175.0000 - val_fp: 72.0000 - val_tn: 454.0000 - val_fn: 88.0000 - val_accuracy: 0.7972 - val_precision: 0.7085 - val_recall: 0.6654 - val_auc: 0.8271 - val_prc: 0.7306 - 7s/epoch - 69ms/step\n",
      "Epoch 3/200\n",
      "103/103 - 7s - loss: 0.5314 - tp: 957.0000 - fp: 228.0000 - tn: 2226.0000 - fn: 270.0000 - accuracy: 0.8647 - precision: 0.8076 - recall: 0.7800 - auc: 0.9311 - prc: 0.8808 - val_loss: 1.0341 - val_tp: 165.0000 - val_fp: 87.0000 - val_tn: 439.0000 - val_fn: 98.0000 - val_accuracy: 0.7655 - val_precision: 0.6548 - val_recall: 0.6274 - val_auc: 0.8161 - val_prc: 0.6862 - 7s/epoch - 69ms/step\n",
      "Epoch 4/200\n",
      "103/103 - 7s - loss: 0.3792 - tp: 1042.0000 - fp: 154.0000 - tn: 2300.0000 - fn: 185.0000 - accuracy: 0.9079 - precision: 0.8712 - recall: 0.8492 - auc: 0.9608 - prc: 0.9298 - val_loss: 0.8567 - val_tp: 179.0000 - val_fp: 71.0000 - val_tn: 455.0000 - val_fn: 84.0000 - val_accuracy: 0.8035 - val_precision: 0.7160 - val_recall: 0.6806 - val_auc: 0.8509 - val_prc: 0.7299 - 7s/epoch - 70ms/step\n",
      "Epoch 5/200\n",
      "103/103 - 7s - loss: 0.3309 - tp: 1063.0000 - fp: 136.0000 - tn: 2318.0000 - fn: 164.0000 - accuracy: 0.9185 - precision: 0.8866 - recall: 0.8663 - auc: 0.9697 - prc: 0.9453 - val_loss: 0.9737 - val_tp: 177.0000 - val_fp: 76.0000 - val_tn: 450.0000 - val_fn: 86.0000 - val_accuracy: 0.7947 - val_precision: 0.6996 - val_recall: 0.6730 - val_auc: 0.8360 - val_prc: 0.7322 - 7s/epoch - 68ms/step\n",
      "Epoch 6/200\n",
      "103/103 - 7s - loss: 0.2962 - tp: 1096.0000 - fp: 113.0000 - tn: 2341.0000 - fn: 131.0000 - accuracy: 0.9337 - precision: 0.9065 - recall: 0.8932 - auc: 0.9744 - prc: 0.9505 - val_loss: 1.0626 - val_tp: 183.0000 - val_fp: 67.0000 - val_tn: 459.0000 - val_fn: 80.0000 - val_accuracy: 0.8137 - val_precision: 0.7320 - val_recall: 0.6958 - val_auc: 0.8471 - val_prc: 0.7353 - 7s/epoch - 69ms/step\n",
      "Epoch 7/200\n",
      "103/103 - 7s - loss: 0.2516 - tp: 1107.0000 - fp: 106.0000 - tn: 2348.0000 - fn: 120.0000 - accuracy: 0.9386 - precision: 0.9126 - recall: 0.9022 - auc: 0.9814 - prc: 0.9664 - val_loss: 0.9570 - val_tp: 183.0000 - val_fp: 76.0000 - val_tn: 450.0000 - val_fn: 80.0000 - val_accuracy: 0.8023 - val_precision: 0.7066 - val_recall: 0.6958 - val_auc: 0.8614 - val_prc: 0.7795 - 7s/epoch - 69ms/step\n",
      "Epoch 8/200\n",
      "103/103 - 7s - loss: 0.1866 - tp: 1134.0000 - fp: 80.0000 - tn: 2374.0000 - fn: 93.0000 - accuracy: 0.9530 - precision: 0.9341 - recall: 0.9242 - auc: 0.9900 - prc: 0.9814 - val_loss: 0.9786 - val_tp: 183.0000 - val_fp: 72.0000 - val_tn: 454.0000 - val_fn: 80.0000 - val_accuracy: 0.8074 - val_precision: 0.7176 - val_recall: 0.6958 - val_auc: 0.8557 - val_prc: 0.7581 - 7s/epoch - 69ms/step\n",
      "Epoch 9/200\n",
      "103/103 - 7s - loss: 0.1330 - tp: 1172.0000 - fp: 45.0000 - tn: 2409.0000 - fn: 55.0000 - accuracy: 0.9728 - precision: 0.9630 - recall: 0.9552 - auc: 0.9944 - prc: 0.9890 - val_loss: 1.0254 - val_tp: 185.0000 - val_fp: 75.0000 - val_tn: 451.0000 - val_fn: 78.0000 - val_accuracy: 0.8061 - val_precision: 0.7115 - val_recall: 0.7034 - val_auc: 0.8532 - val_prc: 0.7445 - 7s/epoch - 69ms/step\n",
      "Epoch 9: early stopping\n",
      "9/9 [==============================] - 0s 18ms/step\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.7743 - tp: 200.0000 - fp: 53.0000 - tn: 475.0000 - fn: 64.0000 - accuracy: 0.8523 - precision: 0.7905 - recall: 0.7576 - auc: 0.8947 - prc: 0.8238\n",
      "\n",
      "=============\n",
      "PARAMS:\n",
      "random_state:10\n",
      "is_shuffle:True\n",
      "y_to_categorical:True\n",
      "rebalance:None\n",
      "cat_lentgh:1000\n",
      "hu:2000\n",
      "output_bias:[0.26409872969017484, -0.11216034588646939, -0.15193831734338828]\n",
      "loss:categorical_crossentropy\n",
      "dropout:True\n",
      "gpu:False\n",
      "set_class_weight:False\n",
      "save_check_point:False\n",
      "early_stopping:True\n",
      "patience:5\n",
      "epochs:200\n",
      "batch_size:12\n",
      "timeframe_in_ms:86400000\n",
      "take_profit_rate:0.1\n",
      "stop_loss_rate:0.07\n",
      "max_duration:10\n",
      "lags:60\n",
      "\n",
      "\n",
      "\n",
      "=============\n",
      "CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.80      0.80       124\n",
      "           1       0.72      0.78      0.75        73\n",
      "           2       0.75      0.69      0.72        67\n",
      "\n",
      "    accuracy                           0.77       264\n",
      "   macro avg       0.76      0.76      0.76       264\n",
      "weighted avg       0.77      0.77      0.76       264\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "=============\n",
      "CONFUSION MATRIX:\n",
      "[[99 16  9]\n",
      " [10 57  6]\n",
      " [15  6 46]]\n",
      "\n",
      "\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.7743 - tp: 200.0000 - fp: 53.0000 - tn: 475.0000 - fn: 64.0000 - accuracy: 0.8523 - precision: 0.7905 - recall: 0.7576 - auc: 0.8947 - prc: 0.8238\n",
      "\n",
      "=============\n",
      "SUMMARY REPORT:\n",
      "Loss mean: 1.055498296022415, std: 0.15424428358208353\n",
      "Accuracy mean: 0.8053030252456665, std: 0.02116397379705887\n",
      "Precsion mean: 0.7148843169212341, std: 0.03350678773719971\n",
      "Recall mean: 0.6920454561710357, std: 0.03137543531238715\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dnn.loop_classifier(\n",
    "    hu=2000,\n",
    "    fm=fm,\n",
    "    target_col=\"trade_signal\",\n",
    "    laps=10,\n",
    "    batch_size=12,\n",
    "    set_class_weight=False,\n",
    "    save_check_point=False,\n",
    "    early_stopping=True,\n",
    "    gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8b58b1819899e409cec63cea36e334f732dfc50db3a5ecdff48b63b0a8eb4970"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
