
=============
FEATURES (show 1 for each):
returns_lag_1, dir_lag_1, sma_lag_1, boll_lag_1, boll7_lag_1, boll14_lag_1, boll21_lag_1, min_lag_1, min7_lag_1, min14_lag_1, min21_lag_1, max_lag_1, max7_lag_1, max14_lag_1, max21_lag_1, mom_lag_1, mom7_lag_1, mom14_lag_1, mom21_lag_1, vol_lag_1, vol7_lag_1, vol14_lag_1, vol21_lag_1, obv_lag_1, mfi7_lag_1, mfi14_lag_1, mfi21_lag_1, rsi7_lag_1, rsi14_lag_1, rsi21_lag_1, adx7_lag_1, adx14_lag_1, adx21_lag_1, roc_lag_1, roc7_lag_1, roc14_lag_1, roc21_lag_1, atr7_lag_1, atr14_lag_1, atr21_lag_1, bop_lag_1, ad_lag_1, adosc_lag_1, trange_lag_1, ado_lag_1, willr7_lag_1, willr14_lag_1, willr21_lag_1, dx7_lag_1, dx14_lag_1, dx21_lag_1, trix_lag_1, ultosc_lag_1, high_lag_1, low_lag_1, 


=============
DATA PREPARATION PARAMS:
trade_timeframe:4h
take_profit_rate:0.03
stop_loss_rate:0.03
max_duration:8
lags:30
fold_number:4
train_size:0.7
val_size:0.15
categorical_label:True
rebalance:over

============
DATA:
Total rows: 11581
Label (0,): 3720(32.12%)
Label (1,): 3918(33.83%)
Label (2,): 3943(34.05%)

>>>>>> FOLD 1


DATA IN FOLD
Train: 2652, Validation: 885, Test: 435

Train:
Label (0,): 884(33.33%)
Label (1,): 884(33.33%)
Label (2,): 884(33.33%)

Validation:
Label (0,): 295(33.33%)
Label (1,): 295(33.33%)
Label (2,): 295(33.33%)

Test:
Label (0,): 130(29.89%)
Label (1,): 108(24.83%)
Label (2,): 197(45.29%)

=============
CLASSIFIER PARAMS:
hu:100
output_bias:[0.0, 0.0, 0.0]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:True
save_check_point:False
early_stopping:True
patience:5
epochs:200
shuffle_when_train:False
batch_size:20
class_weight:{0: 1.0000000000000002, 1: 1.0000000000000002, 2: 1.0000000000000002}

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.47      0.63      0.54       130
           1       0.30      0.32      0.31       108
           2       0.52      0.38      0.44       197

    accuracy                           0.44       435
   macro avg       0.43      0.45      0.43       435
weighted avg       0.45      0.44      0.44       435


=============
CONFUSION MATRIX:
        P0   P1   P2  Total    RP0    RP1    RP2
0       82   18   30    130  0.631  0.138  0.231
1       34   35   39    108  0.315  0.324  0.361
2       58   64   75    197  0.294  0.325  0.381
Total  174  117  144    435  0.400  0.269  0.331

>>>>>> FOLD 2


DATA IN FOLD
Train: 2517, Validation: 600, Test: 435

Train:
Label (0,): 839(33.33%)
Label (1,): 839(33.33%)
Label (2,): 839(33.33%)

Validation:
Label (0,): 200(33.33%)
Label (1,): 200(33.33%)
Label (2,): 200(33.33%)

Test:
Label (0,): 92(21.15%)
Label (1,): 191(43.91%)
Label (2,): 152(34.94%)

=============
CLASSIFIER PARAMS:
hu:100
output_bias:[0.0, 0.0, 0.0]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:True
save_check_point:False
early_stopping:True
patience:5
epochs:200
shuffle_when_train:False
batch_size:20
class_weight:{0: 1.0, 1: 1.0, 2: 1.0}

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.39      0.35      0.37        92
           1       0.44      0.49      0.46       191
           2       0.37      0.34      0.35       152

    accuracy                           0.41       435
   macro avg       0.40      0.39      0.39       435
weighted avg       0.40      0.41      0.40       435


=============
CONFUSION MATRIX:
       P0   P1   P2  Total    RP0    RP1    RP2
0      32   48   12     92  0.348  0.522  0.130
1      23   94   74    191  0.120  0.492  0.387
2      28   73   51    152  0.184  0.480  0.336
Total  83  215  137    435  0.191  0.494  0.315

>>>>>> FOLD 3


DATA IN FOLD
Train: 2193, Validation: 777, Test: 435

Train:
Label (0,): 731(33.33%)
Label (1,): 731(33.33%)
Label (2,): 731(33.33%)

Validation:
Label (0,): 259(33.33%)
Label (1,): 259(33.33%)
Label (2,): 259(33.33%)

Test:
Label (0,): 98(22.53%)
Label (1,): 184(42.3%)
Label (2,): 153(35.17%)

=============
CLASSIFIER PARAMS:
hu:100
output_bias:[0.0, 0.0, 0.0]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:True
save_check_point:False
early_stopping:True
patience:5
epochs:200
shuffle_when_train:False
batch_size:20
class_weight:{0: 1.0, 1: 1.0, 2: 1.0}

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.19      0.03      0.05        98
           1       0.43      0.82      0.56       184
           2       0.49      0.22      0.30       153

    accuracy                           0.43       435
   macro avg       0.37      0.36      0.31       435
weighted avg       0.39      0.43      0.36       435


=============
CONFUSION MATRIX:
       P0   P1  P2  Total    RP0    RP1    RP2
0       3   83  12     98  0.031  0.847  0.122
1      10  150  24    184  0.054  0.815  0.130
2       3  116  34    153  0.020  0.758  0.222
Total  16  349  70    435  0.037  0.802  0.161

>>>>>> FOLD 4


DATA IN FOLD
Train: 2394, Validation: 648, Test: 435

Train:
Label (0,): 798(33.33%)
Label (1,): 798(33.33%)
Label (2,): 798(33.33%)

Validation:
Label (0,): 216(33.33%)
Label (1,): 216(33.33%)
Label (2,): 216(33.33%)

Test:
Label (0,): 292(67.13%)
Label (1,): 73(16.78%)
Label (2,): 70(16.09%)

=============
CLASSIFIER PARAMS:
hu:100
output_bias:[0.0, 0.0, 0.0]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:True
save_check_point:False
early_stopping:True
patience:5
epochs:200
shuffle_when_train:False
batch_size:20
class_weight:{0: 1.0, 1: 1.0, 2: 1.0}

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.70      0.42      0.52       292
           1       0.16      0.22      0.19        73
           2       0.22      0.51      0.31        70

    accuracy                           0.40       435
   macro avg       0.36      0.38      0.34       435
weighted avg       0.53      0.40      0.43       435


=============
CONFUSION MATRIX:
        P0  P1   P2  Total    RP0    RP1    RP2
0      122  71   99    292  0.418  0.243  0.339
1       29  16   28     73  0.397  0.219  0.384
2       23  11   36     70  0.329  0.157  0.514
Total  174  98  163    435  0.400  0.225  0.375

>>>>>>
EVALUATION SUMMARY:
       loss  accuracy  precision  recall  precision-0.65  recall-0.65  \
1     1.158     0.441      0.477   0.366           0.574        0.241   
2     1.282     0.407      0.415   0.324           0.433        0.193   
3     1.193     0.430      0.438   0.322           0.521        0.172   
4     1.261     0.400      0.426   0.292           0.375        0.117   
mean  1.223     0.420      0.439   0.326           0.476        0.181   
std   0.058     0.019      0.027   0.030           0.089        0.051   
min   1.158     0.400      0.415   0.292           0.375        0.117   
max   1.282     0.441      0.477   0.366           0.574        0.241   

      precision-0.80  recall-0.80  precision-0.95  recall-0.95  
1              0.684        0.124           1.000        0.041  
2              0.452        0.076           0.529        0.021  
3              0.475        0.044           0.250        0.002  
4              0.409        0.041           0.000        0.000  
mean           0.505        0.071           0.445        0.016  
std            0.122        0.039           0.429        0.019  
min            0.409        0.041           0.000        0.000  
max            0.684        0.124           1.000        0.041  
