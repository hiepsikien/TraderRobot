
=============
FEATURES (show 1 for each):
returns_lag_1, dir_lag_1, hashrate_lag_1, fed_rate_lag_1, gold_lag_1, nasdaq_lag_1, sp500_lag_1, google_trend_lag_1, sma_lag_1, boll_lag_1, min_lag_1, max_lag_1, mom_lag_1, vol_lag_1, obv_lag_1, mfi14_lag_1, rsi14_lag_1, adx14_lag_1, roc_lag_1, atr14_lag_1, bop_lag_1, ad_lag_1, adosc_lag_1, trange_lag_1, ado_lag_1, willr14_lag_1, dx14_lag_1, trix_lag_1, ultosc_lag_1, high_lag_1, low_lag_1, 


=============
DATA PREPARATION PARAMS:
trade_timeframe:1h
take_profit_rate:0.018
stop_loss_rate:0.018
max_duration:12
lags:24
fold_number:3
train_size:0.7
val_size:0.15
categorical_label:True
rebalance:None



============
DATA:
Total rows: 46809
Label 0: 16048(34.28%)
Label 1: 15288(32.66%)
Label 2: 15473(33.06%)

>>>>>> FOLD 1


DATA IN FOLD
Train: 10922, Validation: 2340, Test: 2341

Train:
Label 0: 2844(26.04%)
Label 1: 4072(37.28%)
Label 2: 4006(36.68%)

Validation:
Label 0: 1114(47.61%)
Label 1: 600(25.64%)
Label 2: 626(26.75%)

Test:
Label 0: 1193(50.96%)
Label 1: 621(26.53%)
Label 2: 527(22.51%)

=============
CLASSIFIER PARAMS:
hu:100
output_bias:[-0.23383483580369036, 0.1250879315035366, 0.10874688949894151]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:True
early_stopping:True
patience:5
epochs:200
shuffle_when_train:True
batch_size:48
class_weight:None



=============
CLASSIFICATION REPORT:

              precision    recall  f1-score   support

           0       0.65      0.92      0.76      1193
           1       0.62      0.19      0.29       621
           2       0.44      0.39      0.41       527

    accuracy                           0.61      2341
   macro avg       0.57      0.50      0.49      2341
weighted avg       0.60      0.61      0.56      2341


=============
CONFUSION MATRIX:
         P0   P1   P2  Total    RP0    RP1    RP2
0      1100   25   68   1193  0.922  0.021  0.057
1       308  118  195    621  0.496  0.190  0.314
2       276   47  204    527  0.524  0.089  0.387
Total  1684  190  467   2341  0.719  0.081  0.199

>>>>>> FOLD 2


DATA IN FOLD
Train: 10922, Validation: 2340, Test: 2341

Train:
Label 0: 4560(41.75%)
Label 1: 3117(28.54%)
Label 2: 3245(29.71%)

Validation:
Label 0: 1088(46.5%)
Label 1: 675(28.85%)
Label 2: 577(24.66%)

Test:
Label 0: 136(5.81%)
Label 1: 1189(50.79%)
Label 2: 1016(43.4%)

=============
CLASSIFIER PARAMS:
hu:100
output_bias:[0.24021963882217343, -0.14023198391891953, -0.09998763454806918]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:True
early_stopping:True
patience:5
epochs:200
shuffle_when_train:True
batch_size:48
class_weight:None



=============
CLASSIFICATION REPORT:

              precision    recall  f1-score   support

           0       0.42      0.27      0.33       136
           1       0.52      0.75      0.62      1189
           2       0.48      0.26      0.34      1016

    accuracy                           0.51      2341
   macro avg       0.47      0.43      0.43      2341
weighted avg       0.50      0.51      0.48      2341


=============
CONFUSION MATRIX:
       P0    P1   P2  Total    RP0    RP1    RP2
0      37    75   24    136  0.272  0.551  0.176
1      31   893  265   1189  0.026  0.751  0.223
2      20   733  263   1016  0.020  0.721  0.259
Total  88  1701  552   2341  0.038  0.727  0.236

>>>>>> FOLD 3


DATA IN FOLD
Train: 10922, Validation: 2340, Test: 2341

Train:
Label 0: 2815(25.77%)
Label 1: 3807(34.86%)
Label 2: 4300(39.37%)

Validation:
Label 0: 755(32.26%)
Label 1: 810(34.62%)
Label 2: 775(33.12%)

Test:
Label 0: 1543(65.91%)
Label 1: 397(16.96%)
Label 2: 401(17.13%)

=============
CLASSIFIER PARAMS:
hu:100
output_bias:[-0.24184398211389982, 0.060035233694806844, 0.1818087789939634]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:True
early_stopping:True
patience:5
epochs:200
shuffle_when_train:True
batch_size:48
class_weight:None



=============
CLASSIFICATION REPORT:

              precision    recall  f1-score   support

           0       0.70      0.94      0.80      1543
           1       0.31      0.11      0.17       397
           2       0.43      0.12      0.19       401

    accuracy                           0.66      2341
   macro avg       0.48      0.39      0.39      2341
weighted avg       0.59      0.66      0.59      2341


=============
CONFUSION MATRIX:
         P0   P1   P2  Total    RP0    RP1    RP2
0      1449   62   32   1543  0.939  0.040  0.021
1       319   45   33    397  0.804  0.113  0.083
2       313   38   50    401  0.781  0.095  0.125
Total  2081  145  115   2341  0.889  0.062  0.049

>>>>>>
EVALUATION SUMMARY:

       loss  accuracy  precision  recall  precision-0.65  recall-0.65  \
1     0.945     0.607      0.650   0.525           0.699        0.393   
2     1.559     0.510      0.511   0.489           0.524        0.396   
3     0.871     0.660      0.687   0.631           0.738        0.552   
mean  1.125     0.592      0.616   0.548           0.653        0.447   
std   0.378     0.076      0.093   0.074           0.114        0.091   
min   0.871     0.510      0.511   0.489           0.524        0.393   
max   1.559     0.660      0.687   0.631           0.738        0.552   

      precision-0.80  recall-0.80  precision-0.95  recall-0.95  
1              0.741        0.238           0.692        0.008  
2              0.518        0.291           0.486        0.138  
3              0.784        0.337           0.667        0.006  
mean           0.681        0.288           0.615        0.051  
std            0.143        0.050           0.113        0.076  
min            0.518        0.238           0.486        0.006  
max            0.784        0.337           0.692        0.138  
