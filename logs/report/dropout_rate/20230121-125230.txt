
=============
FEATURES (show 1 for each):
returns_lag_1, dir_lag_1, sma_lag_1, boll_lag_1, boll7_lag_1, boll14_lag_1, boll21_lag_1, min_lag_1, min7_lag_1, min14_lag_1, min21_lag_1, max_lag_1, max7_lag_1, max14_lag_1, max21_lag_1, mom_lag_1, mom7_lag_1, mom14_lag_1, mom21_lag_1, vol_lag_1, vol7_lag_1, vol14_lag_1, vol21_lag_1, obv_lag_1, mfi7_lag_1, mfi14_lag_1, mfi21_lag_1, rsi7_lag_1, rsi14_lag_1, rsi21_lag_1, adx7_lag_1, adx14_lag_1, adx21_lag_1, roc_lag_1, roc7_lag_1, roc14_lag_1, roc21_lag_1, atr7_lag_1, atr14_lag_1, atr21_lag_1, bop_lag_1, ad_lag_1, adosc_lag_1, trange_lag_1, ado_lag_1, willr7_lag_1, willr14_lag_1, willr21_lag_1, dx7_lag_1, dx14_lag_1, dx21_lag_1, trix_lag_1, ultosc_lag_1, high_lag_1, low_lag_1, 


>>>>>> LAP 1

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 418(34.66%)
Label 1: 409(33.91%)
Label 2: 379(31.43%)

Validation:
Label 0: 104(40.31%)
Label 1: 89(34.5%)
Label 2: 65(25.19%)

Test:
Label 0: 97(37.31%)
Label 1: 76(29.23%)
Label 2: 87(33.46%)

=============
PARAMS:
random_state:1
is_shuffle:True
categorical_label:True
rebalance:None
hu:2000
output_bias:[0.039903826239926485, 0.01813754975797201, -0.058041401202403496]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.7000000000000001
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.68      0.62      0.65        97
           1       0.64      0.71      0.67        76
           2       0.77      0.78      0.78        87

    accuracy                           0.70       260
   macro avg       0.70      0.70      0.70       260
weighted avg       0.70      0.70      0.70       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       60   24   13     97  0.619  0.247  0.134
1       15   54    7     76  0.197  0.711  0.092
2       13    6   68     87  0.149  0.069  0.782
Total   88   84   88    260  0.338  0.323  0.338

>>>>>> LAP 2

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 447(37.06%)
Label 1: 387(32.09%)
Label 2: 372(30.85%)

Validation:
Label 0: 86(33.33%)
Label 1: 92(35.66%)
Label 2: 80(31.01%)

Test:
Label 0: 86(33.08%)
Label 1: 95(36.54%)
Label 2: 79(30.38%)

=============
PARAMS:
random_state:2
is_shuffle:True
categorical_label:True
rebalance:None
hu:2000
output_bias:[0.10926621052533671, -0.03486769105844862, -0.0743985298150877]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.7000000000000001
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.65      0.87      0.74        86
           1       0.83      0.61      0.70        95
           2       0.76      0.71      0.73        79

    accuracy                           0.73       260
   macro avg       0.74      0.73      0.73       260
weighted avg       0.75      0.73      0.72       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       75    5    6     86  0.872  0.058  0.070
1       25   58   12     95  0.263  0.611  0.126
2       16    7   56     79  0.203  0.089  0.709
Total  116   70   74    260  0.446  0.269  0.285

>>>>>> LAP 3

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 427(35.41%)
Label 1: 408(33.83%)
Label 2: 371(30.76%)

Validation:
Label 0: 91(35.27%)
Label 1: 83(32.17%)
Label 2: 84(32.56%)

Test:
Label 0: 101(38.85%)
Label 1: 83(31.92%)
Label 2: 76(29.23%)

=============
PARAMS:
random_state:3
is_shuffle:True
categorical_label:True
rebalance:None
hu:2000
output_bias:[0.06203292028968094, 0.016516081465218123, -0.0785490303315086]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.7000000000000001
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.70      0.56      0.62       101
           1       0.73      0.80      0.76        83
           2       0.71      0.82      0.76        76

    accuracy                           0.71       260
   macro avg       0.71      0.73      0.71       260
weighted avg       0.71      0.71      0.71       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       57   21   23    101  0.564  0.208  0.228
1       15   66    2     83  0.181  0.795  0.024
2       10    4   62     76  0.132  0.053  0.816
Total   82   91   87    260  0.315  0.350  0.335

>>>>>> LAP 4

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 425(35.24%)
Label 1: 413(34.25%)
Label 2: 368(30.51%)

Validation:
Label 0: 105(40.7%)
Label 1: 83(32.17%)
Label 2: 70(27.13%)

Test:
Label 0: 89(34.23%)
Label 1: 78(30.0%)
Label 2: 93(35.77%)

=============
PARAMS:
random_state:4
is_shuffle:True
categorical_label:True
rebalance:None
hu:2000
output_bias:[0.05754927293885754, 0.028907696975473404, -0.08645695781662842]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.7000000000000001
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.76      0.69      0.72        89
           1       0.64      0.69      0.66        78
           2       0.74      0.75      0.74        93

    accuracy                           0.71       260
   macro avg       0.71      0.71      0.71       260
weighted avg       0.72      0.71      0.71       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       61   16   12     89  0.685  0.180  0.135
1       11   54   13     78  0.141  0.692  0.167
2        8   15   70     93  0.086  0.161  0.753
Total   80   85   95    260  0.308  0.327  0.365

>>>>>> LAP 5

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 436(36.15%)
Label 1: 402(33.33%)
Label 2: 368(30.51%)

Validation:
Label 0: 84(32.56%)
Label 1: 88(34.11%)
Label 2: 86(33.33%)

Test:
Label 0: 99(38.08%)
Label 1: 84(32.31%)
Label 2: 77(29.62%)

=============
PARAMS:
random_state:5
is_shuffle:True
categorical_label:True
rebalance:None
hu:2000
output_bias:[0.08358315680864217, 0.002393002078628901, -0.08597614837146135]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.7000000000000001
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.66      0.72      0.69        99
           1       0.68      0.74      0.71        84
           2       0.76      0.61      0.68        77

    accuracy                           0.69       260
   macro avg       0.70      0.69      0.69       260
weighted avg       0.70      0.69      0.69       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       71   19    9     99  0.717  0.192  0.091
1       16   62    6     84  0.190  0.738  0.071
2       20   10   47     77  0.260  0.130  0.610
Total  107   91   62    260  0.412  0.350  0.238

>>>>>> LAP 6

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 435(36.07%)
Label 1: 399(33.08%)
Label 2: 372(30.85%)

Validation:
Label 0: 95(36.82%)
Label 1: 88(34.11%)
Label 2: 75(29.07%)

Test:
Label 0: 89(34.23%)
Label 1: 87(33.46%)
Label 2: 84(32.31%)

=============
PARAMS:
random_state:6
is_shuffle:True
categorical_label:True
rebalance:None
hu:2000
output_bias:[0.08094559047542678, -0.005439023723393909, -0.07550658634011088]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.7000000000000001
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.68      0.73      0.70        89
           1       0.73      0.74      0.73        87
           2       0.80      0.73      0.76        84

    accuracy                           0.73       260
   macro avg       0.74      0.73      0.73       260
weighted avg       0.73      0.73      0.73       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       65   14   10     89  0.730  0.157  0.112
1       18   64    5     87  0.207  0.736  0.057
2       13   10   61     84  0.155  0.119  0.726
Total   96   88   76    260  0.369  0.338  0.292

>>>>>> LAP 7

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 439(36.4%)
Label 1: 388(32.17%)
Label 2: 379(31.43%)

Validation:
Label 0: 105(40.7%)
Label 1: 86(33.33%)
Label 2: 67(25.97%)

Test:
Label 0: 75(28.85%)
Label 1: 100(38.46%)
Label 2: 85(32.69%)

=============
PARAMS:
random_state:7
is_shuffle:True
categorical_label:True
rebalance:None
hu:2000
output_bias:[0.09015242551668473, -0.033341647935213374, -0.0568107824760604]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.7000000000000001
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.62      0.72      0.67        75
           1       0.75      0.70      0.73       100
           2       0.78      0.73      0.75        85

    accuracy                           0.72       260
   macro avg       0.72      0.72      0.71       260
weighted avg       0.72      0.72      0.72       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       54   11   10     75  0.720  0.147  0.133
1       22   70    8    100  0.220  0.700  0.080
2       11   12   62     85  0.129  0.141  0.729
Total   87   93   80    260  0.335  0.358  0.308

>>>>>> LAP 8

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 450(37.31%)
Label 1: 394(32.67%)
Label 2: 362(30.02%)

Validation:
Label 0: 90(34.88%)
Label 1: 84(32.56%)
Label 2: 84(32.56%)

Test:
Label 0: 79(30.38%)
Label 1: 96(36.92%)
Label 2: 85(32.69%)

=============
PARAMS:
random_state:8
is_shuffle:True
categorical_label:True
rebalance:None
hu:2000
output_bias:[0.11683335982459347, -0.016063313641838037, -0.10077001111400082]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.7000000000000001
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.59      0.61      0.60        79
           1       0.70      0.73      0.71        96
           2       0.77      0.71      0.74        85

    accuracy                           0.68       260
   macro avg       0.68      0.68      0.68       260
weighted avg       0.69      0.68      0.69       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       48   21   10     79  0.608  0.266  0.127
1       18   70    8     96  0.188  0.729  0.083
2       16    9   60     85  0.188  0.106  0.706
Total   82  100   78    260  0.315  0.385  0.300

>>>>>> LAP 9

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 433(35.9%)
Label 1: 389(32.26%)
Label 2: 384(31.84%)

Validation:
Label 0: 94(36.43%)
Label 1: 94(36.43%)
Label 2: 70(27.13%)

Test:
Label 0: 92(35.38%)
Label 1: 91(35.0%)
Label 2: 77(29.62%)

=============
PARAMS:
random_state:9
is_shuffle:True
categorical_label:True
rebalance:None
hu:2000
output_bias:[0.07575118765407839, -0.03140719672996517, -0.044343987760684614]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.7000000000000001
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.76      0.62      0.68        92
           1       0.74      0.78      0.76        91
           2       0.71      0.82      0.76        77

    accuracy                           0.73       260
   macro avg       0.74      0.74      0.73       260
weighted avg       0.74      0.73      0.73       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       57   20   15     92  0.620  0.217  0.163
1        9   71   11     91  0.099  0.780  0.121
2        9    5   63     77  0.117  0.065  0.818
Total   75   96   89    260  0.288  0.369  0.342

>>>>>> LAP 10

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 440(36.48%)
Label 1: 399(33.08%)
Label 2: 367(30.43%)

Validation:
Label 0: 83(32.17%)
Label 1: 89(34.5%)
Label 2: 86(33.33%)

Test:
Label 0: 96(36.92%)
Label 1: 86(33.08%)
Label 2: 78(30.0%)

=============
PARAMS:
random_state:10
is_shuffle:True
categorical_label:True
rebalance:None
hu:2000
output_bias:[0.09307539768624283, -0.004737912336200705, -0.08833748117149397]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.7000000000000001
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.61      0.69      0.65        96
           1       0.73      0.66      0.70        86
           2       0.69      0.65      0.67        78

    accuracy                           0.67       260
   macro avg       0.68      0.67      0.67       260
weighted avg       0.67      0.67      0.67       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       66   14   16     96  0.688  0.146  0.167
1       22   57    7     86  0.256  0.663  0.081
2       20    7   51     78  0.256  0.090  0.654
Total  108   78   74    260  0.415  0.300  0.285

>>>>>>
EVALUATION SUMMARY:
       loss  accuracy  precision  recall
1     1.285     0.700      0.699   0.688
2     1.084     0.727      0.727   0.715
3     1.148     0.712      0.734   0.712
4     1.010     0.712      0.731   0.712
5     1.253     0.692      0.692   0.692
6     0.929     0.731      0.744   0.715
7     1.087     0.715      0.715   0.704
8     1.043     0.685      0.684   0.642
9     1.104     0.735      0.748   0.731
10    1.244     0.669      0.677   0.669
mean  1.119     0.708      0.715   0.698
std   0.115     0.021      0.025   0.026
min   0.929     0.669      0.677   0.642
max   1.285     0.735      0.748   0.731
