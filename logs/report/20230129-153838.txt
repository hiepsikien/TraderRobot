
=============
FEATURES (show 1 for each):
returns_lag_1, dir_lag_1, hashrate_lag_1, fed_rate_lag_1, gold_lag_1, nasdaq_lag_1, sp500_lag_1, google_trend_lag_1, sma_lag_1, boll_lag_1, boll7_lag_1, boll14_lag_1, boll21_lag_1, min_lag_1, min7_lag_1, min14_lag_1, min21_lag_1, max_lag_1, max7_lag_1, max14_lag_1, max21_lag_1, mom_lag_1, mom7_lag_1, mom14_lag_1, mom21_lag_1, vol_lag_1, vol7_lag_1, vol14_lag_1, vol21_lag_1, obv_lag_1, mfi7_lag_1, mfi14_lag_1, mfi21_lag_1, rsi7_lag_1, rsi14_lag_1, rsi21_lag_1, adx7_lag_1, adx14_lag_1, adx21_lag_1, roc_lag_1, roc7_lag_1, roc14_lag_1, roc21_lag_1, atr7_lag_1, atr14_lag_1, atr21_lag_1, bop_lag_1, ad_lag_1, adosc_lag_1, trange_lag_1, ado_lag_1, willr7_lag_1, willr14_lag_1, willr21_lag_1, dx7_lag_1, dx14_lag_1, dx21_lag_1, trix_lag_1, ultosc_lag_1, high_lag_1, low_lag_1, 


=============
DATA PREPARATION PARAMS:
trade_timeframe:1h
take_profit_rate:0.03
stop_loss_rate:0.03
max_duration:12
lags:12
fold_number:3
train_size:0.7
val_size:0.15
categorical_label:True
rebalance:None



============
DATA:
Total rows: 46821
Label 0: 28524(60.92%)
Label 1: 8820(18.84%)
Label 2: 9477(20.24%)

>>>>>> FOLD 1


DATA IN FOLD
Train: 9364, Validation: 9364, Test: 9364

Train:
Label 0: 4034(43.08%)
Label 1: 2593(27.69%)
Label 2: 2737(29.23%)

Validation:
Label 0: 6446(68.84%)
Label 1: 1396(14.91%)
Label 2: 1522(16.25%)

Test:
Label 0: 6905(73.74%)
Label 1: 1215(12.98%)
Label 2: 1244(13.28%)

=============
CLASSIFIER PARAMS:
hu:500
output_bias:[0.2766129710690486, -0.1653299619064439, -0.11128303840442942]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:True
early_stopping:True
patience:5
epochs:200
shuffle_when_train:True
batch_size:1024
class_weight:None



=============
CLASSIFICATION REPORT:

              precision    recall  f1-score   support

           0       0.76      1.00      0.86      6905
           1       0.38      0.01      0.02      1215
           2       0.49      0.09      0.15      1244

    accuracy                           0.75      9364
   macro avg       0.54      0.37      0.34      9364
weighted avg       0.67      0.75      0.66      9364


=============
CONFUSION MATRIX:
         P0  P1   P2  Total    RP0    RP1    RP2
0      6874   0   31   6905  0.996  0.000  0.004
1      1117  11   87   1215  0.919  0.009  0.072
2      1113  18  113   1244  0.895  0.014  0.091
Total  9104  29  231   9364  0.972  0.003  0.025

>>>>>> FOLD 2


DATA IN FOLD
Train: 9364, Validation: 9364, Test: 9364

Train:
Label 0: 6446(68.84%)
Label 1: 1396(14.91%)
Label 2: 1522(16.25%)

Validation:
Label 0: 6905(73.74%)
Label 1: 1215(12.98%)
Label 2: 1244(13.28%)

Test:
Label 0: 4641(49.56%)
Label 1: 2273(24.27%)
Label 2: 2450(26.16%)

=============
CLASSIFIER PARAMS:
hu:500
output_bias:[0.9910944245551321, -0.5387543545085901, -0.45234009938945835]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:True
early_stopping:True
patience:5
epochs:200
shuffle_when_train:True
batch_size:1024
class_weight:None



=============
CLASSIFICATION REPORT:

              precision    recall  f1-score   support

           0       0.58      0.78      0.67      4641
           1       0.28      0.20      0.23      2273
           2       0.46      0.28      0.35      2450

    accuracy                           0.51      9364
   macro avg       0.44      0.42      0.42      9364
weighted avg       0.48      0.51      0.48      9364


=============
CONFUSION MATRIX:
         P0    P1    P2  Total    RP0    RP1    RP2
0      3638   687   316   4641  0.784  0.148  0.068
1      1342   451   480   2273  0.590  0.198  0.211
2      1300   461   689   2450  0.531  0.188  0.281
Total  6280  1599  1485   9364  0.671  0.171  0.159

>>>>>> FOLD 3


DATA IN FOLD
Train: 9364, Validation: 9364, Test: 9364

Train:
Label 0: 6905(73.74%)
Label 1: 1215(12.98%)
Label 2: 1244(13.28%)

Validation:
Label 0: 4641(49.56%)
Label 1: 2273(24.27%)
Label 2: 2450(26.16%)

Test:
Label 0: 6497(69.38%)
Label 1: 1343(14.34%)
Label 2: 1524(16.28%)

=============
CLASSIFIER PARAMS:
hu:500
output_bias:[1.1504718355774706, -0.5870298744778655, -0.5634419569778205]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:True
early_stopping:True
patience:5
epochs:200
shuffle_when_train:True
batch_size:1024
class_weight:None



=============
CLASSIFICATION REPORT:

              precision    recall  f1-score   support

           0       0.72      0.98      0.83      6497
           1       0.29      0.01      0.02      1343
           2       0.39      0.13      0.20      1524

    accuracy                           0.70      9364
   macro avg       0.47      0.37      0.35      9364
weighted avg       0.61      0.70      0.61      9364


=============
CONFUSION MATRIX:
         P0  P1   P2  Total    RP0    RP1    RP2
0      6363  14  120   6497  0.979  0.002  0.018
1      1142  11  190   1343  0.850  0.008  0.141
2      1309  13  202   1524  0.859  0.009  0.133
Total  8814  38  512   9364  0.941  0.004  0.055

>>>>>>
EVALUATION SUMMARY:

       loss  accuracy  precision  recall  precision-0.65  recall-0.65  \
1     0.720     0.747      0.757   0.739           0.773        0.704   
2     1.016     0.510      0.574   0.379           0.636        0.156   
3     0.809     0.702      0.718   0.684           0.745        0.624   
mean  0.848     0.653      0.683   0.601           0.718        0.495   
std   0.152     0.126      0.096   0.194           0.072        0.296   
min   0.720     0.510      0.574   0.379           0.636        0.156   
max   1.016     0.747      0.757   0.739           0.773        0.704   

      precision-0.80  recall-0.80  precision-0.95  recall-0.95  
1              0.816        0.549           0.533        0.001  
2              0.785        0.022           0.000        0.000  
3              0.789        0.435           1.000        0.012  
mean           0.797        0.335           0.511        0.004  
std            0.017        0.277           0.500        0.007  
min            0.785        0.022           0.000        0.000  
max            0.816        0.549           1.000        0.012  
