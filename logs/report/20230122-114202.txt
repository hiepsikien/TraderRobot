
=============
FEATURES (show 1 for each):
returns_lag_1, dir_lag_1, sma_lag_1, boll_lag_1, boll7_lag_1, boll14_lag_1, boll21_lag_1, min_lag_1, min7_lag_1, min14_lag_1, min21_lag_1, max_lag_1, max7_lag_1, max14_lag_1, max21_lag_1, mom_lag_1, mom7_lag_1, mom14_lag_1, mom21_lag_1, vol_lag_1, vol7_lag_1, vol14_lag_1, vol21_lag_1, obv_lag_1, mfi7_lag_1, mfi14_lag_1, mfi21_lag_1, rsi7_lag_1, rsi14_lag_1, rsi21_lag_1, adx7_lag_1, adx14_lag_1, adx21_lag_1, roc_lag_1, roc7_lag_1, roc14_lag_1, roc21_lag_1, atr7_lag_1, atr14_lag_1, atr21_lag_1, bop_lag_1, ad_lag_1, adosc_lag_1, trange_lag_1, ado_lag_1, willr7_lag_1, willr14_lag_1, willr21_lag_1, dx7_lag_1, dx14_lag_1, dx21_lag_1, trix_lag_1, ultosc_lag_1, high_lag_1, low_lag_1, 


=============
DATA PREPARATION PARAMS:
trade_timeframe:1d
take_profit_rate:0.1
stop_loss_rate:0.08
max_duration:12
lags:60
fold_number:3
train_size:0.7
val_size:0.15
categorical_label:True
rebalance:None
==========
DATA:

FOLD 0
Train: 0, Validation: 408, Test: 87

Train:
Label 0: 161(39.46%)
Label 1: 121(29.66%)
Label 2: 126(30.88%)

Validation:
Label 0: 6(6.9%)
Label 1: 55(63.22%)
Label 2: 26(29.89%)

Test:
Label 0: 39(43.82%)
Label 1: 19(21.35%)
Label 2: 31(34.83%)

FOLD 1
Train: 1, Validation: 408, Test: 87

Train:
Label 0: 163(39.95%)
Label 1: 151(37.01%)
Label 2: 94(23.04%)

Validation:
Label 0: 13(14.94%)
Label 1: 53(60.92%)
Label 2: 21(24.14%)

Test:
Label 0: 20(22.47%)
Label 1: 30(33.71%)
Label 2: 39(43.82%)

FOLD 2
Train: 2, Validation: 408, Test: 87

Train:
Label 0: 107(26.23%)
Label 1: 131(32.11%)
Label 2: 170(41.67%)

Validation:
Label 0: 42(48.28%)
Label 1: 22(25.29%)
Label 2: 23(26.44%)

Test:
Label 0: 68(76.4%)
Label 1: 6(6.74%)
Label 2: 15(16.85%)

>>>>>> FOLD 1


=============
CLASSIFIER PARAMS:
hu:100
output_bias:[0.17691209600307217, -0.10870172338464971, -0.0682103620299127]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.60      0.15      0.24        39
           1       0.31      0.95      0.47        19
           2       0.57      0.39      0.46        31

    accuracy                           0.40        89
   macro avg       0.49      0.50      0.39        89
weighted avg       0.53      0.40      0.37        89


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0        6   24    9     39  0.154  0.615  0.231
1        1   18    0     19  0.053  0.947  0.000
2        3   16   12     31  0.097  0.516  0.387
Total   10   58   21     89  0.112  0.652  0.236

>>>>>> FOLD 2


=============
CLASSIFIER PARAMS:
hu:100
output_bias:[0.2089752640681089, 0.13250490007600677, -0.34148015446881574]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.60      0.15      0.24        20
           1       0.33      0.93      0.49        30
           2       0.00      0.00      0.00        39

    accuracy                           0.35        89
   macro avg       0.31      0.36      0.24        89
weighted avg       0.25      0.35      0.22        89


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1  RP-2
0        3   17    0     20  0.150  0.850   0.0
1        2   28    0     30  0.067  0.933   0.0
2        0   39    0     39  0.000  1.000   0.0
Total    5   84    0     89  0.056  0.944   0.0

>>>>>> FOLD 3


=============
CLASSIFIER PARAMS:
hu:100
output_bias:[-0.22177935379185118, -0.01941086505260573, 0.24119024879650444]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.79      0.32      0.46        68
           1       0.00      0.00      0.00         6
           2       0.29      0.93      0.44        15

    accuracy                           0.40        89
   macro avg       0.36      0.42      0.30        89
weighted avg       0.65      0.40      0.43        89


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       22   13   33     68  0.324  0.191  0.485
1        5    0    1      6  0.833  0.000  0.167
2        1    0   14     15  0.067  0.000  0.933
Total   28   13   48     89  0.315  0.146  0.539

>>>>>>
EVALUATION SUMMARY:
       loss  accuracy  precision  recall  precision-0.65  recall-0.65  \
1     1.391     0.404      0.406   0.292           0.400        0.180   
2     3.856     0.348      0.348   0.348           0.317        0.292   
3     1.391     0.404      0.412   0.393           0.404        0.236   
mean  2.213     0.386      0.389   0.345           0.374        0.236   
std   1.423     0.032      0.035   0.051           0.049        0.056   
min   1.391     0.348      0.348   0.292           0.317        0.180   
max   3.856     0.404      0.412   0.393           0.404        0.292   

      precision-0.80  recall-0.80  precision-0.95  recall-0.95  
1              0.389        0.079           0.667        0.022  
2              0.293        0.247           0.316        0.202  
3              0.381        0.090           0.000        0.000  
mean           0.354        0.139           0.327        0.075  
std            0.053        0.094           0.333        0.111  
min            0.293        0.079           0.000        0.000  
max            0.389        0.247           0.667        0.202  
