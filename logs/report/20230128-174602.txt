
=============
FEATURES (show 1 for each):
returns_lag_1, dir_lag_1, hashrate_lag_1, fed_rate_lag_1, gold_lag_1, nasdaq_lag_1, sp500_lag_1, google_trend_lag_1, sma_lag_1, boll_lag_1, min_lag_1, max_lag_1, mom_lag_1, vol_lag_1, obv_lag_1, mfi14_lag_1, rsi14_lag_1, adx14_lag_1, roc_lag_1, atr14_lag_1, bop_lag_1, ad_lag_1, adosc_lag_1, trange_lag_1, ado_lag_1, willr14_lag_1, dx14_lag_1, trix_lag_1, ultosc_lag_1, high_lag_1, low_lag_1, 


=============
DATA PREPARATION PARAMS:
trade_timeframe:1d
take_profit_rate:0.05
stop_loss_rate:0.05
max_duration:3
lags:90
fold_number:2
train_size:0.7
val_size:0.15
categorical_label:True
rebalance:None



============
DATA:
Total rows: 1724
Label 0: 656(38.05%)
Label 1: 540(31.32%)
Label 2: 528(30.63%)

>>>>>> FOLD 1


DATA IN FOLD
Train: 603, Validation: 129, Test: 130

Train:
Label 0: 255(42.29%)
Label 1: 171(28.36%)
Label 2: 177(29.35%)

Validation:
Label 0: 52(40.31%)
Label 1: 38(29.46%)
Label 2: 39(30.23%)

Test:
Label 0: 64(49.23%)
Label 1: 41(31.54%)
Label 2: 25(19.23%)

=============
CLASSIFIER PARAMS:
hu:100
output_bias:[0.2549046047898721, -0.1446953838658985, -0.11020920779472149]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
shuffle_when_train:True
batch_size:48
class_weight:None



=============
CLASSIFICATION REPORT:

              precision    recall  f1-score   support

           0       0.48      0.61      0.54        64
           1       0.25      0.24      0.25        41
           2       0.11      0.04      0.06        25

    accuracy                           0.38       130
   macro avg       0.28      0.30      0.28       130
weighted avg       0.34      0.38      0.35       130


=============
CONFUSION MATRIX:
       P0  P1  P2  Total    RP0    RP1    RP2
0      39  21   4     64  0.609  0.328  0.062
1      27  10   4     41  0.659  0.244  0.098
2      15   9   1     25  0.600  0.360  0.040
Total  81  40   9    130  0.623  0.308  0.069

>>>>>> FOLD 2


DATA IN FOLD
Train: 603, Validation: 129, Test: 130

Train:
Label 0: 165(27.36%)
Label 1: 227(37.65%)
Label 2: 211(34.99%)

Validation:
Label 0: 41(31.78%)
Label 1: 36(27.91%)
Label 2: 52(40.31%)

Test:
Label 0: 79(60.77%)
Label 1: 27(20.77%)
Label 2: 24(18.46%)

=============
CLASSIFIER PARAMS:
hu:100
output_bias:[-0.188305730194725, 0.1306988133860972, 0.05760692938076067]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
shuffle_when_train:True
batch_size:48
class_weight:None



=============
CLASSIFICATION REPORT:

              precision    recall  f1-score   support

           0       0.62      0.59      0.61        79
           1       0.22      0.41      0.29        27
           2       0.00      0.00      0.00        24

    accuracy                           0.45       130
   macro avg       0.28      0.33      0.30       130
weighted avg       0.42      0.45      0.43       130


=============
CONFUSION MATRIX:
       P0  P1  P2  Total    RP0    RP1    RP2
0      47  31   1     79  0.595  0.392  0.013
1      13  11   3     27  0.481  0.407  0.111
2      16   8   0     24  0.667  0.333  0.000
Total  76  50   4    130  0.585  0.385  0.031

>>>>>>
EVALUATION SUMMARY:

       loss  accuracy  precision  recall  precision-0.65  recall-0.65  \
1     1.359     0.385      0.402   0.300           0.362        0.131   
2     1.294     0.446      0.439   0.362           0.473        0.200   
mean  1.327     0.415      0.421   0.331           0.417        0.165   
std   0.046     0.044      0.026   0.044           0.079        0.049   
min   1.294     0.385      0.402   0.300           0.362        0.131   
max   1.359     0.446      0.439   0.362           0.473        0.200   

      precision-0.80  recall-0.80  precision-0.95  recall-0.95  
1              0.300        0.046             0.0          0.0  
2              0.480        0.092             0.0          0.0  
mean           0.390        0.069             0.0          0.0  
std            0.127        0.033             0.0          0.0  
min            0.300        0.046             0.0          0.0  
max            0.480        0.092             0.0          0.0  
