
=============
FEATURES (show 1 for each):
returns_lag_1, dir_lag_1, sma_lag_1, boll_lag_1, boll7_lag_1, boll14_lag_1, boll21_lag_1, min_lag_1, min7_lag_1, min14_lag_1, min21_lag_1, max_lag_1, max7_lag_1, max14_lag_1, max21_lag_1, mom_lag_1, mom7_lag_1, mom14_lag_1, mom21_lag_1, vol_lag_1, vol7_lag_1, vol14_lag_1, vol21_lag_1, obv_lag_1, mfi7_lag_1, mfi14_lag_1, mfi21_lag_1, rsi7_lag_1, rsi14_lag_1, rsi21_lag_1, adx7_lag_1, adx14_lag_1, adx21_lag_1, roc_lag_1, roc7_lag_1, roc14_lag_1, roc21_lag_1, atr7_lag_1, atr14_lag_1, atr21_lag_1, bop_lag_1, ad_lag_1, adosc_lag_1, trange_lag_1, ado_lag_1, willr7_lag_1, willr14_lag_1, willr21_lag_1, dx7_lag_1, dx14_lag_1, dx21_lag_1, trix_lag_1, ultosc_lag_1, high_lag_1, low_lag_1, 


>>>>>> LAP 1

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 418(34.66%)
Label 1: 409(33.91%)
Label 2: 379(31.43%)

Validation:
Label 0: 104(40.31%)
Label 1: 89(34.5%)
Label 2: 65(25.19%)

Test:
Label 0: 97(37.31%)
Label 1: 76(29.23%)
Label 2: 87(33.46%)

=============
PARAMS:
random_state:1
is_shuffle:True
categorical_label:True
rebalance:None
hu:1500
output_bias:[0.039903826239926485, 0.01813754975797201, -0.058041401202403496]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.77      0.66      0.71        97
           1       0.67      0.75      0.71        76
           2       0.77      0.82      0.79        87

    accuracy                           0.74       260
   macro avg       0.74      0.74      0.74       260
weighted avg       0.74      0.74      0.74       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       64   20   13     97  0.660  0.206  0.134
1       11   57    8     76  0.145  0.750  0.105
2        8    8   71     87  0.092  0.092  0.816
Total   83   85   92    260  0.319  0.327  0.354

>>>>>> LAP 2

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 447(37.06%)
Label 1: 387(32.09%)
Label 2: 372(30.85%)

Validation:
Label 0: 86(33.33%)
Label 1: 92(35.66%)
Label 2: 80(31.01%)

Test:
Label 0: 86(33.08%)
Label 1: 95(36.54%)
Label 2: 79(30.38%)

=============
PARAMS:
random_state:2
is_shuffle:True
categorical_label:True
rebalance:None
hu:1500
output_bias:[0.10926621052533671, -0.03486769105844862, -0.0743985298150877]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.64      0.67      0.66        86
           1       0.73      0.66      0.70        95
           2       0.75      0.78      0.77        79

    accuracy                           0.70       260
   macro avg       0.71      0.71      0.71       260
weighted avg       0.71      0.70      0.70       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       58   18   10     86  0.674  0.209  0.116
1       21   63   11     95  0.221  0.663  0.116
2       12    5   62     79  0.152  0.063  0.785
Total   91   86   83    260  0.350  0.331  0.319

>>>>>> LAP 3

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 427(35.41%)
Label 1: 408(33.83%)
Label 2: 371(30.76%)

Validation:
Label 0: 91(35.27%)
Label 1: 83(32.17%)
Label 2: 84(32.56%)

Test:
Label 0: 101(38.85%)
Label 1: 83(31.92%)
Label 2: 76(29.23%)

=============
PARAMS:
random_state:3
is_shuffle:True
categorical_label:True
rebalance:None
hu:1500
output_bias:[0.06203292028968094, 0.016516081465218123, -0.0785490303315086]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.69      0.66      0.68       101
           1       0.72      0.82      0.76        83
           2       0.76      0.68      0.72        76

    accuracy                           0.72       260
   macro avg       0.72      0.72      0.72       260
weighted avg       0.72      0.72      0.72       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       67   21   13    101  0.663  0.208  0.129
1       12   68    3     83  0.145  0.819  0.036
2       18    6   52     76  0.237  0.079  0.684
Total   97   95   68    260  0.373  0.365  0.262

>>>>>> LAP 4

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 425(35.24%)
Label 1: 413(34.25%)
Label 2: 368(30.51%)

Validation:
Label 0: 105(40.7%)
Label 1: 83(32.17%)
Label 2: 70(27.13%)

Test:
Label 0: 89(34.23%)
Label 1: 78(30.0%)
Label 2: 93(35.77%)

=============
PARAMS:
random_state:4
is_shuffle:True
categorical_label:True
rebalance:None
hu:1500
output_bias:[0.05754927293885754, 0.028907696975473404, -0.08645695781662842]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.76      0.74      0.75        89
           1       0.74      0.73      0.74        78
           2       0.75      0.77      0.76        93

    accuracy                           0.75       260
   macro avg       0.75      0.75      0.75       260
weighted avg       0.75      0.75      0.75       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       66    9   14     89  0.742  0.101  0.157
1       11   57   10     78  0.141  0.731  0.128
2       10   11   72     93  0.108  0.118  0.774
Total   87   77   96    260  0.335  0.296  0.369

>>>>>> LAP 5

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 436(36.15%)
Label 1: 402(33.33%)
Label 2: 368(30.51%)

Validation:
Label 0: 84(32.56%)
Label 1: 88(34.11%)
Label 2: 86(33.33%)

Test:
Label 0: 99(38.08%)
Label 1: 84(32.31%)
Label 2: 77(29.62%)

=============
PARAMS:
random_state:5
is_shuffle:True
categorical_label:True
rebalance:None
hu:1500
output_bias:[0.08358315680864217, 0.002393002078628901, -0.08597614837146135]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.72      0.64      0.68        99
           1       0.69      0.80      0.74        84
           2       0.72      0.71      0.72        77

    accuracy                           0.71       260
   macro avg       0.71      0.72      0.71       260
weighted avg       0.71      0.71      0.71       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       63   22   14     99  0.636  0.222  0.141
1       10   67    7     84  0.119  0.798  0.083
2       14    8   55     77  0.182  0.104  0.714
Total   87   97   76    260  0.335  0.373  0.292

>>>>>> LAP 6

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 435(36.07%)
Label 1: 399(33.08%)
Label 2: 372(30.85%)

Validation:
Label 0: 95(36.82%)
Label 1: 88(34.11%)
Label 2: 75(29.07%)

Test:
Label 0: 89(34.23%)
Label 1: 87(33.46%)
Label 2: 84(32.31%)

=============
PARAMS:
random_state:6
is_shuffle:True
categorical_label:True
rebalance:None
hu:1500
output_bias:[0.08094559047542678, -0.005439023723393909, -0.07550658634011088]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.66      0.66      0.66        89
           1       0.72      0.76      0.74        87
           2       0.80      0.75      0.77        84

    accuracy                           0.72       260
   macro avg       0.73      0.72      0.72       260
weighted avg       0.72      0.72      0.72       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       59   20   10     89  0.663  0.225  0.112
1       15   66    6     87  0.172  0.759  0.069
2       15    6   63     84  0.179  0.071  0.750
Total   89   92   79    260  0.342  0.354  0.304

>>>>>> LAP 7

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 439(36.4%)
Label 1: 388(32.17%)
Label 2: 379(31.43%)

Validation:
Label 0: 105(40.7%)
Label 1: 86(33.33%)
Label 2: 67(25.97%)

Test:
Label 0: 75(28.85%)
Label 1: 100(38.46%)
Label 2: 85(32.69%)

=============
PARAMS:
random_state:7
is_shuffle:True
categorical_label:True
rebalance:None
hu:1500
output_bias:[0.09015242551668473, -0.033341647935213374, -0.0568107824760604]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.62      0.77      0.69        75
           1       0.79      0.75      0.77       100
           2       0.79      0.67      0.73        85

    accuracy                           0.73       260
   macro avg       0.73      0.73      0.73       260
weighted avg       0.74      0.73      0.73       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       58   10    7     75  0.773  0.133  0.093
1       17   75    8    100  0.170  0.750  0.080
2       18   10   57     85  0.212  0.118  0.671
Total   93   95   72    260  0.358  0.365  0.277

>>>>>> LAP 8

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 450(37.31%)
Label 1: 394(32.67%)
Label 2: 362(30.02%)

Validation:
Label 0: 90(34.88%)
Label 1: 84(32.56%)
Label 2: 84(32.56%)

Test:
Label 0: 79(30.38%)
Label 1: 96(36.92%)
Label 2: 85(32.69%)

=============
PARAMS:
random_state:8
is_shuffle:True
categorical_label:True
rebalance:None
hu:1500
output_bias:[0.11683335982459347, -0.016063313641838037, -0.10077001111400082]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.58      0.68      0.63        79
           1       0.79      0.71      0.75        96
           2       0.77      0.73      0.75        85

    accuracy                           0.71       260
   macro avg       0.71      0.71      0.71       260
weighted avg       0.72      0.71      0.71       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       54   11   14     79  0.684  0.139  0.177
1       23   68    5     96  0.240  0.708  0.052
2       16    7   62     85  0.188  0.082  0.729
Total   93   86   81    260  0.358  0.331  0.312

>>>>>> LAP 9

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 433(35.9%)
Label 1: 389(32.26%)
Label 2: 384(31.84%)

Validation:
Label 0: 94(36.43%)
Label 1: 94(36.43%)
Label 2: 70(27.13%)

Test:
Label 0: 92(35.38%)
Label 1: 91(35.0%)
Label 2: 77(29.62%)

=============
PARAMS:
random_state:9
is_shuffle:True
categorical_label:True
rebalance:None
hu:1500
output_bias:[0.07575118765407839, -0.03140719672996517, -0.044343987760684614]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.74      0.64      0.69        92
           1       0.71      0.80      0.75        91
           2       0.74      0.74      0.74        77

    accuracy                           0.73       260
   macro avg       0.73      0.73      0.73       260
weighted avg       0.73      0.73      0.73       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       59   23   10     92  0.641  0.250  0.109
1        8   73   10     91  0.088  0.802  0.110
2       13    7   57     77  0.169  0.091  0.740
Total   80  103   77    260  0.308  0.396  0.296

>>>>>> LAP 10

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 440(36.48%)
Label 1: 399(33.08%)
Label 2: 367(30.43%)

Validation:
Label 0: 83(32.17%)
Label 1: 89(34.5%)
Label 2: 86(33.33%)

Test:
Label 0: 96(36.92%)
Label 1: 86(33.08%)
Label 2: 78(30.0%)

=============
PARAMS:
random_state:10
is_shuffle:True
categorical_label:True
rebalance:None
hu:1500
output_bias:[0.09307539768624283, -0.004737912336200705, -0.08833748117149397]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.76      0.71      0.74        96
           1       0.76      0.81      0.79        86
           2       0.77      0.78      0.78        78

    accuracy                           0.77       260
   macro avg       0.77      0.77      0.77       260
weighted avg       0.77      0.77      0.76       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       68   14   14     96  0.708  0.146  0.146
1       12   70    4     86  0.140  0.814  0.047
2        9    8   61     78  0.115  0.103  0.782
Total   89   92   79    260  0.342  0.354  0.304

>>>>>>
EVALUATION SUMMARY:
       loss  accuracy  precision  recall  precision-0.65  recall-0.65  \
1     1.052     0.738      0.746   0.723           0.806        0.704   
2     1.137     0.704      0.704   0.696           0.721        0.646   
3     1.032     0.719      0.728   0.712           0.747        0.658   
4     0.874     0.750      0.761   0.746           0.786        0.708   
5     1.093     0.712      0.721   0.696           0.765        0.638   
6     0.910     0.723      0.731   0.712           0.769        0.665   
7     1.005     0.731      0.733   0.719           0.756        0.681   
8     1.167     0.708      0.708   0.700           0.723        0.654   
9     1.061     0.727      0.740   0.712           0.769        0.692   
10    1.057     0.765      0.766   0.754           0.785        0.704   
mean  1.039     0.728      0.734   0.717           0.763        0.675   
std   0.091     0.019      0.020   0.020           0.027        0.026   
min   0.874     0.704      0.704   0.696           0.721        0.638   
max   1.167     0.765      0.766   0.754           0.806        0.708   

      precision-0.80  recall-0.80  precision-0.95  recall-0.95  
1              0.843        0.642           0.877        0.465  
2              0.763        0.608           0.806        0.496  
3              0.810        0.623           0.868        0.454  
4              0.828        0.650           0.906        0.485  
5              0.797        0.573           0.863        0.412  
6              0.807        0.612           0.877        0.438  
7              0.770        0.604           0.830        0.450  
8              0.757        0.588           0.805        0.462  
9              0.797        0.635           0.839        0.500  
10             0.789        0.635           0.812        0.465  
mean           0.796        0.617           0.848        0.463  
std            0.028        0.025           0.035        0.027  
min            0.757        0.573           0.805        0.412  
max            0.843        0.650           0.906        0.500  
