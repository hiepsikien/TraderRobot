
=============
FEATURES (show 1 for each):
returns_lag_1, dir_lag_1, sma_lag_1, boll_lag_1, boll7_lag_1, boll14_lag_1, boll21_lag_1, min_lag_1, min7_lag_1, min14_lag_1, min21_lag_1, max_lag_1, max7_lag_1, max14_lag_1, max21_lag_1, mom_lag_1, mom7_lag_1, mom14_lag_1, mom21_lag_1, vol_lag_1, vol7_lag_1, vol14_lag_1, vol21_lag_1, obv_lag_1, mfi7_lag_1, mfi14_lag_1, mfi21_lag_1, rsi7_lag_1, rsi14_lag_1, rsi21_lag_1, adx7_lag_1, adx14_lag_1, adx21_lag_1, roc_lag_1, roc7_lag_1, roc14_lag_1, roc21_lag_1, atr7_lag_1, atr14_lag_1, atr21_lag_1, bop_lag_1, ad_lag_1, adosc_lag_1, trange_lag_1, ado_lag_1, willr7_lag_1, willr14_lag_1, willr21_lag_1, dx7_lag_1, dx14_lag_1, dx21_lag_1, trix_lag_1, ultosc_lag_1, high_lag_1, low_lag_1, 


>>>>>> LAP 1

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 418(34.66%)
Label 1: 409(33.91%)
Label 2: 379(31.43%)

Validation:
Label 0: 104(40.31%)
Label 1: 89(34.5%)
Label 2: 65(25.19%)

Test:
Label 0: 97(37.31%)
Label 1: 76(29.23%)
Label 2: 87(33.46%)

=============
PARAMS:
random_state:1
is_shuffle:True
categorical_label:True
rebalance:None
hu:1400
output_bias:[0.039903826239926485, 0.01813754975797201, -0.058041401202403496]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.76      0.60      0.67        97
           1       0.65      0.82      0.73        76
           2       0.80      0.82      0.81        87

    accuracy                           0.73       260
   macro avg       0.74      0.74      0.73       260
weighted avg       0.74      0.73      0.73       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       58   25   14     97  0.598  0.258  0.144
1       10   62    4     76  0.132  0.816  0.053
2        8    8   71     87  0.092  0.092  0.816
Total   76   95   89    260  0.292  0.365  0.342

>>>>>> LAP 2

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 447(37.06%)
Label 1: 387(32.09%)
Label 2: 372(30.85%)

Validation:
Label 0: 86(33.33%)
Label 1: 92(35.66%)
Label 2: 80(31.01%)

Test:
Label 0: 86(33.08%)
Label 1: 95(36.54%)
Label 2: 79(30.38%)

=============
PARAMS:
random_state:2
is_shuffle:True
categorical_label:True
rebalance:None
hu:1400
output_bias:[0.10926621052533671, -0.03486769105844862, -0.0743985298150877]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.59      0.74      0.66        86
           1       0.78      0.65      0.71        95
           2       0.77      0.71      0.74        79

    accuracy                           0.70       260
   macro avg       0.71      0.70      0.70       260
weighted avg       0.72      0.70      0.70       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       64   13    9     86  0.744  0.151  0.105
1       25   62    8     95  0.263  0.653  0.084
2       19    4   56     79  0.241  0.051  0.709
Total  108   79   73    260  0.415  0.304  0.281

>>>>>> LAP 3

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 427(35.41%)
Label 1: 408(33.83%)
Label 2: 371(30.76%)

Validation:
Label 0: 91(35.27%)
Label 1: 83(32.17%)
Label 2: 84(32.56%)

Test:
Label 0: 101(38.85%)
Label 1: 83(31.92%)
Label 2: 76(29.23%)

=============
PARAMS:
random_state:3
is_shuffle:True
categorical_label:True
rebalance:None
hu:1400
output_bias:[0.06203292028968094, 0.016516081465218123, -0.0785490303315086]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.75      0.58      0.66       101
           1       0.71      0.80      0.75        83
           2       0.73      0.84      0.78        76

    accuracy                           0.73       260
   macro avg       0.73      0.74      0.73       260
weighted avg       0.73      0.73      0.72       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       59   22   20    101  0.584  0.218  0.198
1       13   66    4     83  0.157  0.795  0.048
2        7    5   64     76  0.092  0.066  0.842
Total   79   93   88    260  0.304  0.358  0.338

>>>>>> LAP 4

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 425(35.24%)
Label 1: 413(34.25%)
Label 2: 368(30.51%)

Validation:
Label 0: 105(40.7%)
Label 1: 83(32.17%)
Label 2: 70(27.13%)

Test:
Label 0: 89(34.23%)
Label 1: 78(30.0%)
Label 2: 93(35.77%)

=============
PARAMS:
random_state:4
is_shuffle:True
categorical_label:True
rebalance:None
hu:1400
output_bias:[0.05754927293885754, 0.028907696975473404, -0.08645695781662842]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.76      0.72      0.74        89
           1       0.75      0.81      0.78        78
           2       0.80      0.80      0.80        93

    accuracy                           0.77       260
   macro avg       0.77      0.77      0.77       260
weighted avg       0.77      0.77      0.77       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       64   12   13     89  0.719  0.135  0.146
1       10   63    5     78  0.128  0.808  0.064
2       10    9   74     93  0.108  0.097  0.796
Total   84   84   92    260  0.323  0.323  0.354

>>>>>> LAP 5

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 436(36.15%)
Label 1: 402(33.33%)
Label 2: 368(30.51%)

Validation:
Label 0: 84(32.56%)
Label 1: 88(34.11%)
Label 2: 86(33.33%)

Test:
Label 0: 99(38.08%)
Label 1: 84(32.31%)
Label 2: 77(29.62%)

=============
PARAMS:
random_state:5
is_shuffle:True
categorical_label:True
rebalance:None
hu:1400
output_bias:[0.08358315680864217, 0.002393002078628901, -0.08597614837146135]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.67      0.71      0.69        99
           1       0.70      0.74      0.72        84
           2       0.81      0.70      0.75        77

    accuracy                           0.72       260
   macro avg       0.73      0.72      0.72       260
weighted avg       0.72      0.72      0.72       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       70   19   10     99  0.707  0.192  0.101
1       19   62    3     84  0.226  0.738  0.036
2       16    7   54     77  0.208  0.091  0.701
Total  105   88   67    260  0.404  0.338  0.258

>>>>>> LAP 6

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 435(36.07%)
Label 1: 399(33.08%)
Label 2: 372(30.85%)

Validation:
Label 0: 95(36.82%)
Label 1: 88(34.11%)
Label 2: 75(29.07%)

Test:
Label 0: 89(34.23%)
Label 1: 87(33.46%)
Label 2: 84(32.31%)

=============
PARAMS:
random_state:6
is_shuffle:True
categorical_label:True
rebalance:None
hu:1400
output_bias:[0.08094559047542678, -0.005439023723393909, -0.07550658634011088]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.70      0.74      0.72        89
           1       0.76      0.78      0.77        87
           2       0.82      0.75      0.78        84

    accuracy                           0.76       260
   macro avg       0.76      0.76      0.76       260
weighted avg       0.76      0.76      0.76       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       66   15    8     89  0.742  0.169  0.090
1       13   68    6     87  0.149  0.782  0.069
2       15    6   63     84  0.179  0.071  0.750
Total   94   89   77    260  0.362  0.342  0.296

>>>>>> LAP 7

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 439(36.4%)
Label 1: 388(32.17%)
Label 2: 379(31.43%)

Validation:
Label 0: 105(40.7%)
Label 1: 86(33.33%)
Label 2: 67(25.97%)

Test:
Label 0: 75(28.85%)
Label 1: 100(38.46%)
Label 2: 85(32.69%)

=============
PARAMS:
random_state:7
is_shuffle:True
categorical_label:True
rebalance:None
hu:1400
output_bias:[0.09015242551668473, -0.033341647935213374, -0.0568107824760604]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.68      0.73      0.71        75
           1       0.81      0.79      0.80       100
           2       0.80      0.76      0.78        85

    accuracy                           0.77       260
   macro avg       0.76      0.76      0.76       260
weighted avg       0.77      0.77      0.77       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       55   10   10     75  0.733  0.133  0.133
1       15   79    6    100  0.150  0.790  0.060
2       11    9   65     85  0.129  0.106  0.765
Total   81   98   81    260  0.312  0.377  0.312

>>>>>> LAP 8

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 450(37.31%)
Label 1: 394(32.67%)
Label 2: 362(30.02%)

Validation:
Label 0: 90(34.88%)
Label 1: 84(32.56%)
Label 2: 84(32.56%)

Test:
Label 0: 79(30.38%)
Label 1: 96(36.92%)
Label 2: 85(32.69%)

=============
PARAMS:
random_state:8
is_shuffle:True
categorical_label:True
rebalance:None
hu:1400
output_bias:[0.11683335982459347, -0.016063313641838037, -0.10077001111400082]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.65      0.63      0.64        79
           1       0.75      0.74      0.74        96
           2       0.75      0.78      0.76        85

    accuracy                           0.72       260
   macro avg       0.72      0.72      0.72       260
weighted avg       0.72      0.72      0.72       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       50   17   12     79  0.633  0.215  0.152
1       15   71   10     96  0.156  0.740  0.104
2       12    7   66     85  0.141  0.082  0.776
Total   77   95   88    260  0.296  0.365  0.338

>>>>>> LAP 9

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 433(35.9%)
Label 1: 389(32.26%)
Label 2: 384(31.84%)

Validation:
Label 0: 94(36.43%)
Label 1: 94(36.43%)
Label 2: 70(27.13%)

Test:
Label 0: 92(35.38%)
Label 1: 91(35.0%)
Label 2: 77(29.62%)

=============
PARAMS:
random_state:9
is_shuffle:True
categorical_label:True
rebalance:None
hu:1400
output_bias:[0.07575118765407839, -0.03140719672996517, -0.044343987760684614]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.73      0.65      0.69        92
           1       0.73      0.78      0.76        91
           2       0.74      0.78      0.76        77

    accuracy                           0.73       260
   macro avg       0.73      0.74      0.73       260
weighted avg       0.73      0.73      0.73       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       60   21   11     92  0.652  0.228  0.120
1       10   71   10     91  0.110  0.780  0.110
2       12    5   60     77  0.156  0.065  0.779
Total   82   97   81    260  0.315  0.373  0.312

>>>>>> LAP 10

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 440(36.48%)
Label 1: 399(33.08%)
Label 2: 367(30.43%)

Validation:
Label 0: 83(32.17%)
Label 1: 89(34.5%)
Label 2: 86(33.33%)

Test:
Label 0: 96(36.92%)
Label 1: 86(33.08%)
Label 2: 78(30.0%)

=============
PARAMS:
random_state:10
is_shuffle:True
categorical_label:True
rebalance:None
hu:1400
output_bias:[0.09307539768624283, -0.004737912336200705, -0.08833748117149397]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.72      0.66      0.69        96
           1       0.76      0.74      0.75        86
           2       0.70      0.79      0.74        78

    accuracy                           0.73       260
   macro avg       0.73      0.73      0.73       260
weighted avg       0.73      0.73      0.73       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       63   12   21     96  0.656  0.125  0.219
1       16   64    6     86  0.186  0.744  0.070
2        8    8   62     78  0.103  0.103  0.795
Total   87   84   89    260  0.335  0.323  0.342

>>>>>>
EVALUATION SUMMARY:
       loss  accuracy  precision  recall  precision-0.65  recall-0.65  \
1     0.975     0.735      0.735   0.715           0.806        0.673   
2     0.953     0.700      0.705   0.688           0.740        0.635   
3     0.886     0.727      0.733   0.719           0.783        0.681   
4     0.774     0.773      0.786   0.762           0.826        0.692   
5     1.028     0.715      0.720   0.704           0.763        0.658   
6     0.907     0.758      0.760   0.754           0.786        0.708   
7     0.804     0.765      0.778   0.742           0.797        0.681   
8     0.860     0.719      0.727   0.696           0.758        0.638   
9     0.939     0.735      0.747   0.727           0.770        0.681   
10    1.009     0.727      0.741   0.715           0.776        0.681   
mean  0.914     0.735      0.743   0.722           0.781        0.673   
std   0.084     0.023      0.025   0.024           0.025        0.023   
min   0.774     0.700      0.705   0.688           0.740        0.635   
max   1.028     0.773      0.786   0.762           0.826        0.708   

      precision-0.80  recall-0.80  precision-0.95  recall-0.95  
1              0.824        0.592           0.877        0.438  
2              0.765        0.550           0.839        0.381  
3              0.811        0.592           0.866        0.396  
4              0.859        0.654           0.917        0.469  
5              0.781        0.577           0.864        0.392  
6              0.816        0.650           0.868        0.504  
7              0.812        0.615           0.868        0.454  
8              0.822        0.585           0.870        0.438  
9              0.805        0.635           0.870        0.462  
10             0.812        0.596           0.832        0.438  
mean           0.811        0.605           0.867        0.437  
std            0.025        0.033           0.023        0.038  
min            0.765        0.550           0.832        0.381  
max            0.859        0.654           0.917        0.504  
