
=============
FEATURES (show 1 for each):
returns_lag_1, dir_lag_1, sma_lag_1, boll_lag_1, boll7_lag_1, boll14_lag_1, boll21_lag_1, min_lag_1, min7_lag_1, min14_lag_1, min21_lag_1, max_lag_1, max7_lag_1, max14_lag_1, max21_lag_1, mom_lag_1, mom7_lag_1, mom14_lag_1, mom21_lag_1, vol_lag_1, vol7_lag_1, vol14_lag_1, vol21_lag_1, obv_lag_1, mfi7_lag_1, mfi14_lag_1, mfi21_lag_1, rsi7_lag_1, rsi14_lag_1, rsi21_lag_1, adx7_lag_1, adx14_lag_1, adx21_lag_1, roc_lag_1, roc7_lag_1, roc14_lag_1, roc21_lag_1, atr7_lag_1, atr14_lag_1, atr21_lag_1, bop_lag_1, ad_lag_1, adosc_lag_1, trange_lag_1, ado_lag_1, willr7_lag_1, willr14_lag_1, willr21_lag_1, dx7_lag_1, dx14_lag_1, dx21_lag_1, trix_lag_1, ultosc_lag_1, high_lag_1, low_lag_1, 


>>>>>> LAP 1

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 418(34.66%)
Label 1: 409(33.91%)
Label 2: 379(31.43%)

Validation:
Label 0: 104(40.31%)
Label 1: 89(34.5%)
Label 2: 65(25.19%)

Test:
Label 0: 97(37.31%)
Label 1: 76(29.23%)
Label 2: 87(33.46%)

=============
PARAMS:
random_state:1
is_shuffle:True
categorical_label:True
rebalance:None
hu:1100
output_bias:[0.039903826239926485, 0.01813754975797201, -0.058041401202403496]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.78      0.64      0.70        97
           1       0.63      0.78      0.70        76
           2       0.82      0.82      0.82        87

    accuracy                           0.74       260
   macro avg       0.74      0.74      0.74       260
weighted avg       0.75      0.74      0.74       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       62   25   10     97  0.639  0.258  0.103
1       11   59    6     76  0.145  0.776  0.079
2        7    9   71     87  0.080  0.103  0.816
Total   80   93   87    260  0.308  0.358  0.335

>>>>>> LAP 2

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 447(37.06%)
Label 1: 387(32.09%)
Label 2: 372(30.85%)

Validation:
Label 0: 86(33.33%)
Label 1: 92(35.66%)
Label 2: 80(31.01%)

Test:
Label 0: 86(33.08%)
Label 1: 95(36.54%)
Label 2: 79(30.38%)

=============
PARAMS:
random_state:2
is_shuffle:True
categorical_label:True
rebalance:None
hu:1100
output_bias:[0.10926621052533671, -0.03486769105844862, -0.0743985298150877]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.65      0.72      0.69        86
           1       0.78      0.63      0.70        95
           2       0.69      0.77      0.73        79

    accuracy                           0.70       260
   macro avg       0.71      0.71      0.70       260
weighted avg       0.71      0.70      0.70       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       62   11   13     86  0.721  0.128  0.151
1       21   60   14     95  0.221  0.632  0.147
2       12    6   61     79  0.152  0.076  0.772
Total   95   77   88    260  0.365  0.296  0.338

>>>>>> LAP 3

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 427(35.41%)
Label 1: 408(33.83%)
Label 2: 371(30.76%)

Validation:
Label 0: 91(35.27%)
Label 1: 83(32.17%)
Label 2: 84(32.56%)

Test:
Label 0: 101(38.85%)
Label 1: 83(31.92%)
Label 2: 76(29.23%)

=============
PARAMS:
random_state:3
is_shuffle:True
categorical_label:True
rebalance:None
hu:1100
output_bias:[0.06203292028968094, 0.016516081465218123, -0.0785490303315086]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.71      0.64      0.67       101
           1       0.71      0.77      0.74        83
           2       0.74      0.76      0.75        76

    accuracy                           0.72       260
   macro avg       0.72      0.73      0.72       260
weighted avg       0.72      0.72      0.72       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       65   21   15    101  0.644  0.208  0.149
1       14   64    5     83  0.169  0.771  0.060
2       13    5   58     76  0.171  0.066  0.763
Total   92   90   78    260  0.354  0.346  0.300

>>>>>> LAP 4

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 425(35.24%)
Label 1: 413(34.25%)
Label 2: 368(30.51%)

Validation:
Label 0: 105(40.7%)
Label 1: 83(32.17%)
Label 2: 70(27.13%)

Test:
Label 0: 89(34.23%)
Label 1: 78(30.0%)
Label 2: 93(35.77%)

=============
PARAMS:
random_state:4
is_shuffle:True
categorical_label:True
rebalance:None
hu:1100
output_bias:[0.05754927293885754, 0.028907696975473404, -0.08645695781662842]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.73      0.66      0.69        89
           1       0.69      0.78      0.73        78
           2       0.78      0.76      0.77        93

    accuracy                           0.73       260
   macro avg       0.73      0.74      0.73       260
weighted avg       0.74      0.73      0.73       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       59   17   13     89  0.663  0.191  0.146
1       10   61    7     78  0.128  0.782  0.090
2       12   10   71     93  0.129  0.108  0.763
Total   81   88   91    260  0.312  0.338  0.350

>>>>>> LAP 5

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 436(36.15%)
Label 1: 402(33.33%)
Label 2: 368(30.51%)

Validation:
Label 0: 84(32.56%)
Label 1: 88(34.11%)
Label 2: 86(33.33%)

Test:
Label 0: 99(38.08%)
Label 1: 84(32.31%)
Label 2: 77(29.62%)

=============
PARAMS:
random_state:5
is_shuffle:True
categorical_label:True
rebalance:None
hu:1100
output_bias:[0.08358315680864217, 0.002393002078628901, -0.08597614837146135]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.70      0.68      0.69        99
           1       0.71      0.80      0.75        84
           2       0.75      0.68      0.71        77

    accuracy                           0.72       260
   macro avg       0.72      0.72      0.72       260
weighted avg       0.72      0.72      0.71       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       67   19   13     99  0.677  0.192  0.131
1       13   67    4     84  0.155  0.798  0.048
2       16    9   52     77  0.208  0.117  0.675
Total   96   95   69    260  0.369  0.365  0.265

>>>>>> LAP 6

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 435(36.07%)
Label 1: 399(33.08%)
Label 2: 372(30.85%)

Validation:
Label 0: 95(36.82%)
Label 1: 88(34.11%)
Label 2: 75(29.07%)

Test:
Label 0: 89(34.23%)
Label 1: 87(33.46%)
Label 2: 84(32.31%)

=============
PARAMS:
random_state:6
is_shuffle:True
categorical_label:True
rebalance:None
hu:1100
output_bias:[0.08094559047542678, -0.005439023723393909, -0.07550658634011088]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.66      0.67      0.67        89
           1       0.72      0.77      0.74        87
           2       0.80      0.73      0.76        84

    accuracy                           0.72       260
   macro avg       0.73      0.72      0.72       260
weighted avg       0.73      0.72      0.72       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       60   21    8     89  0.674  0.236  0.090
1       13   67    7     87  0.149  0.770  0.080
2       18    5   61     84  0.214  0.060  0.726
Total   91   93   76    260  0.350  0.358  0.292

>>>>>> LAP 7

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 439(36.4%)
Label 1: 388(32.17%)
Label 2: 379(31.43%)

Validation:
Label 0: 105(40.7%)
Label 1: 86(33.33%)
Label 2: 67(25.97%)

Test:
Label 0: 75(28.85%)
Label 1: 100(38.46%)
Label 2: 85(32.69%)

=============
PARAMS:
random_state:7
is_shuffle:True
categorical_label:True
rebalance:None
hu:1100
output_bias:[0.09015242551668473, -0.033341647935213374, -0.0568107824760604]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.60      0.71      0.65        75
           1       0.76      0.74      0.75       100
           2       0.76      0.66      0.70        85

    accuracy                           0.70       260
   macro avg       0.71      0.70      0.70       260
weighted avg       0.71      0.70      0.71       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       53   10   12     75  0.707  0.133  0.160
1       20   74    6    100  0.200  0.740  0.060
2       16   13   56     85  0.188  0.153  0.659
Total   89   97   74    260  0.342  0.373  0.285

>>>>>> LAP 8

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 450(37.31%)
Label 1: 394(32.67%)
Label 2: 362(30.02%)

Validation:
Label 0: 90(34.88%)
Label 1: 84(32.56%)
Label 2: 84(32.56%)

Test:
Label 0: 79(30.38%)
Label 1: 96(36.92%)
Label 2: 85(32.69%)

=============
PARAMS:
random_state:8
is_shuffle:True
categorical_label:True
rebalance:None
hu:1100
output_bias:[0.11683335982459347, -0.016063313641838037, -0.10077001111400082]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.57      0.70      0.63        79
           1       0.77      0.73      0.75        96
           2       0.79      0.68      0.73        85

    accuracy                           0.70       260
   macro avg       0.71      0.70      0.70       260
weighted avg       0.72      0.70      0.71       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       55   13   11     79  0.696  0.165  0.139
1       22   70    4     96  0.229  0.729  0.042
2       19    8   58     85  0.224  0.094  0.682
Total   96   91   73    260  0.369  0.350  0.281

>>>>>> LAP 9

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 433(35.9%)
Label 1: 389(32.26%)
Label 2: 384(31.84%)

Validation:
Label 0: 94(36.43%)
Label 1: 94(36.43%)
Label 2: 70(27.13%)

Test:
Label 0: 92(35.38%)
Label 1: 91(35.0%)
Label 2: 77(29.62%)

=============
PARAMS:
random_state:9
is_shuffle:True
categorical_label:True
rebalance:None
hu:1100
output_bias:[0.07575118765407839, -0.03140719672996517, -0.044343987760684614]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.74      0.66      0.70        92
           1       0.74      0.80      0.77        91
           2       0.76      0.78      0.77        77

    accuracy                           0.75       260
   macro avg       0.75      0.75      0.75       260
weighted avg       0.75      0.75      0.74       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       61   21   10     92  0.663  0.228  0.109
1        9   73    9     91  0.099  0.802  0.099
2       12    5   60     77  0.156  0.065  0.779
Total   82   99   79    260  0.315  0.381  0.304

>>>>>> LAP 10

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 440(36.48%)
Label 1: 399(33.08%)
Label 2: 367(30.43%)

Validation:
Label 0: 83(32.17%)
Label 1: 89(34.5%)
Label 2: 86(33.33%)

Test:
Label 0: 96(36.92%)
Label 1: 86(33.08%)
Label 2: 78(30.0%)

=============
PARAMS:
random_state:10
is_shuffle:True
categorical_label:True
rebalance:None
hu:1100
output_bias:[0.09307539768624283, -0.004737912336200705, -0.08833748117149397]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.74      0.68      0.71        96
           1       0.75      0.76      0.75        86
           2       0.75      0.82      0.79        78

    accuracy                           0.75       260
   macro avg       0.75      0.75      0.75       260
weighted avg       0.75      0.75      0.75       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       65   17   14     96  0.677  0.177  0.146
1       14   65    7     86  0.163  0.756  0.081
2        9    5   64     78  0.115  0.064  0.821
Total   88   87   85    260  0.338  0.335  0.327

>>>>>>
EVALUATION SUMMARY:
       loss  accuracy  precision  recall  precision-0.65  recall-0.65  \
1     0.883     0.738      0.746   0.735           0.790        0.650   
2     0.988     0.704      0.706   0.692           0.722        0.638   
3     0.969     0.719      0.728   0.700           0.761        0.662   
4     0.876     0.735      0.737   0.723           0.784        0.658   
5     0.919     0.715      0.727   0.708           0.791        0.669   
6     0.919     0.723      0.741   0.715           0.753        0.658   
7     0.901     0.704      0.710   0.677           0.744        0.604   
8     0.991     0.704      0.709   0.692           0.731        0.627   
9     0.935     0.746      0.756   0.738           0.785        0.704   
10    0.963     0.746      0.753   0.738           0.784        0.669   
mean  0.935     0.723      0.731   0.712           0.765        0.654   
std   0.042     0.017      0.018   0.022           0.026        0.027   
min   0.876     0.704      0.706   0.677           0.722        0.604   
max   0.991     0.746      0.756   0.738           0.791        0.704   

      precision-0.80  recall-0.80  precision-0.95  recall-0.95  
1              0.861        0.596           0.927        0.392  
2              0.763        0.569           0.837        0.415  
3              0.791        0.569           0.869        0.408  
4              0.810        0.573           0.883        0.435  
5              0.812        0.600           0.859        0.469  
6              0.776        0.573           0.876        0.408  
7              0.794        0.550           0.852        0.400  
8              0.747        0.535           0.809        0.342  
9              0.819        0.642           0.856        0.481  
10             0.786        0.581           0.815        0.388  
mean           0.796        0.579           0.858        0.414  
std            0.032        0.029           0.034        0.040  
min            0.747        0.535           0.809        0.342  
max            0.861        0.642           0.927        0.481  
