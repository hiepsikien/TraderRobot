
=============
FEATURES (show 1 for each):
returns_lag_1, dir_lag_1, sma_lag_1, boll_lag_1, boll7_lag_1, boll14_lag_1, boll21_lag_1, min_lag_1, min7_lag_1, min14_lag_1, min21_lag_1, max_lag_1, max7_lag_1, max14_lag_1, max21_lag_1, mom_lag_1, mom7_lag_1, mom14_lag_1, mom21_lag_1, vol_lag_1, vol7_lag_1, vol14_lag_1, vol21_lag_1, obv_lag_1, mfi7_lag_1, mfi14_lag_1, mfi21_lag_1, rsi7_lag_1, rsi14_lag_1, rsi21_lag_1, adx7_lag_1, adx14_lag_1, adx21_lag_1, roc_lag_1, roc7_lag_1, roc14_lag_1, roc21_lag_1, atr7_lag_1, atr14_lag_1, atr21_lag_1, bop_lag_1, ad_lag_1, adosc_lag_1, trange_lag_1, ado_lag_1, willr7_lag_1, willr14_lag_1, willr21_lag_1, dx7_lag_1, dx14_lag_1, dx21_lag_1, trix_lag_1, ultosc_lag_1, high_lag_1, low_lag_1, 


>>>>>> LAP 1

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 418(34.66%)
Label 1: 409(33.91%)
Label 2: 379(31.43%)

Validation:
Label 0: 104(40.31%)
Label 1: 89(34.5%)
Label 2: 65(25.19%)

Test:
Label 0: 97(37.31%)
Label 1: 76(29.23%)
Label 2: 87(33.46%)

=============
PARAMS:
random_state:1
is_shuffle:True
categorical_label:True
rebalance:None
hu:800
output_bias:[0.039903826239926485, 0.01813754975797201, -0.058041401202403496]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.78      0.60      0.68        97
           1       0.66      0.87      0.75        76
           2       0.79      0.78      0.79        87

    accuracy                           0.74       260
   macro avg       0.74      0.75      0.74       260
weighted avg       0.75      0.74      0.74       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       58   25   14     97  0.598  0.258  0.144
1        6   66    4     76  0.079  0.868  0.053
2       10    9   68     87  0.115  0.103  0.782
Total   74  100   86    260  0.285  0.385  0.331

>>>>>> LAP 2

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 447(37.06%)
Label 1: 387(32.09%)
Label 2: 372(30.85%)

Validation:
Label 0: 86(33.33%)
Label 1: 92(35.66%)
Label 2: 80(31.01%)

Test:
Label 0: 86(33.08%)
Label 1: 95(36.54%)
Label 2: 79(30.38%)

=============
PARAMS:
random_state:2
is_shuffle:True
categorical_label:True
rebalance:None
hu:800
output_bias:[0.10926621052533671, -0.03486769105844862, -0.0743985298150877]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.63      0.70      0.66        86
           1       0.76      0.67      0.72        95
           2       0.73      0.75      0.74        79

    accuracy                           0.70       260
   macro avg       0.71      0.71      0.71       260
weighted avg       0.71      0.70      0.70       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       60   15   11     86  0.698  0.174  0.128
1       20   64   11     95  0.211  0.674  0.116
2       15    5   59     79  0.190  0.063  0.747
Total   95   84   81    260  0.365  0.323  0.312

>>>>>> LAP 3

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 427(35.41%)
Label 1: 408(33.83%)
Label 2: 371(30.76%)

Validation:
Label 0: 91(35.27%)
Label 1: 83(32.17%)
Label 2: 84(32.56%)

Test:
Label 0: 101(38.85%)
Label 1: 83(31.92%)
Label 2: 76(29.23%)

=============
PARAMS:
random_state:3
is_shuffle:True
categorical_label:True
rebalance:None
hu:800
output_bias:[0.06203292028968094, 0.016516081465218123, -0.0785490303315086]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.73      0.70      0.72       101
           1       0.77      0.77      0.77        83
           2       0.76      0.80      0.78        76

    accuracy                           0.75       260
   macro avg       0.76      0.76      0.76       260
weighted avg       0.75      0.75      0.75       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       71   16   14    101  0.703  0.158  0.139
1       14   64    5     83  0.169  0.771  0.060
2       12    3   61     76  0.158  0.039  0.803
Total   97   83   80    260  0.373  0.319  0.308

>>>>>> LAP 4

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 425(35.24%)
Label 1: 413(34.25%)
Label 2: 368(30.51%)

Validation:
Label 0: 105(40.7%)
Label 1: 83(32.17%)
Label 2: 70(27.13%)

Test:
Label 0: 89(34.23%)
Label 1: 78(30.0%)
Label 2: 93(35.77%)

=============
PARAMS:
random_state:4
is_shuffle:True
categorical_label:True
rebalance:None
hu:800
output_bias:[0.05754927293885754, 0.028907696975473404, -0.08645695781662842]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.73      0.72      0.72        89
           1       0.76      0.77      0.76        78
           2       0.78      0.78      0.78        93

    accuracy                           0.76       260
   macro avg       0.76      0.76      0.76       260
weighted avg       0.76      0.76      0.76       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       64   12   13     89  0.719  0.135  0.146
1       11   60    7     78  0.141  0.769  0.090
2       13    7   73     93  0.140  0.075  0.785
Total   88   79   93    260  0.338  0.304  0.358

>>>>>> LAP 5

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 436(36.15%)
Label 1: 402(33.33%)
Label 2: 368(30.51%)

Validation:
Label 0: 84(32.56%)
Label 1: 88(34.11%)
Label 2: 86(33.33%)

Test:
Label 0: 99(38.08%)
Label 1: 84(32.31%)
Label 2: 77(29.62%)

=============
PARAMS:
random_state:5
is_shuffle:True
categorical_label:True
rebalance:None
hu:800
output_bias:[0.08358315680864217, 0.002393002078628901, -0.08597614837146135]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.63      0.63      0.63        99
           1       0.69      0.74      0.71        84
           2       0.74      0.69      0.71        77

    accuracy                           0.68       260
   macro avg       0.69      0.68      0.68       260
weighted avg       0.68      0.68      0.68       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       62   23   14     99  0.626  0.232  0.141
1       17   62    5     84  0.202  0.738  0.060
2       19    5   53     77  0.247  0.065  0.688
Total   98   90   72    260  0.377  0.346  0.277

>>>>>> LAP 6

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 435(36.07%)
Label 1: 399(33.08%)
Label 2: 372(30.85%)

Validation:
Label 0: 95(36.82%)
Label 1: 88(34.11%)
Label 2: 75(29.07%)

Test:
Label 0: 89(34.23%)
Label 1: 87(33.46%)
Label 2: 84(32.31%)

=============
PARAMS:
random_state:6
is_shuffle:True
categorical_label:True
rebalance:None
hu:800
output_bias:[0.08094559047542678, -0.005439023723393909, -0.07550658634011088]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.64      0.69      0.66        89
           1       0.69      0.68      0.68        87
           2       0.81      0.76      0.79        84

    accuracy                           0.71       260
   macro avg       0.71      0.71      0.71       260
weighted avg       0.71      0.71      0.71       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       61   20    8     89  0.685  0.225  0.090
1       21   59    7     87  0.241  0.678  0.080
2       13    7   64     84  0.155  0.083  0.762
Total   95   86   79    260  0.365  0.331  0.304

>>>>>> LAP 7

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 439(36.4%)
Label 1: 388(32.17%)
Label 2: 379(31.43%)

Validation:
Label 0: 105(40.7%)
Label 1: 86(33.33%)
Label 2: 67(25.97%)

Test:
Label 0: 75(28.85%)
Label 1: 100(38.46%)
Label 2: 85(32.69%)

=============
PARAMS:
random_state:7
is_shuffle:True
categorical_label:True
rebalance:None
hu:800
output_bias:[0.09015242551668473, -0.033341647935213374, -0.0568107824760604]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.62      0.67      0.65        75
           1       0.75      0.76      0.76       100
           2       0.73      0.68      0.71        85

    accuracy                           0.71       260
   macro avg       0.70      0.70      0.70       260
weighted avg       0.71      0.71      0.71       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       50   13   12     75  0.667  0.173  0.160
1       15   76    9    100  0.150  0.760  0.090
2       15   12   58     85  0.176  0.141  0.682
Total   80  101   79    260  0.308  0.388  0.304

>>>>>> LAP 8

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 450(37.31%)
Label 1: 394(32.67%)
Label 2: 362(30.02%)

Validation:
Label 0: 90(34.88%)
Label 1: 84(32.56%)
Label 2: 84(32.56%)

Test:
Label 0: 79(30.38%)
Label 1: 96(36.92%)
Label 2: 85(32.69%)

=============
PARAMS:
random_state:8
is_shuffle:True
categorical_label:True
rebalance:None
hu:800
output_bias:[0.11683335982459347, -0.016063313641838037, -0.10077001111400082]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.60      0.65      0.62        79
           1       0.72      0.76      0.74        96
           2       0.80      0.69      0.74        85

    accuracy                           0.70       260
   macro avg       0.71      0.70      0.70       260
weighted avg       0.71      0.70      0.71       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       51   18   10     79  0.646  0.228  0.127
1       18   73    5     96  0.188  0.760  0.052
2       16   10   59     85  0.188  0.118  0.694
Total   85  101   74    260  0.327  0.388  0.285

>>>>>> LAP 9

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 433(35.9%)
Label 1: 389(32.26%)
Label 2: 384(31.84%)

Validation:
Label 0: 94(36.43%)
Label 1: 94(36.43%)
Label 2: 70(27.13%)

Test:
Label 0: 92(35.38%)
Label 1: 91(35.0%)
Label 2: 77(29.62%)

=============
PARAMS:
random_state:9
is_shuffle:True
categorical_label:True
rebalance:None
hu:800
output_bias:[0.07575118765407839, -0.03140719672996517, -0.044343987760684614]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.73      0.64      0.68        92
           1       0.73      0.76      0.74        91
           2       0.73      0.79      0.76        77

    accuracy                           0.73       260
   macro avg       0.73      0.73      0.73       260
weighted avg       0.73      0.73      0.73       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       59   21   12     92  0.641  0.228  0.130
1       11   69   11     91  0.121  0.758  0.121
2       11    5   61     77  0.143  0.065  0.792
Total   81   95   84    260  0.312  0.365  0.323

>>>>>> LAP 10

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 440(36.48%)
Label 1: 399(33.08%)
Label 2: 367(30.43%)

Validation:
Label 0: 83(32.17%)
Label 1: 89(34.5%)
Label 2: 86(33.33%)

Test:
Label 0: 96(36.92%)
Label 1: 86(33.08%)
Label 2: 78(30.0%)

=============
PARAMS:
random_state:10
is_shuffle:True
categorical_label:True
rebalance:None
hu:800
output_bias:[0.09307539768624283, -0.004737912336200705, -0.08833748117149397]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.65      0.64      0.64        96
           1       0.71      0.76      0.73        86
           2       0.73      0.69      0.71        78

    accuracy                           0.69       260
   macro avg       0.70      0.69      0.69       260
weighted avg       0.69      0.69      0.69       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       61   21   14     96  0.635  0.219  0.146
1       15   65    6     86  0.174  0.756  0.070
2       18    6   54     78  0.231  0.077  0.692
Total   94   92   74    260  0.362  0.354  0.285

>>>>>>
EVALUATION SUMMARY:
       loss  accuracy  precision  recall  precision-0.65  recall-0.65  \
1     0.987     0.738      0.747   0.727           0.802        0.685   
2     0.878     0.704      0.719   0.688           0.737        0.658   
3     0.868     0.754      0.765   0.738           0.784        0.669   
4     0.780     0.758      0.770   0.758           0.777        0.669   
5     1.024     0.681      0.693   0.677           0.736        0.623   
6     0.848     0.708      0.722   0.688           0.766        0.642   
7     0.857     0.708      0.722   0.688           0.777        0.642   
8     0.857     0.704      0.722   0.700           0.753        0.646   
9     0.876     0.727      0.740   0.712           0.783        0.692   
10    0.922     0.692      0.703   0.673           0.742        0.619   
mean  0.890     0.717      0.730   0.705           0.766        0.655   
std   0.071     0.026      0.025   0.028           0.023        0.024   
min   0.780     0.681      0.693   0.673           0.736        0.619   
max   1.024     0.758      0.770   0.758           0.802        0.692   

      precision-0.80  recall-0.80  precision-0.95  recall-0.95  
1              0.828        0.631           0.909        0.500  
2              0.769        0.577           0.868        0.404  
3              0.817        0.585           0.838        0.358  
4              0.824        0.612           0.906        0.408  
5              0.797        0.588           0.866        0.423  
6              0.835        0.585           0.868        0.454  
7              0.811        0.562           0.864        0.392  
8              0.794        0.535           0.833        0.327  
9              0.810        0.638           0.874        0.400  
10             0.777        0.562           0.840        0.342  
mean           0.806        0.587           0.867        0.401  
std            0.022        0.032           0.026        0.052  
min            0.769        0.535           0.833        0.327  
max            0.835        0.638           0.909        0.500  
