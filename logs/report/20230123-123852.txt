
=============
FEATURES (show 1 for each):
returns_lag_1, dir_lag_1, sma_lag_1, boll_lag_1, boll7_lag_1, boll14_lag_1, boll21_lag_1, min_lag_1, min7_lag_1, min14_lag_1, min21_lag_1, max_lag_1, max7_lag_1, max14_lag_1, max21_lag_1, mom_lag_1, mom7_lag_1, mom14_lag_1, mom21_lag_1, vol_lag_1, vol7_lag_1, vol14_lag_1, vol21_lag_1, obv_lag_1, mfi7_lag_1, mfi14_lag_1, mfi21_lag_1, rsi7_lag_1, rsi14_lag_1, rsi21_lag_1, adx7_lag_1, adx14_lag_1, adx21_lag_1, roc_lag_1, roc7_lag_1, roc14_lag_1, roc21_lag_1, atr7_lag_1, atr14_lag_1, atr21_lag_1, bop_lag_1, ad_lag_1, adosc_lag_1, trange_lag_1, ado_lag_1, willr7_lag_1, willr14_lag_1, willr21_lag_1, dx7_lag_1, dx14_lag_1, dx21_lag_1, trix_lag_1, ultosc_lag_1, high_lag_1, low_lag_1, 


=============
DATA PREPARATION PARAMS:
trade_timeframe:4h
take_profit_rate:0.03
stop_loss_rate:0.03
max_duration:8
lags:14
fold_number:5
train_size:0.7
val_size:0.15
categorical_label:True
rebalance:over



============
DATA:
Total rows: 11597
Label 0: 3720(32.08%)
Label 1: 3925(33.84%)
Label 2: 3952(34.08%)

>>>>>> FOLD 1


DATA IN FOLD
Train: 2253, Validation: 390, Test: 349

Train:
Label 0: 751(33.33%)
Label 1: 751(33.33%)
Label 2: 751(33.33%)

Validation:
Label 0: 130(33.33%)
Label 1: 130(33.33%)
Label 2: 130(33.33%)

Test:
Label 0: 169(48.42%)
Label 1: 98(28.08%)
Label 2: 82(23.5%)

=============
CLASSIFIER PARAMS:
hu:100
output_bias:[0.0, 0.0, 0.0]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.2
learning_rate:0.0001
gpu:False
set_class_weight:True
save_check_point:False
early_stopping:True
patience:5
epochs:200
shuffle_when_train:False
batch_size:48
class_weight:{0: 1.0, 1: 1.0, 2: 1.0}



=============
CLASSIFICATION REPORT:

              precision    recall  f1-score   support

           0       0.52      0.88      0.65       169
           1       0.60      0.09      0.16        98
           2       0.34      0.20      0.25        82

    accuracy                           0.50       349
   macro avg       0.49      0.39      0.35       349
weighted avg       0.50      0.50      0.42       349


=============
CONFUSION MATRIX:
        P0  P1  P2  Total    RP0    RP1    RP2
0      148   2  19    169  0.876  0.012  0.112
1       77   9  12     98  0.786  0.092  0.122
2       62   4  16     82  0.756  0.049  0.195
Total  287  15  47    349  0.822  0.043  0.135

>>>>>> FOLD 2


DATA IN FOLD
Train: 2253, Validation: 462, Test: 349

Train:
Label 0: 751(33.33%)
Label 1: 751(33.33%)
Label 2: 751(33.33%)

Validation:
Label 0: 154(33.33%)
Label 1: 154(33.33%)
Label 2: 154(33.33%)

Test:
Label 0: 150(42.98%)
Label 1: 89(25.5%)
Label 2: 110(31.52%)

=============
CLASSIFIER PARAMS:
hu:100
output_bias:[0.0, 0.0, 0.0]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.2
learning_rate:0.0001
gpu:False
set_class_weight:True
save_check_point:False
early_stopping:True
patience:5
epochs:200
shuffle_when_train:False
batch_size:48
class_weight:{0: 1.0, 1: 1.0, 2: 1.0}



=============
CLASSIFICATION REPORT:

              precision    recall  f1-score   support

           0       0.46      0.69      0.55       150
           1       0.26      0.17      0.20        89
           2       0.39      0.23      0.29       110

    accuracy                           0.41       349
   macro avg       0.37      0.36      0.35       349
weighted avg       0.39      0.41      0.38       349


=============
CONFUSION MATRIX:
        P0  P1  P2  Total    RP0    RP1    RP2
0      104  31  15    150  0.693  0.207  0.100
1       50  15  24     89  0.562  0.169  0.270
2       73  12  25    110  0.664  0.109  0.227
Total  227  58  64    349  0.650  0.166  0.183

>>>>>> FOLD 3


DATA IN FOLD
Train: 2163, Validation: 516, Test: 349

Train:
Label 0: 721(33.33%)
Label 1: 721(33.33%)
Label 2: 721(33.33%)

Validation:
Label 0: 172(33.33%)
Label 1: 172(33.33%)
Label 2: 172(33.33%)

Test:
Label 0: 172(49.28%)
Label 1: 127(36.39%)
Label 2: 50(14.33%)

=============
CLASSIFIER PARAMS:
hu:100
output_bias:[0.0, 0.0, 0.0]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.2
learning_rate:0.0001
gpu:False
set_class_weight:True
save_check_point:False
early_stopping:True
patience:5
epochs:200
shuffle_when_train:False
batch_size:48
class_weight:{0: 1.0, 1: 1.0, 2: 1.0}



=============
CLASSIFICATION REPORT:

              precision    recall  f1-score   support

           0       0.55      0.77      0.64       172
           1       0.46      0.24      0.32       127
           2       0.15      0.12      0.13        50

    accuracy                           0.48       349
   macro avg       0.38      0.38      0.36       349
weighted avg       0.46      0.48      0.45       349


=============
CONFUSION MATRIX:
        P0  P1  P2  Total    RP0    RP1    RP2
0      132  20  20    172  0.767  0.116  0.116
1       81  31  15    127  0.638  0.244  0.118
2       27  17   6     50  0.540  0.340  0.120
Total  240  68  41    349  0.688  0.195  0.117

>>>>>> FOLD 4


DATA IN FOLD
Train: 2193, Validation: 372, Test: 349

Train:
Label 0: 731(33.33%)
Label 1: 731(33.33%)
Label 2: 731(33.33%)

Validation:
Label 0: 124(33.33%)
Label 1: 124(33.33%)
Label 2: 124(33.33%)

Test:
Label 0: 80(22.92%)
Label 1: 132(37.82%)
Label 2: 137(39.26%)

=============
CLASSIFIER PARAMS:
hu:100
output_bias:[0.0, 0.0, 0.0]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.2
learning_rate:0.0001
gpu:False
set_class_weight:True
save_check_point:False
early_stopping:True
patience:5
epochs:200
shuffle_when_train:False
batch_size:48
class_weight:{0: 1.0, 1: 1.0, 2: 1.0}



=============
CLASSIFICATION REPORT:

              precision    recall  f1-score   support

           0       0.20      0.34      0.25        80
           1       0.34      0.32      0.33       132
           2       0.46      0.30      0.36       137

    accuracy                           0.32       349
   macro avg       0.33      0.32      0.31       349
weighted avg       0.35      0.32      0.32       349


=============
CONFUSION MATRIX:
        P0   P1  P2  Total    RP0    RP1    RP2
0       27   34  19     80  0.338  0.425  0.238
1       61   42  29    132  0.462  0.318  0.220
2       48   48  41    137  0.350  0.350  0.299
Total  136  124  89    349  0.390  0.355  0.255

>>>>>> FOLD 5


DATA IN FOLD
Train: 1881, Validation: 552, Test: 349

Train:
Label 0: 627(33.33%)
Label 1: 627(33.33%)
Label 2: 627(33.33%)

Validation:
Label 0: 184(33.33%)
Label 1: 184(33.33%)
Label 2: 184(33.33%)

Test:
Label 0: 230(65.9%)
Label 1: 56(16.05%)
Label 2: 63(18.05%)

=============
CLASSIFIER PARAMS:
hu:100
output_bias:[0.0, 0.0, 0.0]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.2
learning_rate:0.0001
gpu:False
set_class_weight:True
save_check_point:False
early_stopping:True
patience:5
epochs:200
shuffle_when_train:False
batch_size:48
class_weight:{0: 1.0, 1: 1.0, 2: 1.0}



=============
CLASSIFICATION REPORT:

              precision    recall  f1-score   support

           0       0.65      0.51      0.57       230
           1       0.08      0.07      0.08        56
           2       0.25      0.48      0.33        63

    accuracy                           0.43       349
   macro avg       0.33      0.35      0.33       349
weighted avg       0.48      0.43      0.45       349


=============
CONFUSION MATRIX:
        P0  P1   P2  Total    RP0    RP1    RP2
0      117  37   76    230  0.509  0.161  0.330
1       40   4   12     56  0.714  0.071  0.214
2       24   9   30     63  0.381  0.143  0.476
Total  181  50  118    349  0.519  0.143  0.338

>>>>>>
EVALUATION SUMMARY:

       loss  accuracy  precision  recall  precision-0.65  recall-0.65  \
1     1.241     0.496      0.513   0.393           0.489        0.261   
2     1.322     0.413      0.403   0.321           0.382        0.149   
3     1.090     0.484      0.522   0.367           0.559        0.178   
4     1.276     0.315      0.320   0.160           0.339        0.054   
5     1.195     0.433      0.428   0.298           0.384        0.095   
mean  1.225     0.428      0.437   0.308           0.431        0.147   
std   0.089     0.072      0.084   0.090           0.090        0.079   
min   1.090     0.315      0.320   0.160           0.339        0.054   
max   1.322     0.496      0.522   0.393           0.559        0.261   

      precision-0.80  recall-0.80  precision-0.95  recall-0.95  
1              0.468        0.126           0.375        0.009  
2              0.488        0.057           0.500        0.003  
3              0.471        0.046           0.000        0.000  
4              0.300        0.009           0.000        0.000  
5              0.385        0.014           0.000        0.000  
mean           0.422        0.050           0.175        0.002  
std            0.079        0.047           0.244        0.004  
min            0.300        0.009           0.000        0.000  
max            0.488        0.126           0.500        0.009  
