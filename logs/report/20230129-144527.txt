
=============
FEATURES (show 1 for each):
returns_lag_1, dir_lag_1, hashrate_lag_1, fed_rate_lag_1, gold_lag_1, nasdaq_lag_1, sp500_lag_1, google_trend_lag_1, sma_lag_1, boll_lag_1, boll7_lag_1, boll14_lag_1, boll21_lag_1, min_lag_1, min7_lag_1, min14_lag_1, min21_lag_1, max_lag_1, max7_lag_1, max14_lag_1, max21_lag_1, mom_lag_1, mom7_lag_1, mom14_lag_1, mom21_lag_1, vol_lag_1, vol7_lag_1, vol14_lag_1, vol21_lag_1, obv_lag_1, mfi7_lag_1, mfi14_lag_1, mfi21_lag_1, rsi7_lag_1, rsi14_lag_1, rsi21_lag_1, adx7_lag_1, adx14_lag_1, adx21_lag_1, roc_lag_1, roc7_lag_1, roc14_lag_1, roc21_lag_1, atr7_lag_1, atr14_lag_1, atr21_lag_1, bop_lag_1, ad_lag_1, adosc_lag_1, trange_lag_1, ado_lag_1, willr7_lag_1, willr14_lag_1, willr21_lag_1, dx7_lag_1, dx14_lag_1, dx21_lag_1, trix_lag_1, ultosc_lag_1, high_lag_1, low_lag_1, 


=============
DATA PREPARATION PARAMS:
trade_timeframe:1h
take_profit_rate:0.03
stop_loss_rate:0.03
max_duration:12
lags:12
fold_number:1
train_size:0.7
val_size:0.15
categorical_label:True
rebalance:None



============
DATA:
Total rows: 46821
Label 0: 28524(60.92%)
Label 1: 8820(18.84%)
Label 2: 9477(20.24%)

>>>>>> FOLD 1


DATA IN FOLD
Train: 32774, Validation: 7023, Test: 7024

Train:
Label 0: 19322(58.96%)
Label 1: 6516(19.88%)
Label 2: 6936(21.16%)

Validation:
Label 0: 4146(59.03%)
Label 1: 1377(19.61%)
Label 2: 1500(21.36%)

Test:
Label 0: 5056(71.98%)
Label 1: 927(13.2%)
Label 2: 1041(14.82%)

=============
CLASSIFIER PARAMS:
hu:500
output_bias:[0.7038342553604124, -0.3831493969851989, -0.3206848482665495]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:True
early_stopping:True
patience:5
epochs:200
shuffle_when_train:True
batch_size:1024
class_weight:None



=============
CLASSIFICATION REPORT:

              precision    recall  f1-score   support

           0       0.75      1.00      0.85      5056
           1       0.43      0.02      0.04       927
           2       0.56      0.13      0.21      1041

    accuracy                           0.74      7024
   macro avg       0.58      0.38      0.37      7024
weighted avg       0.68      0.74      0.65      7024


=============
CONFUSION MATRIX:
         P0  P1   P2  Total    RP0    RP1    RP2
0      5037   0   19   5056  0.996  0.000  0.004
1       821  20   86    927  0.886  0.022  0.093
2       883  27  131   1041  0.848  0.026  0.126
Total  6741  47  236   7024  0.960  0.007  0.034

>>>>>>
EVALUATION SUMMARY:

       loss  accuracy  precision  recall  precision-0.65  recall-0.65  \
1     0.748     0.739       0.75   0.724           0.765        0.695   
mean  0.748     0.739       0.75   0.724           0.765        0.695   
std     NaN       NaN        NaN     NaN             NaN          NaN   
min   0.748     0.739       0.75   0.724           0.765        0.695   
max   0.748     0.739       0.75   0.724           0.765        0.695   

      precision-0.80  recall-0.80  precision-0.95  recall-0.95  
1              0.797        0.549           0.932        0.196  
mean           0.797        0.549           0.932        0.196  
std              NaN          NaN             NaN          NaN  
min            0.797        0.549           0.932        0.196  
max            0.797        0.549           0.932        0.196  
