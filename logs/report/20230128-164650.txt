
=============
FEATURES (show 1 for each):
returns_lag_1, hashrate_lag_1, fed_rate_lag_1, gold_lag_1, nasdaq_lag_1, sp500_lag_1, google_trend_lag_1, sma_lag_1, boll_lag_1, boll7_lag_1, boll14_lag_1, boll21_lag_1, min_lag_1, min7_lag_1, min14_lag_1, min21_lag_1, max_lag_1, max7_lag_1, max14_lag_1, max21_lag_1, mom_lag_1, mom7_lag_1, mom14_lag_1, mom21_lag_1, vol_lag_1, vol7_lag_1, vol14_lag_1, vol21_lag_1, obv_lag_1, mfi7_lag_1, mfi14_lag_1, mfi21_lag_1, rsi7_lag_1, rsi14_lag_1, rsi21_lag_1, adx7_lag_1, adx14_lag_1, adx21_lag_1, roc_lag_1, roc7_lag_1, roc14_lag_1, roc21_lag_1, atr7_lag_1, atr14_lag_1, atr21_lag_1, bop_lag_1, ad_lag_1, adosc_lag_1, trange_lag_1, ado_lag_1, willr7_lag_1, willr14_lag_1, willr21_lag_1, dx7_lag_1, dx14_lag_1, dx21_lag_1, trix_lag_1, ultosc_lag_1, high_lag_1, low_lag_1, 


=============
DATA PREPARATION PARAMS:
trade_timeframe:1d
take_profit_rate:0.05
stop_loss_rate:0.05
max_duration:3
lags:14
fold_number:2
train_size:0.7
val_size:0.15
categorical_label:True
rebalance:over



============
DATA:
Total rows: 1500
Label 0: 555(37.0%)
Label 1: 486(32.4%)
Label 2: 459(30.6%)

>>>>>> FOLD 1


DATA IN FOLD
Train: 633, Validation: 112, Test: 113

Train:
Label 0: 211(33.33%)
Label 1: 211(33.33%)
Label 2: 211(33.33%)

Validation:
Label 0: 58(51.79%)
Label 1: 32(28.57%)
Label 2: 22(19.64%)

Test:
Label 0: 58(51.33%)
Label 1: 35(30.97%)
Label 2: 20(17.7%)

=============
CLASSIFIER PARAMS:
hu:200
output_bias:[0.0, 0.0, 0.0]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:True
save_check_point:False
early_stopping:True
patience:5
epochs:200
shuffle_when_train:True
batch_size:48
class_weight:{0: 1.0000000000000002, 1: 1.0000000000000002, 2: 1.0000000000000002}



=============
CLASSIFICATION REPORT:

              precision    recall  f1-score   support

           0       0.59      0.28      0.38        58
           1       0.32      0.54      0.40        35
           2       0.22      0.30      0.26        20

    accuracy                           0.36       113
   macro avg       0.38      0.37      0.35       113
weighted avg       0.44      0.36      0.36       113


=============
CONFUSION MATRIX:
       P0  P1  P2  Total    RP0    RP1    RP2
0      16  32  10     58  0.276  0.552  0.172
1       5  19  11     35  0.143  0.543  0.314
2       6   8   6     20  0.300  0.400  0.300
Total  27  59  27    113  0.239  0.522  0.239

>>>>>> FOLD 2


DATA IN FOLD
Train: 618, Validation: 112, Test: 113

Train:
Label 0: 206(33.33%)
Label 1: 206(33.33%)
Label 2: 206(33.33%)

Validation:
Label 0: 38(33.93%)
Label 1: 32(28.57%)
Label 2: 42(37.5%)

Test:
Label 0: 72(63.72%)
Label 1: 22(19.47%)
Label 2: 19(16.81%)

=============
CLASSIFIER PARAMS:
hu:200
output_bias:[0.0, 0.0, 0.0]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:True
save_check_point:False
early_stopping:True
patience:5
epochs:200
shuffle_when_train:True
batch_size:48
class_weight:{0: 0.9999999999999999, 1: 0.9999999999999999, 2: 0.9999999999999999}



=============
CLASSIFICATION REPORT:

              precision    recall  f1-score   support

           0       0.76      0.39      0.51        72
           1       0.15      0.32      0.20        22
           2       0.17      0.26      0.21        19

    accuracy                           0.35       113
   macro avg       0.36      0.32      0.31       113
weighted avg       0.54      0.35      0.40       113


=============
CONFUSION MATRIX:
       P0  P1  P2  Total    RP0    RP1    RP2
0      28  27  17     72  0.389  0.375  0.236
1       8   7   7     22  0.364  0.318  0.318
2       1  13   5     19  0.053  0.684  0.263
Total  37  47  29    113  0.327  0.416  0.257

>>>>>>
EVALUATION SUMMARY:

       loss  accuracy  precision  recall  precision-0.65  recall-0.65  \
1     1.112     0.363      0.387   0.212           0.300        0.027   
2     1.271     0.354      0.377   0.204           0.440        0.097   
mean  1.192     0.358      0.382   0.208           0.370        0.062   
std   0.112     0.006      0.007   0.006           0.099        0.050   
min   1.112     0.354      0.377   0.204           0.300        0.027   
max   1.271     0.363      0.387   0.212           0.440        0.097   

      precision-0.80  recall-0.80  precision-0.95  recall-0.95  
1              0.000        0.000             0.0          0.0  
2              0.333        0.027             0.0          0.0  
mean           0.167        0.013             0.0          0.0  
std            0.236        0.019             0.0          0.0  
min            0.000        0.000             0.0          0.0  
max            0.333        0.027             0.0          0.0  
