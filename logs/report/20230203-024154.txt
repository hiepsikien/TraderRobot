
=============
FEATURES (show 1 for each):
sma_3_10_trade_lag_1, sma_7_30_trade_lag_1, sma_14_50_trade_lag_1, sma_28_90_trade_lag_1, rsi7_trade_lag_1, rsi14_trade_lag_1, rsi30_trade_lag_1, CDL2CROWS_trade_lag_1, CDL3BLACKCROWS_trade_lag_1, CDL3INSIDE_trade_lag_1, CDL3LINESTRIKE_trade_lag_1, CDL3OUTSIDE_trade_lag_1, CDL3STARSINSOUTH_trade_lag_1, CDL3WHITESOLDIERS_trade_lag_1, CDLABANDONEDBABY_trade_lag_1, CDLADVANCEBLOCK_trade_lag_1, CDLBELTHOLD_trade_lag_1, CDLBREAKAWAY_trade_lag_1, CDLCLOSINGMARUBOZU_trade_lag_1, CDLCONCEALBABYSWALL_trade_lag_1, CDLCOUNTERATTACK_trade_lag_1, CDLDARKCLOUDCOVER_trade_lag_1, CDLDOJI_trade_lag_1, CDLDOJISTAR_trade_lag_1, CDLDRAGONFLYDOJI_trade_lag_1, CDLENGULFING_trade_lag_1, CDLEVENINGDOJISTAR_trade_lag_1, CDLEVENINGSTAR_trade_lag_1, CDLGAPSIDESIDEWHITE_trade_lag_1, CDLGRAVESTONEDOJI_trade_lag_1, CDLHAMMER_trade_lag_1, CDLHANGINGMAN_trade_lag_1, CDLHARAMI_trade_lag_1, CDLHARAMICROSS_trade_lag_1, CDLHIGHWAVE_trade_lag_1, CDLHIKKAKE_trade_lag_1, CDLHIKKAKEMOD_trade_lag_1, CDLHOMINGPIGEON_trade_lag_1, CDLIDENTICAL3CROWS_trade_lag_1, CDLINNECK_trade_lag_1, CDLINVERTEDHAMMER_trade_lag_1, CDLKICKING_trade_lag_1, CDLKICKINGBYLENGTH_trade_lag_1, CDLLADDERBOTTOM_trade_lag_1, CDLLONGLEGGEDDOJI_trade_lag_1, CDLLONGLINE_trade_lag_1, CDLMARUBOZU_trade_lag_1, CDLMATCHINGLOW_trade_lag_1, CDLMATHOLD_trade_lag_1, CDLMORNINGDOJISTAR_trade_lag_1, CDLMORNINGSTAR_trade_lag_1, CDLONNECK_trade_lag_1, CDLPIERCING_trade_lag_1, CDLRICKSHAWMAN_trade_lag_1, CDLRISEFALL3METHODS_trade_lag_1, CDLSEPARATINGLINES_trade_lag_1, CDLSHOOTINGSTAR_trade_lag_1, CDLSHORTLINE_trade_lag_1, CDLSPINNINGTOP_trade_lag_1, CDLSTALLEDPATTERN_trade_lag_1, CDLSTICKSANDWICH_trade_lag_1, CDLTAKURI_trade_lag_1, CDLTASUKIGAP_trade_lag_1, CDLTHRUSTING_trade_lag_1, CDLTRISTAR_trade_lag_1, CDLUNIQUE3RIVER_trade_lag_1, CDLUPSIDEGAP2CROWS_trade_lag_1, CDLXSIDEGAP3METHODS_trade_lag_1, sma_3_10_macro_lag_1, sma_14_50_macro_lag_1, rsi7_macro_lag_1, rsi30_macro_lag_1, rsi90_macro_lag_1, CDL3BLACKCROWS_macro_lag_1, CDL3LINESTRIKE_macro_lag_1, CDL3STARSINSOUTH_macro_lag_1, CDLABANDONEDBABY_macro_lag_1, CDLBELTHOLD_macro_lag_1, CDLCLOSINGMARUBOZU_macro_lag_1, CDLCOUNTERATTACK_macro_lag_1, CDLDOJI_macro_lag_1, CDLDRAGONFLYDOJI_macro_lag_1, CDLEVENINGDOJISTAR_macro_lag_1, CDLGAPSIDESIDEWHITE_macro_lag_1, CDLHAMMER_macro_lag_1, CDLHARAMI_macro_lag_1, CDLHIGHWAVE_macro_lag_1, CDLHIKKAKEMOD_macro_lag_1, CDLIDENTICAL3CROWS_macro_lag_1, CDLINVERTEDHAMMER_macro_lag_1, CDLKICKINGBYLENGTH_macro_lag_1, CDLLONGLEGGEDDOJI_macro_lag_1, CDLMARUBOZU_macro_lag_1, CDLMATHOLD_macro_lag_1, CDLMORNINGSTAR_macro_lag_1, CDLPIERCING_macro_lag_1, CDLRISEFALL3METHODS_macro_lag_1, CDLSHOOTINGSTAR_macro_lag_1, CDLSPINNINGTOP_macro_lag_1, CDLSTICKSANDWICH_macro_lag_1, CDLTASUKIGAP_macro_lag_1, CDLTRISTAR_macro_lag_1, CDLUPSIDEGAP2CROWS_macro_lag_1, sma_3_10_super_lag_1, rsi14_super_lag_3, CDL3INSIDE_super_lag_1, CDL3STARSINSOUTH_super_lag_3, CDLBELTHOLD_super_lag_1, CDLCONCEALBABYSWALL_super_lag_3, CDLDOJISTAR_super_lag_1, CDLEVENINGDOJISTAR_super_lag_3, CDLHAMMER_super_lag_1, CDLHARAMICROSS_super_lag_3, CDLHOMINGPIGEON_super_lag_1, CDLINVERTEDHAMMER_super_lag_3, CDLLONGLEGGEDDOJI_super_lag_1, CDLMATCHINGLOW_super_lag_3, CDLONNECK_super_lag_1, CDLRISEFALL3METHODS_super_lag_3, CDLSPINNINGTOP_super_lag_1, CDLTAKURI_super_lag_3, CDLUNIQUE3RIVER_super_lag_1, 


=============
DATA PREPARATION PARAMS:
trade_timeframe:4h
take_profit_rate:0.042
stop_loss_rate:0.042
max_duration:14
lags:14
scaler:MaxAbs
split_type:time_series_split
fold_number:10
categorical_label:True
rebalance:None



============
DATA:
Total rows: 10359
Label 0.0: 3804(36.72%)
Label 1.0: 3271(31.58%)
Label 2.0: 3284(31.7%)

>>>>>> FOLD 1


DATA IN FOLD
Train: 863, Validation: 863, Test: 863

Train:
Label 0: 254(29.43%)
Label 1: 294(34.07%)
Label 2: 315(36.5%)

Validation:
Label 0: 418(48.44%)
Label 1: 175(20.28%)
Label 2: 270(31.29%)

Test:
Label 0: 460(53.3%)
Label 1: 283(32.79%)
Label 2: 120(13.9%)

=============
CLASSIFIER PARAMS:
hu:2000
output_bias:[-0.1204946411876942, 0.025750859132450787, 0.0947437306194023]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:True
early_stopping:True
patience:5
epochs:200
shuffle_when_train:True
batch_size:1024
class_weight:None



=============
CLASSIFICATION REPORT:

              precision    recall  f1-score   support

           0       0.75      0.26      0.39       460
           1       0.39      0.40      0.39       283
           2       0.17      0.57      0.26       120

    accuracy                           0.35       863
   macro avg       0.44      0.41      0.35       863
weighted avg       0.55      0.35      0.37       863


=============
CONFUSION MATRIX:
        P0   P1   P2  Total    RP0    RP1    RP2
0      121  132  207    460  0.263  0.287  0.450
1       30  112  141    283  0.106  0.396  0.498
2       10   41   69    120  0.083  0.342  0.575
Total  161  285  417    863  0.187  0.330  0.483

>>>>>> FOLD 2


DATA IN FOLD
Train: 1726, Validation: 863, Test: 863

Train:
Label 0: 672(38.93%)
Label 1: 469(27.17%)
Label 2: 585(33.89%)

Validation:
Label 0: 460(53.3%)
Label 1: 283(32.79%)
Label 2: 120(13.9%)

Test:
Label 0: 289(33.49%)
Label 1: 287(33.26%)
Label 2: 287(33.26%)

=============
CLASSIFIER PARAMS:
hu:2000
output_bias:[0.16610069002597638, -0.19355488205089388, 0.02745419673468309]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:True
early_stopping:True
patience:5
epochs:200
shuffle_when_train:True
batch_size:1024
class_weight:None



=============
CLASSIFICATION REPORT:

              precision    recall  f1-score   support

           0       0.34      0.72      0.47       289
           1       0.60      0.01      0.02       287
           2       0.34      0.29      0.31       287

    accuracy                           0.34       863
   macro avg       0.43      0.34      0.27       863
weighted avg       0.43      0.34      0.27       863


=============
CONFUSION MATRIX:
        P0  P1   P2  Total    RP0    RP1    RP2
0      209   1   79    289  0.723  0.003  0.273
1      197   3   87    287  0.686  0.010  0.303
2      202   1   84    287  0.704  0.003  0.293
Total  608   5  250    863  0.705  0.006  0.290

>>>>>> FOLD 3


DATA IN FOLD
Train: 2589, Validation: 863, Test: 863

Train:
Label 0: 1132(43.72%)
Label 1: 752(29.05%)
Label 2: 705(27.23%)

Validation:
Label 0: 289(33.49%)
Label 1: 287(33.26%)
Label 2: 287(33.26%)

Test:
Label 0: 342(39.63%)
Label 1: 228(26.42%)
Label 2: 293(33.95%)

=============
CLASSIFIER PARAMS:
hu:2000
output_bias:[0.2941827946860279, -0.11482214012538315, -0.179360661267342]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:True
early_stopping:True
patience:5
epochs:200
shuffle_when_train:True
batch_size:1024
class_weight:None



=============
CLASSIFICATION REPORT:

              precision    recall  f1-score   support

           0       0.43      0.65      0.51       342
           1       0.27      0.16      0.20       228
           2       0.40      0.29      0.34       293

    accuracy                           0.40       863
   macro avg       0.37      0.37      0.35       863
weighted avg       0.38      0.40      0.37       863


=============
CONFUSION MATRIX:
        P0   P1   P2  Total    RP0    RP1    RP2
0      221   53   68    342  0.646  0.155  0.199
1      135   37   56    228  0.592  0.162  0.246
2      161   48   84    293  0.549  0.164  0.287
Total  517  138  208    863  0.599  0.160  0.241

>>>>>> FOLD 4


DATA IN FOLD
Train: 3452, Validation: 863, Test: 863

Train:
Label 0: 1421(41.16%)
Label 1: 1039(30.1%)
Label 2: 992(28.74%)

Validation:
Label 0: 342(39.63%)
Label 1: 228(26.42%)
Label 2: 293(33.95%)

Test:
Label 0: 417(48.32%)
Label 1: 262(30.36%)
Label 2: 184(21.32%)

=============
CLASSIFIER PARAMS:
hu:2000
output_bias:[0.22416505595930705, -0.08893708103856612, -0.13522796485292116]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:True
early_stopping:True
patience:5
epochs:200
shuffle_when_train:True
batch_size:1024
class_weight:None



=============
CLASSIFICATION REPORT:

              precision    recall  f1-score   support

           0       0.50      0.82      0.62       417
           1       0.47      0.13      0.21       262
           2       0.26      0.14      0.18       184

    accuracy                           0.47       863
   macro avg       0.41      0.36      0.34       863
weighted avg       0.44      0.47      0.40       863


=============
CONFUSION MATRIX:
        P0  P1  P2  Total    RP0    RP1    RP2
0      344  22  51    417  0.825  0.053  0.122
1      206  35  21    262  0.786  0.134  0.080
2      142  17  25    184  0.772  0.092  0.136
Total  692  74  97    863  0.802  0.086  0.112

>>>>>> FOLD 5


DATA IN FOLD
Train: 4315, Validation: 863, Test: 863

Train:
Label 0: 1763(40.86%)
Label 1: 1267(29.36%)
Label 2: 1285(29.78%)

Validation:
Label 0: 417(48.32%)
Label 1: 262(30.36%)
Label 2: 184(21.32%)

Test:
Label 0: 354(41.02%)
Label 1: 316(36.62%)
Label 2: 193(22.36%)

=============
CLASSIFIER PARAMS:
hu:2000
output_bias:[0.21554106733544032, -0.1148239347412908, -0.10071711773310964]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:True
early_stopping:True
patience:5
epochs:200
shuffle_when_train:True
batch_size:1024
class_weight:None



=============
CLASSIFICATION REPORT:

              precision    recall  f1-score   support

           0       0.55      0.39      0.46       354
           1       0.36      0.41      0.38       316
           2       0.19      0.25      0.22       193

    accuracy                           0.37       863
   macro avg       0.37      0.35      0.35       863
weighted avg       0.40      0.37      0.38       863


=============
CONFUSION MATRIX:
        P0   P1   P2  Total    RP0    RP1    RP2
0      139  157   58    354  0.393  0.444  0.164
1       42  128  146    316  0.133  0.405  0.462
2       70   75   48    193  0.363  0.389  0.249
Total  251  360  252    863  0.291  0.417  0.292

>>>>>> FOLD 6


DATA IN FOLD
Train: 5178, Validation: 863, Test: 863

Train:
Label 0: 2180(42.1%)
Label 1: 1529(29.53%)
Label 2: 1469(28.37%)

Validation:
Label 0: 354(41.02%)
Label 1: 316(36.62%)
Label 2: 193(22.36%)

Test:
Label 0: 81(9.39%)
Label 1: 379(43.92%)
Label 2: 403(46.7%)

=============
CLASSIFIER PARAMS:
hu:2000
output_bias:[0.24981798708241149, -0.104892962771661, -0.1449249925268458]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:True
early_stopping:True
patience:5
epochs:200
shuffle_when_train:True
batch_size:1024
class_weight:None



=============
CLASSIFICATION REPORT:

              precision    recall  f1-score   support

           0       0.12      0.09      0.10        81
           1       0.45      0.41      0.43       379
           2       0.49      0.55      0.52       403

    accuracy                           0.45       863
   macro avg       0.35      0.35      0.35       863
weighted avg       0.44      0.45      0.44       863


=============
CONFUSION MATRIX:
       P0   P1   P2  Total    RP0    RP1    RP2
0       7   38   36     81  0.086  0.469  0.444
1      26  157  196    379  0.069  0.414  0.517
2      24  156  223    403  0.060  0.387  0.553
Total  57  351  455    863  0.066  0.407  0.527

>>>>>> FOLD 7


DATA IN FOLD
Train: 6041, Validation: 863, Test: 863

Train:
Label 0: 2534(41.95%)
Label 1: 1845(30.54%)
Label 2: 1662(27.51%)

Validation:
Label 0: 81(9.39%)
Label 1: 379(43.92%)
Label 2: 403(46.7%)

Test:
Label 0: 178(20.63%)
Label 1: 344(39.86%)
Label 2: 341(39.51%)

=============
CLASSIFIER PARAMS:
hu:2000
output_bias:[0.24636572969406095, -0.07095407471064907, -0.17541165577462284]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:True
early_stopping:True
patience:5
epochs:200
shuffle_when_train:True
batch_size:1024
class_weight:None



=============
CLASSIFICATION REPORT:

              precision    recall  f1-score   support

           0       0.14      0.31      0.19       178
           1       0.39      0.30      0.34       344
           2       0.32      0.19      0.24       341

    accuracy                           0.26       863
   macro avg       0.28      0.27      0.26       863
weighted avg       0.31      0.26      0.27       863


=============
CONFUSION MATRIX:
        P0   P1   P2  Total    RP0    RP1    RP2
0       56   74   48    178  0.315  0.416  0.270
1      151  103   90    344  0.439  0.299  0.262
2      191   85   65    341  0.560  0.249  0.191
Total  398  262  203    863  0.461  0.304  0.235

>>>>>> FOLD 8


DATA IN FOLD
Train: 6904, Validation: 863, Test: 863

Train:
Label 0: 2615(37.88%)
Label 1: 2224(32.21%)
Label 2: 2065(29.91%)

Validation:
Label 0: 178(20.63%)
Label 1: 344(39.86%)
Label 2: 341(39.51%)

Test:
Label 0: 244(28.27%)
Label 1: 272(31.52%)
Label 2: 347(40.21%)

=============
CLASSIFIER PARAMS:
hu:2000
output_bias:[0.13269687054008755, -0.029259850588459955, -0.10343700056380664]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:True
early_stopping:True
patience:5
epochs:200
shuffle_when_train:True
batch_size:1024
class_weight:None



=============
CLASSIFICATION REPORT:

              precision    recall  f1-score   support

           0       0.32      0.66      0.43       244
           1       0.34      0.22      0.27       272
           2       0.48      0.24      0.32       347

    accuracy                           0.35       863
   macro avg       0.38      0.37      0.34       863
weighted avg       0.39      0.35      0.33       863


=============
CONFUSION MATRIX:
        P0   P1   P2  Total    RP0    RP1    RP2
0      161   53   30    244  0.660  0.217  0.123
1      152   60   60    272  0.559  0.221  0.221
2      198   66   83    347  0.571  0.190  0.239
Total  511  179  173    863  0.592  0.207  0.200

>>>>>> FOLD 9


DATA IN FOLD
Train: 7767, Validation: 863, Test: 863

Train:
Label 0: 2793(35.96%)
Label 1: 2568(33.06%)
Label 2: 2406(30.98%)

Validation:
Label 0: 244(28.27%)
Label 1: 272(31.52%)
Label 2: 347(40.21%)

Test:
Label 0: 243(28.16%)
Label 1: 267(30.94%)
Label 2: 353(40.9%)

=============
CLASSIFIER PARAMS:
hu:2000
output_bias:[0.07771318558312691, -0.006275715552198016, -0.07143748382742572]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:True
early_stopping:True
patience:5
epochs:200
shuffle_when_train:True
batch_size:1024
class_weight:None



=============
CLASSIFICATION REPORT:

              precision    recall  f1-score   support

           0       0.40      0.65      0.49       243
           1       0.41      0.19      0.26       267
           2       0.48      0.46      0.47       353

    accuracy                           0.43       863
   macro avg       0.43      0.43      0.41       863
weighted avg       0.44      0.43      0.41       863


=============
CONFUSION MATRIX:
        P0   P1   P2  Total    RP0    RP1    RP2
0      158   38   47    243  0.650  0.156  0.193
1       86   52  129    267  0.322  0.195  0.483
2      155   36  162    353  0.439  0.102  0.459
Total  399  126  338    863  0.462  0.146  0.392

>>>>>> FOLD 10


DATA IN FOLD
Train: 8630, Validation: 863, Test: 863

Train:
Label 0: 3037(35.19%)
Label 1: 2840(32.91%)
Label 2: 2753(31.9%)

Validation:
Label 0: 243(28.16%)
Label 1: 267(30.94%)
Label 2: 353(40.9%)

Test:
Label 0: 521(60.37%)
Label 1: 164(19.0%)
Label 2: 178(20.63%)

=============
CLASSIFIER PARAMS:
hu:2000
output_bias:[0.05508170245007128, -0.011984431440425828, -0.04309725745307037]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:True
early_stopping:True
patience:5
epochs:200
shuffle_when_train:True
batch_size:1024
class_weight:None



=============
CLASSIFICATION REPORT:

              precision    recall  f1-score   support

           0       0.65      0.70      0.68       521
           1       0.20      0.28      0.23       164
           2       0.32      0.12      0.18       178

    accuracy                           0.50       863
   macro avg       0.39      0.37      0.36       863
weighted avg       0.50      0.50      0.49       863


=============
CONFUSION MATRIX:
        P0   P1  P2  Total    RP0    RP1    RP2
0      367  118  36    521  0.704  0.226  0.069
1      107   46  11    164  0.652  0.280  0.067
2       91   65  22    178  0.511  0.365  0.124
Total  565  229  69    863  0.655  0.265  0.080

>>>>>>
EVALUATION SUMMARY:

       loss  accuracy  precision  recall  precision-0.65  recall-0.65  \
1     1.106     0.350      0.091   0.001           0.000        0.000   
2     1.137     0.343      0.371   0.076           1.000        0.005   
3     1.101     0.396      0.437   0.109           0.387        0.014   
4     1.044     0.468      0.573   0.149           0.500        0.001   
5     1.116     0.365      0.454   0.138           0.684        0.030   
6     1.026     0.448      0.452   0.233           0.478        0.037   
7     1.381     0.260      0.224   0.126           0.175        0.042   
8     1.166     0.352      0.416   0.185           0.167        0.008   
9     1.126     0.431      0.394   0.143           0.341        0.016   
10    0.997     0.504      0.642   0.337           0.695        0.196   
mean  1.120     0.392      0.405   0.150           0.443        0.035   
std   0.106     0.072      0.157   0.090           0.297        0.058   
min   0.997     0.260      0.091   0.001           0.000        0.000   
max   1.381     0.504      0.642   0.337           1.000        0.196   

      precision-0.80  recall-0.80  precision-0.95  recall-0.95  
1              0.000        0.000             0.0          0.0  
2              0.000        0.000             0.0          0.0  
3              0.000        0.000             0.0          0.0  
4              0.000        0.000             0.0          0.0  
5              0.000        0.000             0.0          0.0  
6              0.333        0.002             0.0          0.0  
7              0.000        0.000             0.0          0.0  
8              0.333        0.001             0.0          0.0  
9              0.000        0.000             0.0          0.0  
10             0.607        0.039             0.0          0.0  
mean           0.127        0.004             0.0          0.0  
std            0.218        0.012             0.0          0.0  
min            0.000        0.000             0.0          0.0  
max            0.607        0.039             0.0          0.0  
