
=============
FEATURES (show 1 for each):
returns_lag_1, hashrate_lag_1, fed_rate_lag_1, gold_lag_1, nasdaq_lag_1, sp500_lag_1, google_trend_lag_1, sma_lag_1, boll_lag_1, boll7_lag_1, boll14_lag_1, boll21_lag_1, min_lag_1, min7_lag_1, min14_lag_1, min21_lag_1, max_lag_1, max7_lag_1, max14_lag_1, max21_lag_1, mom_lag_1, mom7_lag_1, mom14_lag_1, mom21_lag_1, vol_lag_1, vol7_lag_1, vol14_lag_1, vol21_lag_1, obv_lag_1, mfi7_lag_1, mfi14_lag_1, mfi21_lag_1, rsi7_lag_1, rsi14_lag_1, rsi21_lag_1, adx7_lag_1, adx14_lag_1, adx21_lag_1, roc_lag_1, roc7_lag_1, roc14_lag_1, roc21_lag_1, atr7_lag_1, atr14_lag_1, atr21_lag_1, bop_lag_1, ad_lag_1, adosc_lag_1, trange_lag_1, ado_lag_1, willr7_lag_1, willr14_lag_1, willr21_lag_1, dx7_lag_1, dx14_lag_1, dx21_lag_1, trix_lag_1, ultosc_lag_1, high_lag_1, low_lag_1, 


=============
DATA PREPARATION PARAMS:
trade_timeframe:1d
take_profit_rate:0.05
stop_loss_rate:0.05
max_duration:3
lags:1
fold_number:3
train_size:0.7
val_size:0.15
categorical_label:True
rebalance:over



============
DATA:
Total rows: 1813
Label 0: 659(36.35%)
Label 1: 575(31.72%)
Label 2: 579(31.94%)

>>>>>> FOLD 1


DATA IN FOLD
Train: 468, Validation: 90, Test: 92

Train:
Label 0: 156(33.33%)
Label 1: 156(33.33%)
Label 2: 156(33.33%)

Validation:
Label 0: 38(42.22%)
Label 1: 34(37.78%)
Label 2: 18(20.0%)

Test:
Label 0: 22(23.91%)
Label 1: 40(43.48%)
Label 2: 30(32.61%)

=============
CLASSIFIER PARAMS:
hu:100
output_bias:[0.0, 0.0, 0.0]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:True
save_check_point:False
early_stopping:True
patience:5
epochs:200
shuffle_when_train:True
batch_size:48
class_weight:{0: 1.0, 1: 1.0, 2: 1.0}



=============
CLASSIFICATION REPORT:

              precision    recall  f1-score   support

           0       0.50      0.18      0.27        22
           1       0.51      0.97      0.67        40
           2       0.43      0.10      0.16        30

    accuracy                           0.50        92
   macro avg       0.48      0.42      0.37        92
weighted avg       0.48      0.50      0.41        92


=============
CONFUSION MATRIX:
       P0  P1  P2  Total    RP0    RP1    RP2
0       4  15   3     22  0.182  0.682  0.136
1       0  39   1     40  0.000  0.975  0.025
2       4  23   3     30  0.133  0.767  0.100
Total   8  77   7     92  0.087  0.837  0.076

>>>>>> FOLD 2


DATA IN FOLD
Train: 618, Validation: 90, Test: 92

Train:
Label 0: 206(33.33%)
Label 1: 206(33.33%)
Label 2: 206(33.33%)

Validation:
Label 0: 10(11.11%)
Label 1: 48(53.33%)
Label 2: 32(35.56%)

Test:
Label 0: 14(15.22%)
Label 1: 44(47.83%)
Label 2: 34(36.96%)

=============
CLASSIFIER PARAMS:
hu:100
output_bias:[0.0, 0.0, 0.0]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:True
save_check_point:False
early_stopping:True
patience:5
epochs:200
shuffle_when_train:True
batch_size:48
class_weight:{0: 0.9999999999999999, 1: 0.9999999999999999, 2: 0.9999999999999999}



=============
CLASSIFICATION REPORT:

              precision    recall  f1-score   support

           0       0.25      0.71      0.37        14
           1       0.51      0.41      0.46        44
           2       0.35      0.18      0.24        34

    accuracy                           0.37        92
   macro avg       0.37      0.43      0.35        92
weighted avg       0.41      0.37      0.36        92


=============
CONFUSION MATRIX:
       P0  P1  P2  Total    RP0    RP1    RP2
0      10   3   1     14  0.714  0.214  0.071
1      16  18  10     44  0.364  0.409  0.227
2      14  14   6     34  0.412  0.412  0.176
Total  40  35  17     92  0.435  0.380  0.185

>>>>>> FOLD 3


DATA IN FOLD
Train: 522, Validation: 90, Test: 92

Train:
Label 0: 174(33.33%)
Label 1: 174(33.33%)
Label 2: 174(33.33%)

Validation:
Label 0: 37(41.11%)
Label 1: 29(32.22%)
Label 2: 24(26.67%)

Test:
Label 0: 64(69.57%)
Label 1: 15(16.3%)
Label 2: 13(14.13%)

=============
CLASSIFIER PARAMS:
hu:100
output_bias:[0.0, 0.0, 0.0]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:True
save_check_point:False
early_stopping:True
patience:5
epochs:200
shuffle_when_train:True
batch_size:48
class_weight:{0: 1.0, 1: 1.0, 2: 1.0}



=============
CLASSIFICATION REPORT:

              precision    recall  f1-score   support

           0       0.71      0.84      0.77        64
           1       0.09      0.07      0.08        15
           2       0.40      0.15      0.22        13

    accuracy                           0.62        92
   macro avg       0.40      0.35      0.36        92
weighted avg       0.57      0.62      0.58        92


=============
CONFUSION MATRIX:
       P0  P1  P2  Total    RP0    RP1    RP2
0      54   7   3     64  0.844  0.109  0.047
1      14   1   0     15  0.933  0.067  0.000
2       8   3   2     13  0.615  0.231  0.154
Total  76  11   5     92  0.826  0.120  0.054

>>>>>>
EVALUATION SUMMARY:

       loss  accuracy  precision  recall  precision-0.65  recall-0.65  \
1     1.102     0.500      0.449   0.239           0.471        0.087   
2     1.258     0.370      0.356   0.174           0.417        0.054   
3     0.884     0.620      0.708   0.500           0.880        0.239   
mean  1.082     0.496      0.504   0.304           0.589        0.127   
std   0.188     0.125      0.182   0.173           0.253        0.099   
min   0.884     0.370      0.356   0.174           0.417        0.054   
max   1.258     0.620      0.708   0.500           0.880        0.239   

      precision-0.80  recall-0.80  precision-0.95  recall-0.95  
1              0.429        0.033           1.000        0.011  
2              0.000        0.000           0.000        0.000  
3              0.000        0.000           0.000        0.000  
mean           0.143        0.011           0.333        0.004  
std            0.247        0.019           0.577        0.006  
min            0.000        0.000           0.000        0.000  
max            0.429        0.033           1.000        0.011  
