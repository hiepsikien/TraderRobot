
=============
FEATURES (show 1 for each):
returns_lag_1, dir_lag_1, hashrate_lag_1, fed_rate_lag_1, gold_lag_1, nasdaq_lag_1, sp500_lag_1, google_trend_lag_1, sma_lag_1, boll_lag_1, min_lag_1, max_lag_1, mom_lag_1, vol_lag_1, obv_lag_1, mfi14_lag_1, rsi14_lag_1, adx14_lag_1, roc_lag_1, atr14_lag_1, bop_lag_1, ad_lag_1, adosc_lag_1, trange_lag_1, ado_lag_1, willr14_lag_1, dx14_lag_1, trix_lag_1, ultosc_lag_1, high_lag_1, low_lag_1, 


=============
DATA PREPARATION PARAMS:
trade_timeframe:15m
take_profit_rate:0.008
stop_loss_rate:0.008
max_duration:12
lags:24
fold_number:10
train_size:0.7
val_size:0.15
categorical_label:True
rebalance:None



============
DATA:
Total rows: 187702
Label 0: 60582(32.28%)
Label 1: 62104(33.09%)
Label 2: 65016(34.64%)

>>>>>> FOLD 1


DATA IN FOLD
Train: 13139, Validation: 2815, Test: 2816

Train:
Label 0: 856(6.51%)
Label 1: 6535(49.74%)
Label 2: 5748(43.75%)

Validation:
Label 0: 7(0.25%)
Label 1: 1295(46.0%)
Label 2: 1513(53.75%)

Test:
Label 0: 52(1.85%)
Label 1: 1476(52.41%)
Label 2: 1288(45.74%)

=============
CLASSIFIER PARAMS:
hu:500
output_bias:[-1.3123313662145273, 0.720325883701629, 0.5920055048428517]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:True
early_stopping:True
patience:5
epochs:200
shuffle_when_train:True
batch_size:1024
class_weight:None



=============
CLASSIFICATION REPORT:

              precision    recall  f1-score   support

           0       0.00      0.00      0.00        52
           1       0.55      0.58      0.56      1476
           2       0.49      0.48      0.48      1288

    accuracy                           0.52      2816
   macro avg       0.35      0.35      0.35      2816
weighted avg       0.51      0.52      0.52      2816


=============
CONFUSION MATRIX:
       P0    P1    P2  Total  RP0    RP1    RP2
0       0    35    17     52  0.0  0.673  0.327
1       0   857   619   1476  0.0  0.581  0.419
2       0   674   614   1288  0.0  0.523  0.477
Total   0  1566  1250   2816  0.0  0.556  0.444

>>>>>> FOLD 2


DATA IN FOLD
Train: 13139, Validation: 2815, Test: 2816

Train:
Label 0: 3262(24.83%)
Label 1: 4652(35.41%)
Label 2: 5225(39.77%)

Validation:
Label 0: 707(25.12%)
Label 1: 1060(37.66%)
Label 2: 1048(37.23%)

Test:
Label 0: 1266(44.96%)
Label 1: 770(27.34%)
Label 2: 780(27.7%)

=============
CLASSIFIER PARAMS:
hu:500
output_bias:[-0.27535700737947444, 0.07959972307812133, 0.19575728627257807]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:True
early_stopping:True
patience:5
epochs:200
shuffle_when_train:True
batch_size:1024
class_weight:None



=============
CLASSIFICATION REPORT:

              precision    recall  f1-score   support

           0       0.48      0.80      0.60      1266
           1       0.26      0.10      0.15       770
           2       0.40      0.20      0.27       780

    accuracy                           0.44      2816
   macro avg       0.38      0.37      0.34      2816
weighted avg       0.40      0.44      0.39      2816


=============
CONFUSION MATRIX:
         P0   P1   P2  Total    RP0    RP1    RP2
0      1016  122  128   1266  0.803  0.096  0.101
1       580   80  110    770  0.753  0.104  0.143
2       513  110  157    780  0.658  0.141  0.201
Total  2109  312  395   2816  0.749  0.111  0.140

>>>>>> FOLD 3


DATA IN FOLD
Train: 13139, Validation: 2815, Test: 2816

Train:
Label 0: 6495(49.43%)
Label 1: 2997(22.81%)
Label 2: 3647(27.76%)

Validation:
Label 0: 2007(71.3%)
Label 1: 429(15.24%)
Label 2: 379(13.46%)

Test:
Label 0: 2263(80.36%)
Label 1: 273(9.69%)
Label 2: 280(9.94%)

=============
CLASSIFIER PARAMS:
hu:500
output_bias:[0.4501828810484179, -0.3232379807396205, -0.12694485724760254]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:True
early_stopping:True
patience:5
epochs:200
shuffle_when_train:True
batch_size:1024
class_weight:None



=============
CLASSIFICATION REPORT:

              precision    recall  f1-score   support

           0       0.80      1.00      0.89      2263
           1       0.00      0.00      0.00       273
           2       0.00      0.00      0.00       280

    accuracy                           0.80      2816
   macro avg       0.27      0.33      0.30      2816
weighted avg       0.65      0.80      0.72      2816


=============
CONFUSION MATRIX:
         P0  P1  P2  Total  RP0  RP1  RP2
0      2263   0   0   2263  1.0  0.0  0.0
1       273   0   0    273  1.0  0.0  0.0
2       280   0   0    280  1.0  0.0  0.0
Total  2816   0   0   2816  1.0  0.0  0.0

>>>>>> FOLD 4


DATA IN FOLD
Train: 13139, Validation: 2815, Test: 2816

Train:
Label 0: 3333(25.37%)
Label 1: 4867(37.04%)
Label 2: 4939(37.59%)

Validation:
Label 0: 1076(38.22%)
Label 1: 836(29.7%)
Label 2: 903(32.08%)

Test:
Label 0: 1374(48.79%)
Label 1: 627(22.27%)
Label 2: 815(28.94%)

=============
CLASSIFIER PARAMS:
hu:500
output_bias:[-0.25729834320206146, 0.12130658832365313, 0.13599173903082368]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:True
early_stopping:True
patience:5
epochs:200
shuffle_when_train:True
batch_size:1024
class_weight:None



=============
CLASSIFICATION REPORT:

              precision    recall  f1-score   support

           0       0.56      0.92      0.69      1374
           1       0.43      0.29      0.35       627
           2       0.54      0.07      0.13       815

    accuracy                           0.54      2816
   macro avg       0.51      0.43      0.39      2816
weighted avg       0.52      0.54      0.45      2816


=============
CONFUSION MATRIX:
         P0   P1   P2  Total    RP0    RP1    RP2
0      1270   82   22   1374  0.924  0.060  0.016
1       416  181   30    627  0.663  0.289  0.048
2       599  156   60    815  0.735  0.191  0.074
Total  2285  419  112   2816  0.811  0.149  0.040

>>>>>> FOLD 5


DATA IN FOLD
Train: 13139, Validation: 2815, Test: 2816

Train:
Label 0: 6377(48.53%)
Label 1: 3194(24.31%)
Label 2: 3568(27.16%)

Validation:
Label 0: 485(17.23%)
Label 1: 1112(39.5%)
Label 2: 1218(43.27%)

Test:
Label 0: 780(27.7%)
Label 1: 1075(38.17%)
Label 2: 961(34.13%)

=============
CLASSIFIER PARAMS:
hu:500
output_bias:[0.4240387543159478, -0.26738496322158906, -0.15665379830113224]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:True
early_stopping:True
patience:5
epochs:200
shuffle_when_train:True
batch_size:1024
class_weight:None



=============
CLASSIFICATION REPORT:

              precision    recall  f1-score   support

           0       0.46      0.28      0.35       780
           1       0.39      0.39      0.39      1075
           2       0.39      0.52      0.45       961

    accuracy                           0.40      2816
   macro avg       0.41      0.40      0.40      2816
weighted avg       0.41      0.40      0.40      2816


=============
CONFUSION MATRIX:
        P0    P1    P2  Total    RP0    RP1    RP2
0      217   297   266    780  0.278  0.381  0.341
1      145   424   506   1075  0.135  0.394  0.471
2      110   355   496    961  0.114  0.369  0.516
Total  472  1076  1268   2816  0.168  0.382  0.450

>>>>>> FOLD 6


DATA IN FOLD
Train: 13139, Validation: 2815, Test: 2816

Train:
Label 0: 6603(50.25%)
Label 1: 3024(23.02%)
Label 2: 3512(26.73%)

Validation:
Label 0: 1977(70.23%)
Label 1: 441(15.67%)
Label 2: 397(14.1%)

Test:
Label 0: 1295(45.99%)
Label 1: 786(27.91%)
Label 2: 735(26.1%)

=============
CLASSIFIER PARAMS:
hu:500
output_bias:[0.4707606897640122, -0.31018294313090766, -0.16057772567560497]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:True
early_stopping:True
patience:5
epochs:200
shuffle_when_train:True
batch_size:1024
class_weight:None



=============
CLASSIFICATION REPORT:

              precision    recall  f1-score   support

           0       0.59      0.84      0.69      1295
           1       0.56      0.06      0.11       786
           2       0.41      0.50      0.45       735

    accuracy                           0.53      2816
   macro avg       0.52      0.47      0.42      2816
weighted avg       0.53      0.53      0.47      2816


=============
CONFUSION MATRIX:
         P0  P1   P2  Total    RP0    RP1    RP2
0      1085  15  195   1295  0.838  0.012  0.151
1       414  47  325    786  0.527  0.060  0.413
2       345  22  368    735  0.469  0.030  0.501
Total  1844  84  888   2816  0.655  0.030  0.315

>>>>>> FOLD 7


DATA IN FOLD
Train: 13139, Validation: 2815, Test: 2816

Train:
Label 0: 1240(9.44%)
Label 1: 5983(45.54%)
Label 2: 5916(45.03%)

Validation:
Label 0: 669(23.77%)
Label 1: 997(35.42%)
Label 2: 1149(40.82%)

Test:
Label 0: 162(5.75%)
Label 1: 1249(44.35%)
Label 2: 1405(49.89%)

=============
CLASSIFIER PARAMS:
hu:500
output_bias:[-1.0454533151911323, 0.5283574196001276, 0.5170958500401905]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:True
early_stopping:True
patience:5
epochs:200
shuffle_when_train:True
batch_size:1024
class_weight:None



=============
CLASSIFICATION REPORT:

              precision    recall  f1-score   support

           0       0.50      0.04      0.07       162
           1       0.46      0.42      0.44      1249
           2       0.52      0.61      0.56      1405

    accuracy                           0.49      2816
   macro avg       0.49      0.36      0.36      2816
weighted avg       0.49      0.49      0.48      2816


=============
CONFUSION MATRIX:
       P0    P1    P2  Total    RP0    RP1    RP2
0       6    70    86    162  0.037  0.432  0.531
1       2   521   726   1249  0.002  0.417  0.581
2       4   538   863   1405  0.003  0.383  0.614
Total  12  1129  1675   2816  0.004  0.401  0.595

>>>>>> FOLD 8


DATA IN FOLD
Train: 13139, Validation: 2815, Test: 2816

Train:
Label 0: 2295(17.47%)
Label 1: 5410(41.18%)
Label 2: 5434(41.36%)

Validation:
Label 0: 742(26.36%)
Label 1: 965(34.28%)
Label 2: 1108(39.36%)

Test:
Label 0: 670(23.79%)
Label 1: 1014(36.01%)
Label 2: 1132(40.2%)

=============
CLASSIFIER PARAMS:
hu:500
output_bias:[-0.5731529716554131, 0.28436327769049224, 0.28878969583621056]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:True
early_stopping:True
patience:5
epochs:200
shuffle_when_train:True
batch_size:1024
class_weight:None



=============
CLASSIFICATION REPORT:

              precision    recall  f1-score   support

           0       0.34      0.25      0.29       670
           1       0.35      0.32      0.33      1014
           2       0.43      0.52      0.47      1132

    accuracy                           0.38      2816
   macro avg       0.37      0.36      0.36      2816
weighted avg       0.38      0.38      0.38      2816


=============
CONFUSION MATRIX:
        P0   P1    P2  Total    RP0    RP1    RP2
0      170  204   296    670  0.254  0.304  0.442
1      190  324   500   1014  0.187  0.320  0.493
2      137  406   589   1132  0.121  0.359  0.520
Total  497  934  1385   2816  0.176  0.332  0.492

>>>>>> FOLD 9


DATA IN FOLD
Train: 13139, Validation: 2815, Test: 2816

Train:
Label 0: 4106(31.25%)
Label 1: 4353(33.13%)
Label 2: 4680(35.62%)

Validation:
Label 0: 840(29.84%)
Label 1: 878(31.19%)
Label 2: 1097(38.97%)

Test:
Label 0: 875(31.07%)
Label 1: 889(31.57%)
Label 2: 1052(37.36%)

=============
CLASSIFIER PARAMS:
hu:500
output_bias:[-0.0630882382571356, -0.004672294283716949, 0.06776055307567798]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:True
early_stopping:True
patience:5
epochs:200
shuffle_when_train:True
batch_size:1024
class_weight:None



=============
CLASSIFICATION REPORT:

              precision    recall  f1-score   support

           0       0.50      0.65      0.56       875
           1       0.34      0.51      0.41       889
           2       0.47      0.16      0.24      1052

    accuracy                           0.42      2816
   macro avg       0.44      0.44      0.40      2816
weighted avg       0.44      0.42      0.39      2816


=============
CONFUSION MATRIX:
         P0    P1   P2  Total    RP0    RP1    RP2
0       569   233   73    875  0.650  0.266  0.083
1       328   449  112    889  0.369  0.505  0.126
2       246   639  167   1052  0.234  0.607  0.159
Total  1143  1321  352   2816  0.406  0.469  0.125

>>>>>> FOLD 10


DATA IN FOLD
Train: 13139, Validation: 2815, Test: 2816

Train:
Label 0: 5030(38.28%)
Label 1: 4024(30.63%)
Label 2: 4085(31.09%)

Validation:
Label 0: 1399(49.7%)
Label 1: 674(23.94%)
Label 2: 742(26.36%)

Test:
Label 0: 2337(82.99%)
Label 1: 219(7.78%)
Label 2: 260(9.23%)

=============
CLASSIFIER PARAMS:
hu:500
output_bias:[0.14374727227196596, -0.07939627904225667, -0.06435098352770388]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:True
early_stopping:True
patience:5
epochs:200
shuffle_when_train:True
batch_size:1024
class_weight:None



=============
CLASSIFICATION REPORT:

              precision    recall  f1-score   support

           0       0.83      0.99      0.90      2337
           1       0.11      0.00      0.01       219
           2       0.28      0.04      0.07       260

    accuracy                           0.82      2816
   macro avg       0.41      0.34      0.33      2816
weighted avg       0.73      0.82      0.76      2816


=============
CONFUSION MATRIX:
         P0  P1  P2  Total    RP0    RP1    RP2
0      2308   4  25   2337  0.988  0.002  0.011
1       217   1   1    219  0.991  0.005  0.005
2       246   4  10    260  0.946  0.015  0.038
Total  2771   9  36   2816  0.984  0.003  0.013

>>>>>>
EVALUATION SUMMARY:

       loss  accuracy  precision  recall  precision-0.65  recall-0.65  \
1     0.767     0.522      0.530   0.477           0.592        0.110   
2     1.099     0.445      0.469   0.225           0.509        0.068   
3     0.667     0.804      0.803   0.803           0.802        0.776   
4     1.037     0.537      0.562   0.438           0.595        0.240   
5     1.081     0.404      0.401   0.138           0.482        0.024   
6     1.009     0.533      0.586   0.429           0.661        0.275   
7     0.888     0.494      0.498   0.417           0.529        0.132   
8     1.141     0.385      0.414   0.215           0.443        0.070   
9     1.090     0.421      0.467   0.272           0.470        0.123   
10    0.561     0.824      0.831   0.812           0.844        0.747   
mean  0.934     0.537      0.556   0.423           0.593        0.257   
std   0.203     0.156      0.150   0.232           0.138        0.277   
min   0.561     0.385      0.401   0.138           0.443        0.024   
max   1.141     0.824      0.831   0.812           0.844        0.776   

      precision-0.80  recall-0.80  precision-0.95  recall-0.95  
1              0.679        0.013           0.000        0.000  
2              0.714        0.011           0.000        0.000  
3              0.793        0.483           0.500        0.003  
4              0.663        0.043           0.333        0.000  
5              0.333        0.001           0.000        0.000  
6              0.735        0.104           1.000        0.001  
7              0.413        0.016           0.750        0.003  
8              0.520        0.023           0.692        0.003  
9              0.482        0.038           0.571        0.006  
10             0.919        0.467           0.000        0.000  
mean           0.625        0.120           0.385        0.002  
std            0.183        0.189           0.373        0.002  
min            0.333        0.001           0.000        0.000  
max            0.919        0.483           1.000        0.006  
