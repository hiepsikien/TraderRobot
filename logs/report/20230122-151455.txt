
=============
FEATURES (show 1 for each):
returns_lag_1, dir_lag_1, sma_lag_1, boll_lag_1, boll7_lag_1, boll14_lag_1, boll21_lag_1, min_lag_1, min7_lag_1, min14_lag_1, min21_lag_1, max_lag_1, max7_lag_1, max14_lag_1, max21_lag_1, mom_lag_1, mom7_lag_1, mom14_lag_1, mom21_lag_1, vol_lag_1, vol7_lag_1, vol14_lag_1, vol21_lag_1, obv_lag_1, mfi7_lag_1, mfi14_lag_1, mfi21_lag_1, rsi7_lag_1, rsi14_lag_1, rsi21_lag_1, adx7_lag_1, adx14_lag_1, adx21_lag_1, roc_lag_1, roc7_lag_1, roc14_lag_1, roc21_lag_1, atr7_lag_1, atr14_lag_1, atr21_lag_1, bop_lag_1, ad_lag_1, adosc_lag_1, trange_lag_1, ado_lag_1, willr7_lag_1, willr14_lag_1, willr21_lag_1, dx7_lag_1, dx14_lag_1, dx21_lag_1, trix_lag_1, ultosc_lag_1, high_lag_1, low_lag_1, 


=============
DATA PREPARATION PARAMS:
trade_timeframe:1h
take_profit_rate:0.05
stop_loss_rate:0.05
max_duration:12
lags:90
fold_number:10
train_size:0.7
val_size:0.15
categorical_label:True
rebalance:None
==========
DATA:

FOLD 1
Train: 3271, Validation: 701, Test: 702

Train:
Label 0: 1849(56.53%)
Label 1: 696(21.28%)
Label 2: 726(22.2%)

Validation:
Label 0: 192(27.39%)
Label 1: 195(27.82%)
Label 2: 314(44.79%)

Test:
Label 0: 434(61.82%)
Label 1: 137(19.52%)
Label 2: 131(18.66%)

FOLD 2
Train: 3271, Validation: 701, Test: 702

Train:
Label 0: 2775(84.84%)
Label 1: 229(7.0%)
Label 2: 267(8.16%)

Validation:
Label 0: 592(84.45%)
Label 1: 35(4.99%)
Label 2: 74(10.56%)

Test:
Label 0: 673(95.87%)
Label 1: 2(0.28%)
Label 2: 27(3.85%)

FOLD 3
Train: 3271, Validation: 701, Test: 702

Train:
Label 0: 2840(86.82%)
Label 1: 191(5.84%)
Label 2: 240(7.34%)

Validation:
Label 0: 661(94.29%)
Label 1: 28(3.99%)
Label 2: 12(1.71%)

Test:
Label 0: 663(94.44%)
Label 1: 31(4.42%)
Label 2: 8(1.14%)

FOLD 4
Train: 3271, Validation: 701, Test: 702

Train:
Label 0: 2658(81.26%)
Label 1: 290(8.87%)
Label 2: 323(9.87%)

Validation:
Label 0: 653(93.15%)
Label 1: 18(2.57%)
Label 2: 30(4.28%)

Test:
Label 0: 652(92.88%)
Label 1: 21(2.99%)
Label 2: 29(4.13%)

FOLD 5
Train: 3271, Validation: 701, Test: 702

Train:
Label 0: 3020(92.33%)
Label 1: 144(4.4%)
Label 2: 107(3.27%)

Validation:
Label 0: 404(57.63%)
Label 1: 143(20.4%)
Label 2: 154(21.97%)

Test:
Label 0: 593(84.47%)
Label 1: 74(10.54%)
Label 2: 35(4.99%)

FOLD 6
Train: 3271, Validation: 701, Test: 702

Train:
Label 0: 3079(94.13%)
Label 1: 67(2.05%)
Label 2: 125(3.82%)

Validation:
Label 0: 694(99.0%)
Label 2: 7(1.0%)

Test:
Label 0: 647(92.17%)
Label 1: 34(4.84%)
Label 2: 21(2.99%)

FOLD 7
Train: 3271, Validation: 701, Test: 702

Train:
Label 0: 2330(71.23%)
Label 1: 482(14.74%)
Label 2: 459(14.03%)

Validation:
Label 0: 614(87.59%)
Label 1: 35(4.99%)
Label 2: 52(7.42%)

Test:
Label 0: 417(59.4%)
Label 1: 95(13.53%)
Label 2: 190(27.07%)

FOLD 8
Train: 3271, Validation: 701, Test: 702

Train:
Label 0: 2725(83.31%)
Label 1: 274(8.38%)
Label 2: 272(8.32%)

Validation:
Label 0: 644(91.87%)
Label 1: 30(4.28%)
Label 2: 27(3.85%)

Test:
Label 0: 616(87.75%)
Label 1: 21(2.99%)
Label 2: 65(9.26%)

FOLD 9
Train: 3271, Validation: 701, Test: 702

Train:
Label 0: 2963(90.58%)
Label 1: 130(3.97%)
Label 2: 178(5.44%)

Validation:
Label 0: 580(82.74%)
Label 1: 46(6.56%)
Label 2: 75(10.7%)

Test:
Label 0: 513(73.08%)
Label 1: 69(9.83%)
Label 2: 120(17.09%)

FOLD 10
Train: 3271, Validation: 701, Test: 702

Train:
Label 0: 3006(91.9%)
Label 1: 167(5.11%)
Label 2: 98(3.0%)

Validation:
Label 0: 619(88.3%)
Label 1: 28(3.99%)
Label 2: 54(7.7%)

Test:
Label 0: 702(100.0%)

>>>>>> FOLD 1


=============
CLASSIFIER PARAMS:
hu:1000
output_bias:[0.6373002660806335, -0.3397503049720718, -0.29754995048169525]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:True
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.70      0.51      0.59       434
           1       0.19      0.23      0.21       137
           2       0.32      0.53      0.40       131

    accuracy                           0.46       702
   macro avg       0.40      0.43      0.40       702
weighted avg       0.53      0.46      0.48       702


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0      223  108  103    434  0.514  0.249  0.237
1       62   32   43    137  0.453  0.234  0.314
2       34   27   70    131  0.260  0.206  0.534
Total  319  167  216    702  0.454  0.238  0.308

>>>>>> FOLD 2


=============
CLASSIFIER PARAMS:
hu:1000
output_bias:[1.6119471264072571, -0.8827368962270173, -0.7292102413639008]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:True
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.97      0.91      0.94       673
           1       0.02      0.50      0.04         2
           2       0.22      0.19      0.20        27

    accuracy                           0.88       702
   macro avg       0.40      0.53      0.39       702
weighted avg       0.93      0.88      0.91       702


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0      612   43   18    673  0.909  0.064  0.027
1        1    1    0      2  0.500  0.500  0.000
2       21    1    5     27  0.778  0.037  0.185
Total  634   45   23    702  0.903  0.064  0.033

>>>>>> FOLD 3


=============
CLASSIFIER PARAMS:
hu:1000
output_bias:[1.7234021284463124, -0.9758837746621954, -0.7475182793670659]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:True
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.97      0.93      0.95       663
           1       0.21      0.26      0.23        31
           2       0.00      0.00      0.00         8

    accuracy                           0.89       702
   macro avg       0.39      0.40      0.39       702
weighted avg       0.92      0.89      0.90       702


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0      616   23   24    663  0.929  0.035  0.036
1       21    8    2     31  0.677  0.258  0.065
2        1    7    0      8  0.125  0.875  0.000
Total  638   38   26    702  0.909  0.054  0.037

>>>>>> FOLD 4


=============
CLASSIFIER PARAMS:
hu:1000
output_bias:[1.4410417576053236, -0.7744065586881321, -0.6666351584467285]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:True
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.93      0.95      0.94       652
           1       0.00      0.00      0.00        21
           2       0.00      0.00      0.00        29

    accuracy                           0.88       702
   macro avg       0.31      0.32      0.31       702
weighted avg       0.86      0.88      0.87       702


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0      619   20   13    652  0.949  0.031  0.020
1       21    0    0     21  1.000  0.000  0.000
2       29    0    0     29  1.000  0.000  0.000
Total  669   20   13    702  0.953  0.028  0.019

>>>>>> FOLD 5


=============
CLASSIFIER PARAMS:
hu:1000
output_bias:[2.1277939463152653, -0.9154048645034258, -1.2123893295611383]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:True
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.87      0.90      0.88       593
           1       0.09      0.04      0.06        74
           2       0.16      0.26      0.20        35

    accuracy                           0.77       702
   macro avg       0.37      0.40      0.38       702
weighted avg       0.75      0.77      0.76       702


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0      532   28   33    593  0.897  0.047  0.056
1       57    3   14     74  0.770  0.041  0.189
2       23    3    9     35  0.657  0.086  0.257
Total  612   34   56    702  0.872  0.048  0.080

>>>>>> FOLD 6


=============
CLASSIFIER PARAMS:
hu:1000
output_bias:[2.3439047855590465, -1.4837627429749183, -0.8601416250628833]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:True
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.94      0.87      0.90       647
           1       0.00      0.00      0.00        34
           2       0.05      0.14      0.07        21

    accuracy                           0.81       702
   macro avg       0.33      0.34      0.32       702
weighted avg       0.86      0.81      0.83       702


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0      563   39   45    647  0.870  0.060  0.070
1       21    0   13     34  0.618  0.000  0.382
2       18    0    3     21  0.857  0.000  0.143
Total  602   39   61    702  0.858  0.056  0.087

>>>>>> FOLD 7


=============
CLASSIFIER PARAMS:
hu:1000
output_bias:[1.0667509509191215, -0.5089284816213046, -0.557822385546681]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:True
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.69      0.76      0.73       417
           1       0.39      0.60      0.47        95
           2       0.34      0.17      0.23       190

    accuracy                           0.58       702
   macro avg       0.47      0.51      0.48       702
weighted avg       0.56      0.58      0.56       702


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0      318   42   57    417  0.763  0.101  0.137
1       32   57    6     95  0.337  0.600  0.063
2      109   48   33    190  0.574  0.253  0.174
Total  459  147   96    702  0.654  0.209  0.137

>>>>>> FOLD 8


=============
CLASSIFIER PARAMS:
hu:1000
output_bias:[1.5338390575389116, -0.7632565431994872, -0.7705825832340871]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:True
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.88      0.87      0.87       616
           1       0.00      0.00      0.00        21
           2       0.07      0.06      0.07        65

    accuracy                           0.77       702
   macro avg       0.32      0.31      0.31       702
weighted avg       0.78      0.77      0.77       702


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0      535   33   48    616  0.869  0.054  0.078
1       18    0    3     21  0.857  0.000  0.143
2       58    3    4     65  0.892  0.046  0.062
Total  611   36   55    702  0.870  0.051  0.078

>>>>>> FOLD 9


=============
CLASSIFIER PARAMS:
hu:1000
output_bias:[1.9795324093671922, -1.1468906877493237, -0.8326415879155716]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:True
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.80      0.78      0.79       513
           1       0.24      0.45      0.32        69
           2       0.26      0.17      0.20       120

    accuracy                           0.64       702
   macro avg       0.44      0.47      0.44       702
weighted avg       0.66      0.64      0.65       702


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0      401   72   40    513  0.782  0.140  0.078
1       22   31   16     69  0.319  0.449  0.232
2       76   24   20    120  0.633  0.200  0.167
Total  499  127   76    702  0.711  0.181  0.108

>>>>>> FOLD 10


=============
CLASSIFIER PARAMS:
hu:1000
output_bias:[2.1045899196149795, -0.7857818382816572, -1.3188081720267197]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:True
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20
