
=============
FEATURES (show 1 for each):
returns_lag_1, dir_lag_1, sma_lag_1, boll_lag_1, boll7_lag_1, boll14_lag_1, boll21_lag_1, min_lag_1, min7_lag_1, min14_lag_1, min21_lag_1, max_lag_1, max7_lag_1, max14_lag_1, max21_lag_1, mom_lag_1, mom7_lag_1, mom14_lag_1, mom21_lag_1, vol_lag_1, vol7_lag_1, vol14_lag_1, vol21_lag_1, obv_lag_1, mfi7_lag_1, mfi14_lag_1, mfi21_lag_1, rsi7_lag_1, rsi14_lag_1, rsi21_lag_1, adx7_lag_1, adx14_lag_1, adx21_lag_1, roc_lag_1, roc7_lag_1, roc14_lag_1, roc21_lag_1, atr7_lag_1, atr14_lag_1, atr21_lag_1, bop_lag_1, ad_lag_1, adosc_lag_1, trange_lag_1, ado_lag_1, willr7_lag_1, willr14_lag_1, willr21_lag_1, dx7_lag_1, dx14_lag_1, dx21_lag_1, trix_lag_1, ultosc_lag_1, high_lag_1, low_lag_1, 


=============
DATA PREPARATION PARAMS:
trade_timeframe:4h
take_profit_rate:0.03
stop_loss_rate:0.03
max_duration:8
lags:14
fold_number:5
train_size:0.7
val_size:0.15
categorical_label:True
rebalance:over

============
DATA:
Total rows: 11597
Label 0: 3720(32.08%)
Label 1: 3925(33.84%)
Label 2: 3952(34.08%)

>>>>>> FOLD 1


DATA IN FOLD
Train: 2253, Validation: 390, Test: 349

Train:
Label 0: 751(33.33%)
Label 1: 751(33.33%)
Label 2: 751(33.33%)

Validation:
Label 0: 130(33.33%)
Label 1: 130(33.33%)
Label 2: 130(33.33%)

Test:
Label 0: 169(48.42%)
Label 1: 98(28.08%)
Label 2: 82(23.5%)

=============
CLASSIFIER PARAMS:
hu:100
output_bias:[0.0, 0.0, 0.0]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:True
save_check_point:False
early_stopping:True
patience:5
epochs:200
shuffle_when_train:False
batch_size:48
class_weight:{0: 1.0, 1: 1.0, 2: 1.0}

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.52      0.96      0.67       169
           1       0.55      0.06      0.11        98
           2       0.48      0.15      0.22        82

    accuracy                           0.52       349
   macro avg       0.51      0.39      0.34       349
weighted avg       0.52      0.52      0.41       349


=============
CONFUSION MATRIX:
        P0  P1  P2  Total    RP0    RP1    RP2
0      162   1   6    169  0.959  0.006  0.036
1       85   6   7     98  0.867  0.061  0.071
2       66   4  12     82  0.805  0.049  0.146
Total  313  11  25    349  0.897  0.032  0.072

>>>>>> FOLD 2


DATA IN FOLD
Train: 2253, Validation: 462, Test: 349

Train:
Label 0: 751(33.33%)
Label 1: 751(33.33%)
Label 2: 751(33.33%)

Validation:
Label 0: 154(33.33%)
Label 1: 154(33.33%)
Label 2: 154(33.33%)

Test:
Label 0: 150(42.98%)
Label 1: 89(25.5%)
Label 2: 110(31.52%)

=============
CLASSIFIER PARAMS:
hu:100
output_bias:[0.0, 0.0, 0.0]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:True
save_check_point:False
early_stopping:True
patience:5
epochs:200
shuffle_when_train:False
batch_size:48
class_weight:{0: 1.0, 1: 1.0, 2: 1.0}

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.46      0.69      0.55       150
           1       0.25      0.17      0.20        89
           2       0.44      0.25      0.32       110

    accuracy                           0.42       349
   macro avg       0.38      0.37      0.36       349
weighted avg       0.40      0.42      0.39       349


=============
CONFUSION MATRIX:
        P0  P1  P2  Total    RP0    RP1    RP2
0      103  35  12    150  0.687  0.233  0.080
1       51  15  23     89  0.573  0.169  0.258
2       71  11  28    110  0.645  0.100  0.255
Total  225  61  63    349  0.645  0.175  0.181

>>>>>> FOLD 3


DATA IN FOLD
Train: 2163, Validation: 516, Test: 349

Train:
Label 0: 721(33.33%)
Label 1: 721(33.33%)
Label 2: 721(33.33%)

Validation:
Label 0: 172(33.33%)
Label 1: 172(33.33%)
Label 2: 172(33.33%)

Test:
Label 0: 172(49.28%)
Label 1: 127(36.39%)
Label 2: 50(14.33%)

=============
CLASSIFIER PARAMS:
hu:100
output_bias:[0.0, 0.0, 0.0]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:True
save_check_point:False
early_stopping:True
patience:5
epochs:200
shuffle_when_train:False
batch_size:48
class_weight:{0: 1.0, 1: 1.0, 2: 1.0}

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.57      0.75      0.65       172
           1       0.47      0.34      0.39       127
           2       0.12      0.08      0.10        50

    accuracy                           0.50       349
   macro avg       0.39      0.39      0.38       349
weighted avg       0.47      0.50      0.48       349


=============
CONFUSION MATRIX:
        P0  P1  P2  Total    RP0    RP1    RP2
0      129  25  18    172  0.750  0.145  0.105
1       73  43  11    127  0.575  0.339  0.087
2       23  23   4     50  0.460  0.460  0.080
Total  225  91  33    349  0.645  0.261  0.095

>>>>>> FOLD 4


DATA IN FOLD
Train: 2193, Validation: 372, Test: 349

Train:
Label 0: 731(33.33%)
Label 1: 731(33.33%)
Label 2: 731(33.33%)

Validation:
Label 0: 124(33.33%)
Label 1: 124(33.33%)
Label 2: 124(33.33%)

Test:
Label 0: 80(22.92%)
Label 1: 132(37.82%)
Label 2: 137(39.26%)

=============
CLASSIFIER PARAMS:
hu:100
output_bias:[0.0, 0.0, 0.0]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:True
save_check_point:False
early_stopping:True
patience:5
epochs:200
shuffle_when_train:False
batch_size:48
class_weight:{0: 1.0, 1: 1.0, 2: 1.0}

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.20      0.41      0.27        80
           1       0.38      0.31      0.34       132
           2       0.47      0.26      0.33       137

    accuracy                           0.31       349
   macro avg       0.35      0.33      0.31       349
weighted avg       0.37      0.31      0.32       349


=============
CONFUSION MATRIX:
        P0   P1  P2  Total    RP0    RP1    RP2
0       33   31  16     80  0.412  0.388  0.200
1       68   41  23    132  0.515  0.311  0.174
2       65   37  35    137  0.474  0.270  0.255
Total  166  109  74    349  0.476  0.312  0.212

>>>>>> FOLD 5


DATA IN FOLD
Train: 1881, Validation: 552, Test: 349

Train:
Label 0: 627(33.33%)
Label 1: 627(33.33%)
Label 2: 627(33.33%)

Validation:
Label 0: 184(33.33%)
Label 1: 184(33.33%)
Label 2: 184(33.33%)

Test:
Label 0: 230(65.9%)
Label 1: 56(16.05%)
Label 2: 63(18.05%)

=============
CLASSIFIER PARAMS:
hu:100
output_bias:[0.0, 0.0, 0.0]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:True
save_check_point:False
early_stopping:True
patience:5
epochs:200
shuffle_when_train:False
batch_size:48
class_weight:{0: 1.0, 1: 1.0, 2: 1.0}

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.66      0.52      0.58       230
           1       0.14      0.14      0.14        56
           2       0.25      0.44      0.32        63

    accuracy                           0.45       349
   macro avg       0.35      0.37      0.35       349
weighted avg       0.50      0.45      0.47       349


=============
CONFUSION MATRIX:
        P0  P1   P2  Total    RP0    RP1    RP2
0      120  36   74    230  0.522  0.157  0.322
1       39   8    9     56  0.696  0.143  0.161
2       22  13   28     63  0.349  0.206  0.444
Total  181  57  111    349  0.519  0.163  0.318

>>>>>>
EVALUATION SUMMARY:
       loss  accuracy  precision  recall  precision-0.65  recall-0.65  \
1     1.179     0.516      0.519   0.424           0.508        0.275   
2     1.339     0.418      0.404   0.315           0.400        0.143   
3     1.045     0.504      0.547   0.367           0.523        0.132   
4     1.272     0.312      0.295   0.149           0.346        0.052   
5     1.158     0.447      0.420   0.278           0.387        0.089   
mean  1.199     0.440      0.437   0.307           0.433        0.138   
std   0.113     0.082      0.100   0.104           0.078        0.085   
min   1.045     0.312      0.295   0.149           0.346        0.052   
max   1.339     0.516      0.547   0.424           0.523        0.275   

      precision-0.80  recall-0.80  precision-0.95  recall-0.95  
1              0.527        0.140           1.000        0.003  
2              0.459        0.049           0.500        0.003  
3              0.375        0.017           0.000        0.000  
4              0.111        0.003           0.000        0.000  
5              0.429        0.009           0.000        0.000  
mean           0.380        0.044           0.300        0.001  
std            0.160        0.057           0.447        0.002  
min            0.111        0.003           0.000        0.000  
max            0.527        0.140           1.000        0.003  
