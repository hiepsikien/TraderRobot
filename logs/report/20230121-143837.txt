
=============
FEATURES (show 1 for each):
returns_lag_1, dir_lag_1, sma_lag_1, boll_lag_1, boll7_lag_1, boll14_lag_1, boll21_lag_1, min_lag_1, min7_lag_1, min14_lag_1, min21_lag_1, max_lag_1, max7_lag_1, max14_lag_1, max21_lag_1, mom_lag_1, mom7_lag_1, mom14_lag_1, mom21_lag_1, vol_lag_1, vol7_lag_1, vol14_lag_1, vol21_lag_1, obv_lag_1, mfi7_lag_1, mfi14_lag_1, mfi21_lag_1, rsi7_lag_1, rsi14_lag_1, rsi21_lag_1, adx7_lag_1, adx14_lag_1, adx21_lag_1, roc_lag_1, roc7_lag_1, roc14_lag_1, roc21_lag_1, atr7_lag_1, atr14_lag_1, atr21_lag_1, bop_lag_1, ad_lag_1, adosc_lag_1, trange_lag_1, ado_lag_1, willr7_lag_1, willr14_lag_1, willr21_lag_1, dx7_lag_1, dx14_lag_1, dx21_lag_1, trix_lag_1, ultosc_lag_1, high_lag_1, low_lag_1, 


>>>>>> LAP 1

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 418(34.66%)
Label 1: 409(33.91%)
Label 2: 379(31.43%)

Validation:
Label 0: 104(40.31%)
Label 1: 89(34.5%)
Label 2: 65(25.19%)

Test:
Label 0: 97(37.31%)
Label 1: 76(29.23%)
Label 2: 87(33.46%)

=============
PARAMS:
random_state:1
is_shuffle:True
categorical_label:True
rebalance:None
hu:2000
output_bias:[0.039903826239926485, 0.01813754975797201, -0.058041401202403496]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.76      0.70      0.73        97
           1       0.67      0.80      0.73        76
           2       0.82      0.76      0.79        87

    accuracy                           0.75       260
   macro avg       0.75      0.75      0.75       260
weighted avg       0.76      0.75      0.75       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       68   20    9     97  0.701  0.206  0.093
1       10   61    5     76  0.132  0.803  0.066
2       11   10   66     87  0.126  0.115  0.759
Total   89   91   80    260  0.342  0.350  0.308

>>>>>> LAP 2

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 447(37.06%)
Label 1: 387(32.09%)
Label 2: 372(30.85%)

Validation:
Label 0: 86(33.33%)
Label 1: 92(35.66%)
Label 2: 80(31.01%)

Test:
Label 0: 86(33.08%)
Label 1: 95(36.54%)
Label 2: 79(30.38%)

=============
PARAMS:
random_state:2
is_shuffle:True
categorical_label:True
rebalance:None
hu:2000
output_bias:[0.10926621052533671, -0.03486769105844862, -0.0743985298150877]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.65      0.78      0.71        86
           1       0.79      0.64      0.71        95
           2       0.72      0.73      0.73        79

    accuracy                           0.72       260
   macro avg       0.72      0.72      0.72       260
weighted avg       0.72      0.72      0.72       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       67    9   10     86  0.779  0.105  0.116
1       22   61   12     95  0.232  0.642  0.126
2       14    7   58     79  0.177  0.089  0.734
Total  103   77   80    260  0.396  0.296  0.308

>>>>>> LAP 3

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 427(35.41%)
Label 1: 408(33.83%)
Label 2: 371(30.76%)

Validation:
Label 0: 91(35.27%)
Label 1: 83(32.17%)
Label 2: 84(32.56%)

Test:
Label 0: 101(38.85%)
Label 1: 83(31.92%)
Label 2: 76(29.23%)

=============
PARAMS:
random_state:3
is_shuffle:True
categorical_label:True
rebalance:None
hu:2000
output_bias:[0.06203292028968094, 0.016516081465218123, -0.0785490303315086]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.70      0.58      0.64       101
           1       0.73      0.81      0.77        83
           2       0.70      0.78      0.74        76

    accuracy                           0.71       260
   macro avg       0.71      0.72      0.71       260
weighted avg       0.71      0.71      0.71       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       59   21   21    101  0.584  0.208  0.208
1       12   67    4     83  0.145  0.807  0.048
2       13    4   59     76  0.171  0.053  0.776
Total   84   92   84    260  0.323  0.354  0.323

>>>>>> LAP 4

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 425(35.24%)
Label 1: 413(34.25%)
Label 2: 368(30.51%)

Validation:
Label 0: 105(40.7%)
Label 1: 83(32.17%)
Label 2: 70(27.13%)

Test:
Label 0: 89(34.23%)
Label 1: 78(30.0%)
Label 2: 93(35.77%)

=============
PARAMS:
random_state:4
is_shuffle:True
categorical_label:True
rebalance:None
hu:2000
output_bias:[0.05754927293885754, 0.028907696975473404, -0.08645695781662842]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.77      0.81      0.79        89
           1       0.78      0.76      0.77        78
           2       0.81      0.80      0.80        93

    accuracy                           0.79       260
   macro avg       0.79      0.79      0.79       260
weighted avg       0.79      0.79      0.79       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       72    8    9     89  0.809  0.090  0.101
1       11   59    8     78  0.141  0.756  0.103
2       10    9   74     93  0.108  0.097  0.796
Total   93   76   91    260  0.358  0.292  0.350

>>>>>> LAP 5

==========
DATA:
Data Train: 1206, Validation: 258, Test: 260

Train:
Label 0: 436(36.15%)
Label 1: 402(33.33%)
Label 2: 368(30.51%)

Validation:
Label 0: 84(32.56%)
Label 1: 88(34.11%)
Label 2: 86(33.33%)

Test:
Label 0: 99(38.08%)
Label 1: 84(32.31%)
Label 2: 77(29.62%)

=============
PARAMS:
random_state:5
is_shuffle:True
categorical_label:True
rebalance:None
hu:2000
output_bias:[0.08358315680864217, 0.002393002078628901, -0.08597614837146135]
loss:categorical_crossentropy
dropout:True
dropout_rate:0.3
learning_rate:0.0001
gpu:False
set_class_weight:False
save_check_point:False
early_stopping:True
patience:5
epochs:200
batch_size:20

=============
CLASSIFICATION REPORT:
              precision    recall  f1-score   support

           0       0.73      0.68      0.70        99
           1       0.67      0.82      0.74        84
           2       0.82      0.69      0.75        77

    accuracy                           0.73       260
   macro avg       0.74      0.73      0.73       260
weighted avg       0.74      0.73      0.73       260


=============
CONFUSION MATRIX:
       P-0  P-1  P-2  Total   RP-0   RP-1   RP-2
0       67   23    9     99  0.677  0.232  0.091
1       12   69    3     84  0.143  0.821  0.036
2       13   11   53     77  0.169  0.143  0.688
Total   92  103   65    260  0.354  0.396  0.250

>>>>>>
EVALUATION SUMMARY:
       loss  accuracy  precision  recall  precision-0.65  recall-0.65  \
1     1.094     0.750      0.755   0.746           0.798        0.700   
2     1.057     0.715      0.723   0.712           0.738        0.650   
3     1.056     0.712      0.718   0.704           0.754        0.662   
4     0.781     0.788      0.796   0.781           0.840        0.727   
5     1.057     0.727      0.730   0.719           0.762        0.654   
mean  1.009     0.738      0.744   0.732           0.779        0.678   
std   0.129     0.032      0.032   0.031           0.041        0.034   
min   0.781     0.712      0.718   0.704           0.738        0.650   
max   1.094     0.788      0.796   0.781           0.840        0.727   

      precision-0.80  recall-0.80  precision-0.95  recall-0.95  
1              0.834        0.619           0.866        0.496  
2              0.746        0.588           0.836        0.450  
3              0.795        0.612           0.836        0.469  
4              0.859        0.631           0.922        0.458  
5              0.786        0.581           0.815        0.388  
mean           0.804        0.606           0.855        0.452  
std            0.044        0.021           0.042        0.040  
min            0.746        0.581           0.815        0.388  
max            0.859        0.631           0.922        0.496  
